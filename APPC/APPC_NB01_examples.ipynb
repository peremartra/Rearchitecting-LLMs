{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/main/APPC/APPC_NB01_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEZwciT20NQP"
      },
      "source": [
        "# Tailoring LLM Architectures\n",
        "## Surgical Optimization Beyond Fine-Tuning\n",
        "\n",
        "\n",
        "### Appendix C: Energy consumption evaluation\n",
        "### Notebook: 01. How to use `measure_energy_consumption`\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white)](https://x.com/PereMartra) [![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* gemma-3-270m\n",
        "_____\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q lm-eval transformers torch accelerate codecarbon\n",
        "\n",
        "# Import libraries\n",
        "import torch, gc, time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from codecarbon import EmissionsTracker\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EO-r6iW8fEM",
        "outputId": "3fd8b1af-26ee-4031-b588-e1bf0f982145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.6/357.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.55.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPY6ryBtUrHY"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"google/gemma-3-270m\"\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"Model loaded on device: {model.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RECOVERY_SAMPLES = 100\n",
        "MAX_LENGTH = 1024\n",
        "BATCH_SIZE = 4\n",
        "MAX_SAMPLES_CARBON=50"
      ],
      "metadata": {
        "id": "51kHe9eVBVC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset, text_field='text'):\n",
        "    \"\"\"\n",
        "    Tokenizes and prepares a dataset for calibration.\n",
        "\n",
        "    Handles different dataset formats (WikiText uses 'text', SMS uses 'sms' field).\n",
        "    \"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        if text_field in examples:\n",
        "            texts = examples[text_field]\n",
        "        elif 'sms' in examples:  # SMS dataset specific\n",
        "            texts = examples['sms']\n",
        "        elif 'text' in examples:\n",
        "            texts = examples['text']\n",
        "        else:\n",
        "            texts = examples[list(examples.keys())[0]]\n",
        "\n",
        "        return tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "    return DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "DW8Bsz7xBDUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datawiki = load_dataset('wikitext', 'wikitext-2-raw-v1', split=f'train[:{RECOVERY_SAMPLES}]')\n",
        "dataloaderwiki = prepare_dataset(datawiki)  # WikiText (largo)"
      ],
      "metadata": {
        "id": "RlppN27sAyJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_gpu_cache():\n",
        "    \"\"\"Clear GPU cache completely\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "3tlCSwK28WHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing C.1 Calibrating GPU idle power\n"
      ],
      "metadata": {
        "id": "MGH-ML0wHk-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate_idle_power(device=\"cuda\", duration_seconds=30):\n",
        "    clear_gpu_cache()  #A\n",
        "\n",
        "\n",
        "    tracker = EmissionsTracker(\n",
        "        project_name=\"idle_calibration\",\n",
        "        measure_power_secs=1,  #B\n",
        "        save_to_file=False,\n",
        "        log_level=\"error\"\n",
        "    )\n",
        "    tracker.start()\n",
        "    start_time = time.time()\n",
        "    time.sleep(duration_seconds)  #C\n",
        "    emissions_data = tracker.stop()\n",
        "    actual_duration = time.time() - start_time\n",
        "\n",
        "    energy_kwh = tracker._total_energy.kWh #D\n",
        "    idle_power_watts = (energy_kwh * 1000) / (actual_duration / 3600) #E\n",
        "\n",
        "    gpu_temp = None\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            gpu_temp = torch.cuda.temperature()\n",
        "        except:\n",
        "            gpu_temp = None\n",
        "\n",
        "    if gpu_temp is not None:\n",
        "        print(f\"   GPU Temperature: {gpu_temp:.1f}Â°C\")\n",
        "\n",
        "    return {\n",
        "        \"idle_power_watts\": idle_power_watts,\n",
        "        \"gpu_temp_celsius\": gpu_temp,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "iRycoirsE-8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idle_calibration = calibrate_idle_power(device=\"cuda\", duration_seconds=30)\n",
        "idle_watts = idle_calibration[\"idle_power_watts\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CknN0WTx1uHc",
        "outputId": "2bdb875b-81c0-4cf1-affd-def3ed7c38dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 16:01:26] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU Temperature: 47.0Â°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing C.2 Measuring net energy consumption\n"
      ],
      "metadata": {
        "id": "QiihMrYDHvQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_energy_consumption(model, tokenizer, data_source,\n",
        "                               idle_power_watts, max_samples=50,\n",
        "                               max_new_tokens=50):\n",
        "    model.eval()\n",
        "\n",
        "    samples = []\n",
        "    for batch in data_source:\n",
        "        for i in range(len(batch['input_ids'])):\n",
        "            samples.append(batch['input_ids'][i])\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "        if len(samples) >= max_samples:\n",
        "            break  #A\n",
        "\n",
        "    warmup_input = samples[0].unsqueeze(0).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(2):\n",
        "            model.generate(\n",
        "                warmup_input,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "    torch.cuda.synchronize()  #B\n",
        "\n",
        "    tracker = EmissionsTracker(\n",
        "        measure_power_secs=1,\n",
        "        save_to_file=False,\n",
        "        log_level=\"error\"\n",
        "    )  #C\n",
        "\n",
        "    total_tokens_generated = 0\n",
        "    tracker.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in samples:\n",
        "            input_ids = sample.unsqueeze(0).to(model.device)\n",
        "            torch.cuda.synchronize()  #D\n",
        "\n",
        "            outputs = model.generate(\n",
        "                input_ids,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            num_new_tokens = outputs.shape[1] - input_ids.shape[1]\n",
        "            total_tokens_generated += num_new_tokens\n",
        "\n",
        "    tracker.stop()\n",
        "    duration_seconds = time.time() - start_time\n",
        "    emissions_raw_kwh = tracker._total_energy.kWh  #E\n",
        "\n",
        "    idle_energy_kwh = (idle_power_watts * duration_seconds) / 3_600_000\n",
        "    energy_net_kwh = max(0.0, emissions_raw_kwh - idle_energy_kwh)  #F\n",
        "\n",
        "    total_joules_net = energy_net_kwh * 3_600_000\n",
        "    joules_per_token = (total_joules_net / total_tokens_generated\n",
        "                       if total_tokens_generated > 0 else 0.0)  #G\n",
        "\n",
        "    return {\n",
        "        \"energy_net_kwh\": float(energy_net_kwh),\n",
        "        \"efficiency_joules_per_token\": float(joules_per_token)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Me85TJ8V_MiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukco86jjVz7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a008977b-127c-44fc-d803-768a16aea3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "metrics_base_carbon = measure_energy_consumption(model, tokenizer, dataloaderwiki,\n",
        "                                                 idle_power_watts=idle_watts,\n",
        "                                                 max_samples=MAX_SAMPLES_CARBON, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base_carbon"
      ],
      "metadata": {
        "id": "FzHRlIQNBtVK",
        "outputId": "189fd035-56f5-42c8-9757-042652e32552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'energy_net_kwh': 0.0001813826314681858,\n",
              " 'efficiency_joules_per_token': 0.26119098931418755}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}