{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/main/APPB/APPB_NB01_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEZwciT20NQP"
      },
      "source": [
        "# Tailoring LLM Architectures\n",
        "## Surgical Optimization Beyond Fine-Tuning\n",
        "\n",
        "\n",
        "### Appendxi B: Capabilities evaluation with lm-evaluation-harness\n",
        "### Notebook: 01. How to use `model_evaluation`\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white)](https://x.com/PereMartra) [![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* gemma-3-270m\n",
        "_____\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPY6ryBtUrHY"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q lm-eval transformers torch accelerate\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"google/gemma-3-270m\"\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"Model loaded on device: {model.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "20LM1TuWT9vP"
      },
      "outputs": [],
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, device='cuda', limit=None, batch_size=4, save_results=False):\n",
        "    \"\"\"\n",
        "    Runs evaluation tasks on a loaded PyTorch model using the lm-evaluation-harness.\n",
        "\n",
        "    This function wraps a pre-loaded model and tokenizer into an HFLM wrapper,\n",
        "    parses task configurations (supporting both simple strings and few-shot dicts),\n",
        "    and executes the evaluation. Tasks with different few-shot settings are\n",
        "    automatically grouped and evaluated separately. Results are post-processed\n",
        "    to return only the most relevant metrics (perplexity, accuracy, etc.).\n",
        "\n",
        "    Args:\n",
        "        model_obj (PreTrainedModel): The Hugging Face/PyTorch model object.\n",
        "        tokenizer (PreTrainedTokenizer): The associated tokenizer.\n",
        "        tasks (list[str | dict]): A list of tasks. Can be task name strings or\n",
        "            dicts with keys 'name' (str) and 'num_fewshot' (int).\n",
        "        device (str): Device to run evaluation on (e.g., 'cuda', 'cpu').\n",
        "            Defaults to 'cuda'.\n",
        "        limit (int, optional): Number of samples per task for quick testing.\n",
        "            If None, the full dataset is used.\n",
        "        batch_size (int): Batch size for the evaluator. Defaults to 4.\n",
        "        save_results (bool): If True, saves results to a JSON file with metadata.\n",
        "            Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        dict: A cleaned results dictionary where keys are task names and values\n",
        "            are nested dicts containing relevant metrics like 'accuracy',\n",
        "            'perplexity', or 'acc_norm'.\n",
        "\n",
        "    Example:\n",
        "        >>> tasks = [{\"name\": \"hellaswag\", \"num_fewshot\": 5}, \"wikitext\"]\n",
        "        >>> results = model_evaluation(model, tokenizer, tasks, save_results=True)\n",
        "    \"\"\"\n",
        "    print(f\"Starting lm-eval on model '{model_obj.config._name_or_path}' for tasks: {tasks}\")\n",
        "    from lm_eval import evaluator\n",
        "    from lm_eval.models.huggingface import HFLM\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(device)\n",
        "    )\n",
        "\n",
        "    # Parse tasks and group by num_fewshot for efficient evaluation\n",
        "    fewshot_groups = defaultdict(list)\n",
        "    task_fewshot_map = {}\n",
        "\n",
        "    for task in tasks:\n",
        "        if isinstance(task, dict):\n",
        "            task_name = task[\"name\"]\n",
        "            num_fewshot = task.get(\"num_fewshot\", 0)\n",
        "            fewshot_groups[num_fewshot].append(task_name)\n",
        "            task_fewshot_map[task_name] = num_fewshot\n",
        "        else:\n",
        "            # Backward compatibility: simple string list defaults to 0-shot\n",
        "            fewshot_groups[0].append(task)\n",
        "            task_fewshot_map[task] = 0\n",
        "\n",
        "    limit_str = f\"(limit={limit})\" if limit else \"(full dataset)\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Tasks grouped by few-shot: {dict(fewshot_groups)} {limit_str}\")\n",
        "    print(f\"Task-level few-shot config: {task_fewshot_map}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Run evaluation for each few-shot group\n",
        "    all_results = {}\n",
        "    for num_fewshot, task_list in fewshot_groups.items():\n",
        "        print(f\"Evaluating {len(task_list)} task(s) with {num_fewshot}-shot learning...\")\n",
        "        results = evaluator.simple_evaluate(\n",
        "            model=model_wrapper,\n",
        "            tasks=task_list,\n",
        "            num_fewshot=num_fewshot,\n",
        "            limit=limit,\n",
        "            device=str(device),\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        all_results.update(results[\"results\"])\n",
        "\n",
        "    # Define priority metrics with their formatting\n",
        "    PRIORITY_METRICS = {\n",
        "        'perplexity': (['perplexity,none', 'perplexity'], ':.2f'),\n",
        "        'word_perplexity': (['word_perplexity,none', 'word_perplexity'], ':.2f'),\n",
        "        'bits_per_byte': (['bits_per_byte,none', 'bits_per_byte'], ':.4f'),\n",
        "        'accuracy': (['acc,none', 'acc'], ':.4f'),\n",
        "        'acc_norm': (['acc_norm,none', 'acc_norm'], ':.4f'),\n",
        "        'f1': (['f1,none', 'f1'], ':.4f'),\n",
        "        'exact_match': (['exact_match,none', 'em'], ':.4f'),\n",
        "    }\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in all_results.items():\n",
        "        formatted_results[task_name] = {}\n",
        "\n",
        "        # Extract priority metrics with specific formatting\n",
        "        for metric_name, (possible_keys, fmt) in PRIORITY_METRICS.items():\n",
        "            for key in possible_keys:\n",
        "                if key in res:\n",
        "                    val = res[key]\n",
        "                    # Apply dynamic formatting using format() builtin\n",
        "                    formatted_results[task_name][metric_name] = format(val, fmt.strip(':'))\n",
        "                    break\n",
        "\n",
        "        # If no priority metrics found, fallback to all numeric metrics\n",
        "        if not formatted_results[task_name]:\n",
        "            formatted_results[task_name] = {\n",
        "                k: f\"{v:.4f}\" for k, v in res.items()\n",
        "                if isinstance(v, (int, float))\n",
        "            }\n",
        "\n",
        "    # Save results to JSON file if requested\n",
        "    if save_results:\n",
        "        import json\n",
        "        from datetime import datetime\n",
        "\n",
        "        # Prepare output with metadata\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        output_data = {\n",
        "            \"metadata\": {\n",
        "                \"model\": model_obj.config._name_or_path,\n",
        "                \"timestamp\": timestamp,\n",
        "                \"device\": str(device),\n",
        "                \"limit\": limit,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"tasks_config\": task_fewshot_map\n",
        "            },\n",
        "            \"results\": formatted_results\n",
        "        }\n",
        "\n",
        "        # Save to file with timestamp in filename\n",
        "        timestamp_file = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_file = f\"eval_results_{timestamp_file}.json\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(output_data, f, indent=2)\n",
        "        print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "    return formatted_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kibvVv5aUsPM"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "tasks = [\n",
        "    \"boolq\",  # 0-shot by default\n",
        "    {\"name\": \"piqa\", \"num_fewshot\": 5}  # 5-shot\n",
        "]\n",
        "\n",
        "tasks = [\n",
        "    \"boolq\",\n",
        "    {\"name\": \"piqa\", \"num_fewshot\": 5},\n",
        "    {\"name\": \"arc_easy\", \"num_fewshot\": 5},\n",
        "    {\"name\": \"arc_challenge\", \"num_fewshot\": 25},\n",
        "]\n",
        "\n",
        "results = model_evaluation(\n",
        "    model_obj=model,\n",
        "    tokenizer=tokenizer,\n",
        "    tasks=tasks,\n",
        "    device='cuda',\n",
        "    limit=100,\n",
        "    batch_size=4,\n",
        "    save_results=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwH_GHOmR6rx",
        "outputId": "1a17fe8a-4f5e-4510-a6f2-ed211509eecc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6500'},\n",
              " 'arc_easy': {'accuracy': '0.6000', 'acc_norm': '0.6600'},\n",
              " 'piqa': {'accuracy': '0.6700', 'acc_norm': '0.7100'},\n",
              " 'arc_challenge': {'accuracy': '0.2200', 'acc_norm': '0.2200'}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNulJ5MkVAZr",
        "outputId": "a8dc9f42-98ae-4efb-a90a-aef393ec1f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "boolq:\n",
            "  accuracy: 0.6500\n",
            "\n",
            "arc_easy:\n",
            "  accuracy: 0.6000\n",
            "  acc_norm: 0.6600\n",
            "\n",
            "piqa:\n",
            "  accuracy: 0.6700\n",
            "  acc_norm: 0.7100\n",
            "\n",
            "arc_challenge:\n",
            "  accuracy: 0.2200\n",
            "  acc_norm: 0.2200\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "for task_name, metrics in results.items():\n",
        "    print(f\"\\n{task_name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ukco86jjVz7g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNxbn2fI+qZEx1FHALl92Kj",
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
