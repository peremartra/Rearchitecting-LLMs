{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7E50A3FIQMgvS/o0Z5zO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH02/CH02_NB02_Knowledge_Recovery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rearchitecting LLMs**\n",
        "## **Surgical Optimization for Hyper-Efficient Models**\n",
        "\n",
        "### **Chapter 2: Knowledge Recovery via Distillation**\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "**Colab Environment:** GPU T4  \n",
        "- **Models recommended:** google/gemma-3-270m  \n",
        "- **Tested with:** meta-llama/Llama-3.2-1B using a **GPU A100**\n",
        "\n",
        "---\n",
        "\n",
        "Welcome to the second part of your first model rearchitecture. In the previous notebook, we successfully performed depth pruning by removing layers from our model, achieving significant efficiency gains but experiencing expected performance degradation.\n",
        "\n",
        "Now we'll complete the optimization cycle by recovering the lost knowledge through Knowledge Distillation (KD). This notebook demonstrates how a pruned model can learn to mimic the behavior of its original, unpruned version.\n",
        "\n",
        "**What we'll accomplish:**\n",
        "- **Load our models**: Original (teacher) and pruned (student) from the previous notebook\n",
        "- **Prepare recovery data**: Prepare a general-purpose dataset (SlimPajama) to facilitate the knowledge transfer.\n",
        "- **Apply Knowledge Distillation**: Train the pruned model to recover lost capabilities\n",
        "- **Measure recovery**: Quantify how much performance we can restore\n",
        "\n",
        "This notebook bridges the gap between \"breaking\" a model (pruning) and \"fixing\" it (recovery), completing your first end-to-end model tailoring workflow.\n",
        "\n",
        "**Previous notebook:** [CH02_NB01_Depth_pruning_evaluation](https://github.com/peremartra/Rearchitecting-LLMs/blob/main/CH02/CH02_NB01_Depth_pruning_evaluation.ipynb)\n",
        "**Connection:** We'll use the exact same models and evaluation framework to measure our recovery success.\n"
      ],
      "metadata": {
        "id": "7hULT_n96KoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0OhWq985W0i",
        "outputId": "f8266632-3b39-485a-e1d1-c15ec65a09ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Libraries\n",
        "# Install required packages\n",
        "!pip install -q transformers torch optipfair datasets accelerate sentencepiece lm-eval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "from optipfair import prune_model\n",
        "from datasets import load_dataset\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import time\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "import copy\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl-L82JK9BhG",
        "outputId": "7520cb7e-9063-496c-8542-ce0d27a1dc53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Libraries & Models."
      ],
      "metadata": {
        "id": "MRj--518-Vkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration (consistent with previous notebook)\n",
        "MODEL_NAME = \"google/gemma-3-270m\"\n",
        "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
        "MAX_NEW_TOKENS = 50\n",
        "LAYERS_TO_REMOVE = 2\n",
        "TEST_PROMPT = \"Paris is the capital of\"\n",
        "\n",
        "print(f\"Loading base model: {MODEL_NAME}\")\n",
        "\n",
        "# Load the original model (this will be our TEACHER)\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Clean generation config (same as previous notebook)\n",
        "from transformers import GenerationConfig\n",
        "clean_config = GenerationConfig(\n",
        "    max_length=teacher_model.generation_config.max_length,\n",
        "    pad_token_id=teacher_model.generation_config.pad_token_id,\n",
        "    eos_token_id=teacher_model.generation_config.eos_token_id,\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    early_stopping=False\n",
        ")\n",
        "teacher_model.generation_config = clean_config"
      ],
      "metadata": {
        "id": "0Js9yx_29Uv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_params = sum(p.numel() for p in teacher_model.parameters())\n",
        "print(f\"Teacher model parameters: {original_params:,}\")\n",
        "print(f\"Teacher model layers: {len(teacher_model.model.layers)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sNjhJ_X-LFJ",
        "outputId": "394bc079-3c09-48e0-892d-e34c44390811"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher model parameters: 268,098,176\n",
            "Teacher model layers: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Student Model.\n"
      ],
      "metadata": {
        "id": "heQC0dTP_UtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the STUDENT model by pruning (same process as previous notebook)\n",
        "print(f\"\\nCreating student model by removing {LAYERS_TO_REMOVE} layers...\")\n",
        "\n",
        "# Apply depth pruning using optipfair\n",
        "student_model = prune_model(\n",
        "    model=copy.deepcopy(teacher_model),\n",
        "    pruning_type=\"DEPTH\",\n",
        "    num_layers_to_remove=LAYERS_TO_REMOVE,\n",
        "    layer_selection_method=\"last\",\n",
        "    show_progress=True\n",
        ")"
      ],
      "metadata": {
        "id": "oPdWmAal-STI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_params = sum(p.numel() for p in student_model.parameters())\n",
        "param_reduction = (original_params - student_params) / original_params\n",
        "\n",
        "print(f\"Student model parameters: {student_params:,}\")\n",
        "print(f\"Parameter reduction: {param_reduction:.1%}\")\n",
        "print(f\"Student model layers: {len(student_model.model.layers)}\")\n",
        "\n",
        "student_model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpaF1YAm_Lnl",
        "outputId": "d9c5de0f-df19-4d6a-f019-649939bc3f02"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student model parameters: 256,950,912\n",
            "Parameter reduction: 4.2%\n",
            "Student model layers: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Functions & Basic Test"
      ],
      "metadata": {
        "id": "-CKJcRUE_1LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, limit=100):\n",
        "    \"\"\"\n",
        "    Runs lm-eval on a PyTorch model object already in memory.\n",
        "\n",
        "    Args:\n",
        "        model_obj: The PyTorch model object to evaluate.\n",
        "        tokenizer: The tokenizer object.\n",
        "        tasks (list): A list of task names.\n",
        "        limit (int): The number of samples per task.\n",
        "    \"\"\"\n",
        "    print(f\"Starting lm-eval on model '{model_obj.config._name_or_path}' for tasks: {tasks}\")\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(device)\n",
        "    )\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=model_wrapper,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        device=str(device),\n",
        "    )\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in results[\"results\"].items():\n",
        "        # Look for accuracy ('acc') first, then perplexity ('ppl')\n",
        "        if 'acc,none' in res:\n",
        "            metric_val = res.get('acc,none', 0)\n",
        "        elif 'ppl,none' in res:\n",
        "             metric_val = res.get('ppl,none', 0)\n",
        "        else:\n",
        "            metric_val = 0 # Fallback\n",
        "\n",
        "        formatted_results[task_name] = f\"{metric_val:.4f}\"\n",
        "\n",
        "    print(json.dumps(formatted_results, indent=2))\n",
        "    return formatted_results"
      ],
      "metadata": {
        "id": "8OWwlMgJHw49"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick baseline test - confirm degradation from previous notebook\n",
        "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
        "    \"\"\"Generate text with the model (same function as previous notebook)\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test both models with the same prompt\n",
        "print(f\"\\n--- Baseline Test: '{TEST_PROMPT}' ---\")\n",
        "teacher_output = generate_text(teacher_model, tokenizer, TEST_PROMPT)\n",
        "student_output = generate_text(student_model, tokenizer, TEST_PROMPT)\n",
        "\n",
        "print(f\"Teacher: '{teacher_output}'\")\n",
        "print(f\"Student: '{student_output}'\")\n",
        "print(\"\\nReady for knowledge recovery...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIFKAxxc_wTw",
        "outputId": "d66c82a1-174c-4168-9fe7-7da77450935d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Baseline Test: 'Paris is the capital of' ---\n",
            "Teacher: 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n",
            "Student: 'Paris is the capital of France and one of the largest cities in Europe. It occupies approximately 2.5 million hectares of land surrounded by mountains and forests. Parisians love to travel abroad because they enjoy sightseeing tours abroad. Tourists visiting Paris visit museums, monuments, theaters,'\n",
            "\n",
            "Ready for knowledge recovery...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Knowledge Distillation Process\n",
        "\n",
        "Before we begin the recovery process, let's review the performance impact from our depth pruning (results from the previous notebook):\n",
        "\n",
        "### Gemma-3-270m: Depth Pruning Impact\n",
        "\n",
        "| Metric | Original Model | Pruned Model (-2 Layers) | Change |\n",
        "|:-------|:---------------|:-------------------------|:-------|\n",
        "| **Parameters** | 268,098,176 | 256,950,912 | **-4.16%** |\n",
        "| **Inference Time** | 10.324s | 4.322s | **+58.1%** |\n",
        "| **arc_easy** (acc) | 0.5500 | 0.4600 | -16.36% |\n",
        "| **winogrande** (acc) | 0.6000 | 0.4800 | -20.00% |\n",
        "| **boolq** (acc) | 0.6600 | 0.4500 | -31.82% |\n",
        "| **lambada_openai** (acc) | 0.4200 | 0.3400 | -19.05% |\n",
        "\n",
        "**Trade-off**: 58.1% faster inference + 4.16% parameter reduction vs. accuracy degradation across benchmarks.\n",
        "\n",
        "### LLaMA-3.2-1B: Depth Pruning Impact\n",
        "\n",
        "| Metric | Original Model | Pruned Model (-2 Layers) | Change |\n",
        "|:-------|:---------------|:-------------------------|:-------|\n",
        "| **Parameters** | 1,235,814,400 | 1,117,171,392 | **-9.84%** |\n",
        "| **Inference Time** | 6.635s | 4.752s | **+28.4%** |\n",
        "| **arc_easy** (acc) | 0.6600 | 0.4800 | -27.27% |\n",
        "| **winogrande** (acc) | 0.6000 | 0.5400 | -10.00% |\n",
        "| **boolq** (acc) | 0.6700 | 0.7000 | **+4.48%** |\n",
        "| **lambada_openai** (acc) | 0.5700 | 0.1700 | -70.18% |\n",
        "\n",
        "**Interesting observation**: LLaMA-3.2-1B shows a curious **improvement** in boolq (+4.48%) after pruning, suggesting some layers may have been introducing noise for this specific task.\n",
        "\n",
        "**Our KD goal**: Recover the lost performance while maintaining the efficiency gains from pruning.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "53R5rPihCMI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n"
      ],
      "metadata": {
        "id": "EJQW3EJ4Csol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SlimPajama dataset in streaming mode for efficiency\n",
        "print(\"Loading SlimPajama-627B dataset...\")\n",
        "dataset = load_dataset(\n",
        "    \"cerebras/SlimPajama-627B\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Take a representative subset for our recovery process\n",
        "#    You can reduce the number os samples to reduce the execution time\n",
        "#    but you'll see some reduction in lambada benchmark\n",
        "#    due to small number of examples\n",
        "RECOVERY_SAMPLES = 500\n",
        "print(f\"Selecting {RECOVERY_SAMPLES:,} samples for knowledge recovery...\")\n",
        "\n",
        "# Use streaming dataset's take method - much more efficient!\n",
        "distillation_dataset = dataset.take(RECOVERY_SAMPLES)\n",
        "\n",
        "print(f\"✓ Streaming dataset ready: {RECOVERY_SAMPLES:,} samples\")"
      ],
      "metadata": {
        "id": "UBelN4lH_8Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_for_kd(examples, max_length=128):\n",
        "    \"\"\"\n",
        "    Tokenize text for Knowledge Distillation.\n",
        "    Shorter sequences work better for KD as both teacher and student\n",
        "    need to fit in memory simultaneously.\n",
        "    \"\"\"\n",
        "    if isinstance(examples, dict):\n",
        "        texts = examples[\"text\"]\n",
        "    else:\n",
        "        texts = [ex[\"text\"] for ex in examples]\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # For language modeling, labels = input_ids\n",
        "    return {\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": tokenized[\"input_ids\"].clone()\n",
        "    }\n",
        "\n",
        "# Process the recovery dataset\n",
        "print(\"Tokenizing recovery dataset...\")\n",
        "\n",
        "# Convert streaming dataset to list for tokenization\n",
        "recovery_samples = list(distillation_dataset)\n",
        "tokenized_data = tokenize_for_kd(recovery_samples)\n",
        "\n",
        "print(f\"✓ Tokenized {len(recovery_samples):,} samples\")\n",
        "\n",
        "# Convert to format suitable for DataLoader\n",
        "class KDDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "kd_dataset = KDDataset(\n",
        "    tokenized_data[\"input_ids\"],\n",
        "    tokenized_data[\"attention_mask\"],\n",
        "    tokenized_data[\"labels\"]\n",
        ")\n",
        "\n",
        "# Create DataLoader for training\n",
        "kd_dataloader = DataLoader(\n",
        "    kd_dataset,\n",
        "    batch_size=4,  # Small batch size due to memory constraints with two models\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"✓ Knowledge Distillation DataLoader ready: {len(kd_dataloader)} batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VELWqP8NEd3z",
        "outputId": "e27ced2c-907b-4860-92ba-1c4d22526d33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing recovery dataset...\n",
            "✓ Tokenized 500 samples\n",
            "✓ Knowledge Distillation DataLoader ready: 125 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3GQAfAbdEdLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move models to device and set appropriate modes\n",
        "teacher_model.to(device)\n",
        "student_model.to(device)\n",
        "\n",
        "# Teacher stays in eval mode - we don't train it\n",
        "teacher_model.eval()\n",
        "\n",
        "# Student will be trained\n",
        "student_model.train()\n",
        "\n",
        "# KD Hyperparameters\n",
        "TEMPERATURE = 2.0      # Softens probability distributions\n",
        "ALPHA = 1.0           # Weight for distillation loss\n",
        "NUM_EPOCHS = 3        # Conservative for demo\n",
        "LEARNING_RATE = 1e-5  # Lower LR for stability\n",
        "ACCUMULATION_STEPS = 4  # Effective batch size = 4 * 8 = 32\n",
        "\n",
        "# Optimizer for student model only\n",
        "optimizer = AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"Knowledge Distillation Configuration:\")\n",
        "print(f\"  Temperature: {TEMPERATURE}\")\n",
        "print(f\"  Alpha: {ALPHA}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Effective Batch Size: {4 * ACCUMULATION_STEPS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC38ykgpCuy9",
        "outputId": "79f1c0c0-d845-49af-c95f-78a796e557e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Distillation Configuration:\n",
            "  Temperature: 2.0\n",
            "  Alpha: 1.0\n",
            "  Epochs: 3\n",
            "  Learning Rate: 1e-05\n",
            "  Effective Batch Size: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Test generation."
      ],
      "metadata": {
        "id": "jww6LbeRHC1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set student model to evaluation mode\n",
        "teacher_model.to(device)\n",
        "student_model.eval()\n",
        "# Test with the same prompt used in baseline\n",
        "print(f\"--- Qualitative Test: '{TEST_PROMPT}' ---\")\n",
        "\n",
        "# Generate with all three models for comparison\n",
        "teacher_output = generate_text(teacher_model, tokenizer, TEST_PROMPT)\n",
        "student_baseline_output = generate_text(student_model, tokenizer, TEST_PROMPT)\n",
        "\n",
        "print(f\"Teacher (Original):    '{teacher_output}'\")\n",
        "print(f\"Student (Post-KD):     '{student_baseline_output}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W0-J7MuFqiO",
        "outputId": "7a116dfa-fa18-471c-886b-e28a6890f03d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Qualitative Test: 'Paris is the capital of' ---\n",
            "Teacher (Original):    'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n",
            "Student (Post-KD):     'Paris is the capital of France and one of the largest cities in Europe. It occupies approximately 2.5 million hectares of land surrounded by mountains and forests. Parisians love to travel abroad because they enjoy sightseeing tours abroad. Tourists visiting Paris visit museums, monuments, theaters,'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "-3Wi3YX5H9nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the benchmark suite for our diagnostic\n",
        "benchmark_tasks = ['arc_easy', 'winogrande', 'boolq', 'lambada_openai']\n",
        "student_recovered_results = model_evaluation(student_model, tokenizer, benchmark_tasks, limit=100)"
      ],
      "metadata": {
        "id": "xhPEAh9LHWl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Knowledge Recovery Analysis\n",
        "\n",
        "### Gemma-3-270m: Full Recovery Progression\n",
        "\n",
        "| Metric | Teacher (Original) | Student (Pruned) | Student (500 samples) | Student (15K samples) | Final Performance |\n",
        "|:-------|:-------------------|:-----------------|:---------------------|:---------------------|:------------------|\n",
        "| **Parameters** | 268,098,176 | 256,950,912 | 256,950,912 | 256,950,912 | **Maintained** |\n",
        "| **arc_easy** (acc) | 0.5500 | 0.4600 | 0.4700 | **0.5300** | **96.4%** of original |\n",
        "| **winogrande** (acc) | 0.6000 | 0.4800 | 0.5800 | 0.5700 | **95.0%** of original |\n",
        "| **boolq** (acc) | 0.6600 | 0.4500 | 0.5300 | 0.5300 | **80.3%** of original |\n",
        "| **lambada_openai** (acc) | 0.4200 | 0.3400 | 0.2600 | **0.3600** | **85.7%** of original |\n",
        "\n",
        "### Impact of Dataset Size on Recovery:\n",
        "\n",
        "**Major Improvements with 15K samples:**\n",
        "- **arc_easy**: 0.47 → **0.53** (+0.06, now 96.4% of original performance)\n",
        "- **lambada_openai**: 0.26 → **0.36** (+0.10, reached 85.7% of original)\n",
        "\n",
        "**Stable Performance:**\n",
        "- **boolq**: Maintained at 0.53 (80.3% of original)\n",
        "- **winogrande**: 0.58 → 0.57 (95.0% of original, excellent retention)\n",
        "\n",
        "### Key Dataset Size Insights:\n",
        "\n",
        "**500 vs 15K samples comparison:**\n",
        "- **More data = Better recovery** for complex reasoning tasks (arc_easy, lambada)\n",
        "- **Diminishing returns** for some tasks (boolq plateaued)\n",
        "- **winogrande** achieved excellent recovery even with 500 samples\n",
        "\n",
        "### Overall Assessment:\n",
        "\n",
        "**Outstanding Results with 15K samples:**\n",
        "- **ALL benchmarks recovered 80%+ of original performance**\n",
        "- **arc_easy** and **winogrande** nearly fully recovered (95%+)\n",
        "- **Model efficiency maintained** (4.16% parameter reduction + 58.1% inference speedup)\n",
        "\n",
        "**Key Takeaway:** Knowledge Distillation successfully restored most capabilities while preserving all efficiency gains. With adequate training data, a pruned model can recover 85-96% of its original performance across diverse reasoning tasks."
      ],
      "metadata": {
        "id": "iAKuPGXmUTlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Knowledge Recovery Analysis\n",
        "\n",
        "### LLaMA-3.2-1B: Full Recovery Progression\n",
        "\n",
        "| Metric | Teacher (Original) | Student (Pruned) | Student (500 samples) | Final Performance |\n",
        "|:-------|:-------------------|:-----------------|:---------------------|:------------------|\n",
        "| **Parameters** | 1,235,814,400 | 1,117,171,392 | 1,117,171,392 | **Maintained** |\n",
        "| **arc_easy** (acc) | 0.6600 | 0.4800 | **0.5800** | **87.9%** of original |\n",
        "| **winogrande** (acc) | 0.6000 | 0.5400 | 0.5500 | **91.7%** of original |\n",
        "| **boolq** (acc) | 0.6700 | 0.7000 | **0.5400** | **80.6%** of original |\n",
        "| **lambada_openai** (acc) | 0.5700 | 0.1700 | 0.1800 | **31.6%** of original |\n",
        "\n",
        "### Recovery Analysis:\n",
        "\n",
        "**Strong Recovery Performance:**\n",
        "- **winogrande**: 0.54 → 0.55 (+0.01, maintained 91.7% of original)\n",
        "- **arc_easy**: 0.48 → **0.58** (+0.10, recovered to 87.9% of original)\n",
        "\n",
        "**Moderate Recovery:**\n",
        "- **boolq**: 0.70 → 0.54 (-0.16, normalized to 80.6% of original)\n",
        "\n",
        "**Challenge Area:**\n",
        "- **lambada_openai**: 0.17 → 0.18 (+0.01, minimal recovery to 31.6% of original)\n",
        "\n",
        "### Key LLaMA-3.2-1B Insights:\n",
        "\n",
        "**Interesting Observations:**\n",
        "- **boolq paradox resolved**: The pruned model's anomalous improvement (0.67 → 0.70) was corrected during KD, returning to a more realistic 0.54\n",
        "- **arc_easy excellent recovery**: Strong improvement from severe pruning degradation\n",
        "- **lambada remains challenging**: This complex reasoning task shows the deepest impact from layer removal\n",
        "\n",
        "### Overall Assessment:\n",
        "\n",
        "**Results with 500 samples:**\n",
        "- **3 out of 4 benchmarks** show strong performance (80%+ of original)\n",
        "- **Model efficiency maintained** (9.84% parameter reduction + 28.4% inference speedup)\n",
        "- **boolq correction** suggests KD helps normalize anomalous pruning effects\n",
        "\n",
        "**Key Takeaway:** Even with limited training data (500 samples), Knowledge Distillation effectively recovered most capabilities in LLaMA-3.2-1B, with particularly strong results in reasoning tasks like arc_easy and winogrande."
      ],
      "metadata": {
        "id": "xPOu-8o5rJN5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lcbt3SFEICMD"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}