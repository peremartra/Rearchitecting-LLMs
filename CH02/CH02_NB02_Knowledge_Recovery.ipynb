{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/main/CH02/CH02_NB02_Knowledge_Recovery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hULT_n96KoD"
      },
      "source": [
        "# **Tailoring LLM Architectures**\n",
        "## **Surgical Optimization Beyond Fine-Tuning**\n",
        "\n",
        "### **Chapter 2: Knowledge Recovery via Distillation**\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "\n",
        "___\n",
        "\n",
        "**Colab Environment:** GPU T4  \n",
        "- **Models recommended:** google/gemma-3-270m  \n",
        "- **Tested with:** meta-llama/Llama-3.2-1B using a **GPU A100**\n",
        "\n",
        "---\n",
        "\n",
        "Welcome to the second part of your first model rearchitecture. In the previous notebook, we successfully performed depth pruning by removing layers from our model, achieving significant efficiency gains but experiencing expected performance degradation.\n",
        "\n",
        "Now we'll complete the optimization cycle by recovering the lost knowledge through Knowledge Distillation (KD). This notebook demonstrates how a pruned model can learn to mimic the behavior of its original, unpruned version.\n",
        "\n",
        "**What we'll accomplish:**\n",
        "- **Load our models**: Original (teacher) and pruned (student) from the previous notebook\n",
        "- **Prepare recovery data**: Prepare a general-purpose dataset (SlimPajama) to facilitate the knowledge transfer.\n",
        "- **Apply Knowledge Distillation**: Train the pruned model to recover lost capabilities\n",
        "- **Measure recovery**: Quantify how much performance we can restore\n",
        "\n",
        "This notebook bridges the gap between \"breaking\" a model (pruning) and \"fixing\" it (recovery), completing your first end-to-end model tailoring workflow.\n",
        "\n",
        "**Previous notebook:** [CH02_NB01_Depth_pruning_evaluation](https://github.com/peremartra/Rearchitecting-LLMs/blob/main/CH02/CH02_NB01_Depth_pruning_evaluation.ipynb)\n",
        "**Connection:** We'll use the exact same models and evaluation framework to measure our recovery success.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d0OhWq985W0i"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "# Install required packages\n",
        "!pip install -q transformers torch optipfair datasets accelerate sentencepiece lm-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl-L82JK9BhG",
        "outputId": "6c52b687-4d2f-4eb2-e87b-d63fd5371492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "from optipfair import prune_model\n",
        "from datasets import load_dataset\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any\n",
        "import copy, random, os\n",
        "from transformers import set_seed\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_reproducibility(seed=42):\n",
        "    # 1. Semilla para Python y librerÃ­as bÃ¡sicas\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 2. Semilla para PyTorch (CPU y GPU)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # Para multi-GPU\n",
        "\n",
        "    # 3. Configurar cuDNN para que sea determinista\n",
        "    # Nota: Esto puede ralentizar un poco el entrenamiento\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 4. Semilla para Transformers (Hugging Face)\n",
        "    set_seed(seed)\n",
        "\n",
        "    print(f\"âœ… Seed {seed} stablished.\")\n",
        "\n",
        "set_reproducibility(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZJVl3Y0tRju",
        "outputId": "0b6d581c-6d14-461a-c0b9-249aed4b2146"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Seed 42 stablished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRscks9QSEqt"
      },
      "source": [
        "# 2.4 Recovering knowledge with distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRj--518-Vkv"
      },
      "source": [
        "## Load Libraries & Models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0Js9yx_29Uv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeae5599-07ae-4364-b61d-bfd164c19dd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: google/gemma-3-270m\n"
          ]
        }
      ],
      "source": [
        "# Model configuration (consistent with previous notebook)\n",
        "MODEL_NAME = \"google/gemma-3-270m\"\n",
        "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
        "MAX_NEW_TOKENS = 50\n",
        "LAYERS_TO_REMOVE = 2 #Try removing 4, 6, or even 8 layers\n",
        "TEST_PROMPT = \"Paris is the capital of\"\n",
        "\n",
        "print(f\"Loading base model: {MODEL_NAME}\")\n",
        "\n",
        "# Load the original model (this will be our TEACHER)\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Clean generation config (same as previous notebook)\n",
        "from transformers import GenerationConfig\n",
        "clean_config = GenerationConfig(\n",
        "    max_length=teacher_model.generation_config.max_length,\n",
        "    pad_token_id=teacher_model.generation_config.pad_token_id,\n",
        "    eos_token_id=teacher_model.generation_config.eos_token_id,\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    early_stopping=False\n",
        ")\n",
        "teacher_model.generation_config = clean_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sNjhJ_X-LFJ",
        "outputId": "b4e6f9dd-29b0-4367-f0d6-c96e53acb7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher model parameters: 268,098,176\n",
            "Teacher model layers: 18\n"
          ]
        }
      ],
      "source": [
        "original_params = sum(p.numel() for p in teacher_model.parameters())\n",
        "print(f\"Teacher model parameters: {original_params:,}\")\n",
        "print(f\"Teacher model layers: {len(teacher_model.model.layers)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heQC0dTP_UtK"
      },
      "source": [
        "### Create Student Model with optiPfair\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPdWmAal-STI",
        "outputId": "809257c3-99c8-427e-80b5-dded5b4e0201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating student model by removing 2 layers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 202950.19it/s]\n"
          ]
        }
      ],
      "source": [
        "# Create the STUDENT model by pruning (same process as previous notebook)\n",
        "print(f\"\\nCreating student model by removing {LAYERS_TO_REMOVE} layers...\")\n",
        "\n",
        "# Apply depth pruning using optipfair\n",
        "student_model = prune_model(\n",
        "    model=copy.deepcopy(teacher_model),\n",
        "    pruning_type=\"DEPTH\",\n",
        "    num_layers_to_remove=LAYERS_TO_REMOVE,\n",
        "    layer_selection_method=\"last\",\n",
        "    show_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpaF1YAm_Lnl",
        "outputId": "9c82b4a2-d38f-4d2c-f1bc-6a213a9ac568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student model parameters: 256,950,912\n",
            "Parameter reduction: 4.2%\n",
            "Student model layers: 16\n"
          ]
        }
      ],
      "source": [
        "student_params = sum(p.numel() for p in student_model.parameters())\n",
        "param_reduction = (original_params - student_params) / original_params\n",
        "\n",
        "print(f\"Student model parameters: {student_params:,}\")\n",
        "print(f\"Parameter reduction: {param_reduction:.1%}\")\n",
        "print(f\"Student model layers: {len(student_model.model.layers)}\")\n",
        "\n",
        "student_model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKJcRUE_1LK"
      },
      "source": [
        "## Support Functions & Basic Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8OWwlMgJHw49"
      },
      "outputs": [],
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, limit=100):\n",
        "    \"\"\"\n",
        "    Runs lm-eval on a PyTorch model object already in memory.\n",
        "\n",
        "    Args:\n",
        "        model_obj: The PyTorch model object to evaluate.\n",
        "        tokenizer: The tokenizer object.\n",
        "        tasks (list): A list of task names.\n",
        "        limit (int): The number of samples per task.\n",
        "    \"\"\"\n",
        "    print(f\"Starting lm-eval on model '{model_obj.config._name_or_path}' for tasks: {tasks}\")\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(device)\n",
        "    )\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=model_wrapper,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        device=str(device),\n",
        "    )\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in results[\"results\"].items():\n",
        "        # Look for accuracy ('acc') first, then perplexity ('ppl')\n",
        "        if 'acc,none' in res:\n",
        "            metric_val = res.get('acc,none', 0)\n",
        "        elif 'ppl,none' in res:\n",
        "             metric_val = res.get('ppl,none', 0)\n",
        "        else:\n",
        "            metric_val = 0 # Fallback\n",
        "\n",
        "        formatted_results[task_name] = f\"{metric_val:.4f}\"\n",
        "\n",
        "    print(json.dumps(formatted_results, indent=2))\n",
        "    return formatted_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIFKAxxc_wTw",
        "outputId": "a0fc4764-4fce-46bd-a190-dc785bc539ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Baseline Test: 'Paris is the capital of' ---\n",
            "Teacher: 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n",
            "Student: 'Paris is the capital of France and one of the largest cities in Europe. It occupies approximately 2.5 million hectares of land surrounded by mountains and forests. Parisians love to travel abroad because they enjoy sightseeing tours abroad. Tourists visiting Paris visit museums, monuments, theaters,'\n",
            "\n",
            "Ready for knowledge recovery...\n"
          ]
        }
      ],
      "source": [
        "# Quick baseline test - confirm degradation from previous notebook\n",
        "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
        "    \"\"\"Generate text with the model (same function as previous notebook)\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test both models with the same prompt\n",
        "print(f\"\\n--- Baseline Test: '{TEST_PROMPT}' ---\")\n",
        "teacher_output = generate_text(teacher_model, tokenizer, TEST_PROMPT)\n",
        "student_output = generate_text(student_model, tokenizer, TEST_PROMPT)\n",
        "\n",
        "print(f\"Teacher: '{teacher_output}'\")\n",
        "print(f\"Student: '{student_output}'\")\n",
        "print(\"\\nReady for knowledge recovery...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53R5rPihCMI4"
      },
      "source": [
        "##Knowledge Distillation Process\n",
        "\n",
        "Before we begin the recovery process, let's review the performance impact from our depth pruning (results from the previous notebook):\n",
        "\n",
        "### Gemma-3-270m: Depth Pruning Impact\n",
        "\n",
        "| Metric | Original Model | Pruned Model (-2 Layers) | Pruned Model (-3 Layers) | Change (-2L) | Change (-3L) |\n",
        "|:-------|:---------------|:-------------------------|:-------------------------|:-------------|:-------------|\n",
        "| **Parameters** | 268,098,176 | 256,950,912 | 251,377,280 | **-4.16%** | **-6.24%** |\n",
        "| **Inference Time** | 10.324s | 4.322s | - | **+58.1%** | - |\n",
        "| **arc_easy** (acc) | 0.5500 | 0.4600 | 0.3800 | -16.36% | -30.91% |\n",
        "| **winogrande** (acc) | 0.6000 | 0.4800 | 0.5400 | -20.00% | -10.00% |\n",
        "| **boolq** (acc) | 0.6600 | 0.4500 | 0.7000 | -31.82% | **+6.06%** |\n",
        "| **lambada_openai** (acc) | 0.4200 | 0.3400 | 0.1800 | -19.05% | -57.14% |\n",
        "\n",
        "**Trade-off**: 58.1% faster inference + 4.16% parameter reduction vs. accuracy degradation across benchmarks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC9kiV2cTwy_"
      },
      "source": [
        "### LLaMA-3.2-1B: Depth Pruning Impact\n",
        "\n",
        "| Metric | Original Model | Pruned Model (-2 Layers) | Change |\n",
        "|:-------|:---------------|:-------------------------|:-------|\n",
        "| **Parameters** | 1,235,814,400 | 1,117,171,392 | **-9.84%** |\n",
        "| **Inference Time** | 6.635s | 4.752s | **+28.4%** |\n",
        "| **arc_easy** (acc) | 0.6600 | 0.4800 | -27.27% |\n",
        "| **winogrande** (acc) | 0.6000 | 0.5400 | -10.00% |\n",
        "| **boolq** (acc) | 0.6700 | 0.7000 | **+4.48%** |\n",
        "| **lambada_openai** (acc) | 0.5700 | 0.1700 | -70.18% |\n",
        "\n",
        "**Interesting observation**: LLaMA-3.2-1B shows a curious **improvement** in boolq (+4.48%) after pruning, suggesting some layers may have been introducing noise for this specific task.\n",
        "\n",
        "**Our KD goal**: Recover the lost performance while maintaining the efficiency gains from pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJQW3EJ4Csol"
      },
      "source": [
        "## Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBelN4lH_8Um"
      },
      "outputs": [],
      "source": [
        "# Load SlimPajama dataset in streaming mode for efficiency\n",
        "print(\"Loading SlimPajama-627B dataset...\")\n",
        "dataset = load_dataset(\n",
        "    \"cerebras/SlimPajama-627B\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Take a representative subset for our recovery process\n",
        "#    You can reduce the number os samples to reduce the execution time\n",
        "#    but you'll see some reduction in lambada benchmark\n",
        "#    due to small number of examples\n",
        "RECOVERY_SAMPLES = 15000\n",
        "print(f\"Selecting {RECOVERY_SAMPLES:,} samples for knowledge recovery...\")\n",
        "\n",
        "# Use streaming dataset's take method - much more efficient!\n",
        "distillation_dataset = dataset.take(RECOVERY_SAMPLES)\n",
        "\n",
        "print(f\"âœ“ Streaming dataset ready: {RECOVERY_SAMPLES:,} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VELWqP8NEd3z",
        "outputId": "fd746bd7-f670-443f-9f90-f699c94f14c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing recovery dataset...\n",
            "âœ“ Tokenized 15,000 samples\n",
            "âœ“ Knowledge Distillation DataLoader ready: 1875 batches\n"
          ]
        }
      ],
      "source": [
        "def tokenize_for_kd(examples, max_length=128):\n",
        "    \"\"\"\n",
        "    Tokenize text for Knowledge Distillation.\n",
        "    Shorter sequences work better for KD as both teacher and student\n",
        "    need to fit in memory simultaneously.\n",
        "    \"\"\"\n",
        "    if isinstance(examples, dict):\n",
        "        texts = examples[\"text\"]\n",
        "    else:\n",
        "        texts = [ex[\"text\"] for ex in examples]\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # For language modeling, labels = input_ids\n",
        "    return {\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": tokenized[\"input_ids\"].clone()\n",
        "    }\n",
        "\n",
        "# Process the recovery dataset\n",
        "print(\"Tokenizing recovery dataset...\")\n",
        "\n",
        "# Convert streaming dataset to list for tokenization\n",
        "recovery_samples = list(distillation_dataset)\n",
        "tokenized_data = tokenize_for_kd(recovery_samples)\n",
        "\n",
        "print(f\"âœ“ Tokenized {len(recovery_samples):,} samples\")\n",
        "\n",
        "# Convert to format suitable for DataLoader\n",
        "class KDDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "kd_dataset = KDDataset(\n",
        "    tokenized_data[\"input_ids\"],\n",
        "    tokenized_data[\"attention_mask\"],\n",
        "    tokenized_data[\"labels\"]\n",
        ")\n",
        "\n",
        "# Create DataLoader for training\n",
        "kd_dataloader = DataLoader(\n",
        "    kd_dataset,\n",
        "    batch_size=8,  # Small batch size due to memory constraints with two models\n",
        "    shuffle=True,\n",
        "    generator=g\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Knowledge Distillation DataLoader ready: {len(kd_dataloader)} batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC38ykgpCuy9",
        "outputId": "8ed93ca3-465f-4d80-dcd5-65b3ce705382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Distillation Configuration:\n",
            "  Temperature: 2.0\n",
            "  Alpha: 1.0\n",
            "  Epochs: 3\n",
            "  Learning Rate: 1e-05\n",
            "  Effective Batch Size: 16\n"
          ]
        }
      ],
      "source": [
        "# Move models to device and set appropriate modes\n",
        "teacher_model.to(device)\n",
        "student_model.to(device)\n",
        "\n",
        "# Teacher stays in eval mode - we don't train it\n",
        "teacher_model.eval()\n",
        "\n",
        "# Student will be trained\n",
        "student_model.train()\n",
        "\n",
        "# KD Hyperparameters\n",
        "TEMPERATURE = 2.0      # Softens probability distributions\n",
        "ALPHA = 1.0           # Weight for distillation loss\n",
        "NUM_EPOCHS = 3        # Conservative for demo\n",
        "LEARNING_RATE = 1e-5  # Lower LR for stability\n",
        "ACCUMULATION_STEPS = 4  # Effective batch size = 4 * 8 = 32\n",
        "\n",
        "# Optimizer for student model only\n",
        "optimizer = AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"Knowledge Distillation Configuration:\")\n",
        "print(f\"  Temperature: {TEMPERATURE}\")\n",
        "print(f\"  Alpha: {ALPHA}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Effective Batch Size: {4 * ACCUMULATION_STEPS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKAKi0K8B95H",
        "outputId": "d705c3af-52c7-4b4c-b081-3d379a442257"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸŽ“ Starting Knowledge Distillation Training...\n",
            "Training student model to mimic teacher behavior \n",
            "\n",
            "Epoch 1/3 | Batch 100 | Loss: 94.1395\n",
            "Epoch 1/3 | Batch 200 | Loss: 71.8141\n",
            "Epoch 1/3 | Batch 300 | Loss: 62.3443\n",
            "Epoch 1/3 | Batch 400 | Loss: 55.9029\n",
            "Epoch 1/3 | Batch 500 | Loss: 51.7971\n",
            "Epoch 1/3 | Batch 600 | Loss: 48.4098\n",
            "Epoch 1/3 | Batch 700 | Loss: 45.9407\n",
            "Epoch 1/3 | Batch 800 | Loss: 43.7809\n",
            "Epoch 1/3 | Batch 900 | Loss: 42.0326\n",
            "Epoch 1/3 | Batch 1000 | Loss: 40.5638\n",
            "Epoch 1/3 | Batch 1100 | Loss: 39.2979\n",
            "Epoch 1/3 | Batch 1200 | Loss: 38.1127\n",
            "Epoch 1/3 | Batch 1300 | Loss: 37.1784\n",
            "Epoch 1/3 | Batch 1400 | Loss: 36.2236\n",
            "Epoch 1/3 | Batch 1500 | Loss: 35.3580\n",
            "Epoch 1/3 | Batch 1600 | Loss: 34.6001\n",
            "Epoch 1/3 | Batch 1700 | Loss: 33.9242\n",
            "Epoch 1/3 | Batch 1800 | Loss: 33.2808\n",
            "Epoch 1/3 | Average Loss: 32.8369\n",
            "Epoch 2/3 | Batch 100 | Loss: 20.8116\n",
            "Epoch 2/3 | Batch 200 | Loss: 20.6427\n",
            "Epoch 2/3 | Batch 300 | Loss: 20.4804\n",
            "Epoch 2/3 | Batch 400 | Loss: 20.4459\n",
            "Epoch 2/3 | Batch 500 | Loss: 20.3209\n",
            "Epoch 2/3 | Batch 600 | Loss: 20.1816\n",
            "Epoch 2/3 | Batch 700 | Loss: 20.0853\n",
            "Epoch 2/3 | Batch 800 | Loss: 19.9855\n",
            "Epoch 2/3 | Batch 900 | Loss: 19.9146\n",
            "Epoch 2/3 | Batch 1000 | Loss: 19.8657\n",
            "Epoch 2/3 | Batch 1100 | Loss: 19.8048\n",
            "Epoch 2/3 | Batch 1200 | Loss: 19.7607\n",
            "Epoch 2/3 | Batch 1300 | Loss: 19.6482\n",
            "Epoch 2/3 | Batch 1400 | Loss: 19.5217\n",
            "Epoch 2/3 | Batch 1500 | Loss: 19.4515\n",
            "Epoch 2/3 | Batch 1600 | Loss: 19.3433\n",
            "Epoch 2/3 | Batch 1700 | Loss: 19.2940\n",
            "Epoch 2/3 | Batch 1800 | Loss: 19.1862\n",
            "Epoch 2/3 | Average Loss: 19.1395\n",
            "Epoch 3/3 | Batch 100 | Loss: 16.6357\n",
            "Epoch 3/3 | Batch 200 | Loss: 16.6734\n",
            "Epoch 3/3 | Batch 300 | Loss: 16.6059\n",
            "Epoch 3/3 | Batch 400 | Loss: 16.5015\n",
            "Epoch 3/3 | Batch 500 | Loss: 16.4623\n",
            "Epoch 3/3 | Batch 600 | Loss: 16.5194\n",
            "Epoch 3/3 | Batch 700 | Loss: 16.5548\n",
            "Epoch 3/3 | Batch 800 | Loss: 16.5395\n",
            "Epoch 3/3 | Batch 900 | Loss: 16.5488\n",
            "Epoch 3/3 | Batch 1000 | Loss: 16.4631\n",
            "Epoch 3/3 | Batch 1100 | Loss: 16.4458\n",
            "Epoch 3/3 | Batch 1200 | Loss: 16.3927\n",
            "Epoch 3/3 | Batch 1300 | Loss: 16.2996\n",
            "Epoch 3/3 | Batch 1400 | Loss: 16.2433\n",
            "Epoch 3/3 | Batch 1500 | Loss: 16.2277\n",
            "Epoch 3/3 | Batch 1600 | Loss: 16.1686\n",
            "Epoch 3/3 | Batch 1700 | Loss: 16.1285\n",
            "Epoch 3/3 | Batch 1800 | Loss: 16.0710\n",
            "Epoch 3/3 | Average Loss: 16.0383\n",
            "\n",
            "ðŸŽ‰ Knowledge Distillation completed!\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nðŸŽ“ Starting Knowledge Distillation Training...\")\n",
        "print(f\"Training student model to mimic teacher behavior \\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  student_model.train()\n",
        "  total_loss = 0\n",
        "  num_batches = 0\n",
        "  for batch_idx, batch in enumerate(kd_dataloader):\n",
        "    # ###Step 1: Move batch to device###\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    # Move teacher model to device, perform inference, and move back to CPU\n",
        "    teacher_model.to(device)\n",
        "\n",
        "    # ###Step 2: The Master is asked to generate its logits###\n",
        "    with torch.no_grad():\n",
        "      teacher_outputs = teacher_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      teacher_logits = teacher_outputs.logits / TEMPERATURE\n",
        "\n",
        "    # Moving teacher model to CPU to save memory.\n",
        "    teacher_model.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # ###Step 3: Get Student's \"thoughts\" (logits) with training enabled###\n",
        "    # Student inference (with gradients)\n",
        "    student_outputs = student_model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    student_logits = student_outputs.logits / TEMPERATURE\n",
        "\n",
        "    # Compute Knowledge Distillation loss\n",
        "    teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
        "    student_log_probs = F.log_softmax(student_logits, dim=-1)\n",
        "\n",
        "    # ###Step 4:\n",
        "    # KL Divergence loss\n",
        "    kd_loss = F.kl_div(\n",
        "      student_log_probs,\n",
        "      teacher_probs,\n",
        "      reduction='batchmean'\n",
        "    )\n",
        "\n",
        "    # Scale loss for gradient accumulation\n",
        "    loss = kd_loss / ACCUMULATION_STEPS\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient accumulation\n",
        "    if (batch_idx + 1) % ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(kd_dataloader):\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    total_loss += loss.item() * ACCUMULATION_STEPS\n",
        "    num_batches += 1\n",
        "    # Progress update\n",
        "\n",
        "    if (batch_idx + 1) % 100 == 0:\n",
        "      avg_loss = total_loss / num_batches\n",
        "      print(f'Epoch {epoch + 1}/{NUM_EPOCHS} | Batch {batch_idx + 1} | Loss: {avg_loss:.4f}')\n",
        "\n",
        "  # Epoch summary\n",
        "  avg_epoch_loss = total_loss / num_batches\n",
        "  print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} | Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Knowledge Distillation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jww6LbeRHC1K"
      },
      "source": [
        "## Basic Test generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W0-J7MuFqiO",
        "outputId": "0b8f0f7f-1a56-4bc3-b501-9cd4d9b97d95"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Qualitative Test: 'Paris is the capital of' ---\n",
            "Teacher (Original):    'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n",
            "Student (Post-KD):     'Paris is the capital of France and one of the most famous cities in the world. It is a city with a rich history and a vibrant culture. Paris is known for its beautiful architecture, art, and culture, as well as its famous French cuisine. The city is also'\n"
          ]
        }
      ],
      "source": [
        "# Set student model to evaluation mode\n",
        "teacher_model.to(device)\n",
        "student_model.eval()\n",
        "# Test with the same prompt used in baseline\n",
        "print(f\"--- Qualitative Test: '{TEST_PROMPT}' ---\")\n",
        "\n",
        "# Generate with all three models for comparison\n",
        "teacher_output = generate_text(teacher_model, tokenizer, TEST_PROMPT)\n",
        "student_baseline_output = generate_text(student_model, tokenizer, TEST_PROMPT)\n",
        "\n",
        "print(f\"Teacher (Original):    '{teacher_output}'\")\n",
        "print(f\"Student (Post-KD):     '{student_baseline_output}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Wi3YX5H9nQ"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xhPEAh9LHWl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f107dab-0dea-4a0e-8902-d15ab55cc14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting lm-eval on model 'google/gemma-3-270m' for tasks: ['arc_easy', 'winogrande', 'hellaswag', 'lambada_openai']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of hellaswag from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of arc_easy from None to 0\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 534.43it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2225.80it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 65772.37it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1003.28it/s]\n",
            "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1099/1099 [00:37<00:00, 29.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bootstrapping for stddev: perplexity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 598.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"arc_easy\": \"0.5300\",\n",
            "  \"hellaswag\": \"0.4100\",\n",
            "  \"lambada_openai\": \"0.3500\",\n",
            "  \"winogrande\": \"0.6100\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Define the benchmark suite for our diagnostic\n",
        "benchmark_tasks = ['arc_easy', 'winogrande', 'hellaswag', 'lambada_openai']\n",
        "student_recovered_results = model_evaluation(student_model, tokenizer, benchmark_tasks, limit=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tftSlgqbhqMJ",
        "outputId": "719025c4-d835-443f-f2eb-47e13cb346d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'arc_easy': '0.5270',\n",
              " 'hellaswag': '0.3630',\n",
              " 'lambada_openai': '0.3680',\n",
              " 'winogrande': '0.5460'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "student_recovered_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10:47\n",
        "{ \"arc_easy\": \"0.4600\", \"hellaswag\": \"0.3900\", \"lambada_openai\": \"0.3300\", \"winogrande\": \"0.5600\" }\n",
        "\n",
        "Limit 1000 Seed 42\n",
        "{\n",
        "  \"arc_easy\": \"0.5270\",\n",
        "  \"hellaswag\": \"0.3630\",\n",
        "  \"lambada_openai\": \"0.3680\",\n",
        "  \"winogrande\": \"0.5460\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "twZC4d6js81u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYb9jAfHYPKZ"
      },
      "source": [
        "## Complete Knowledge Recovery Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAKuPGXmUTlO"
      },
      "source": [
        "## Results with 2 layers pruned\n",
        "\n",
        "| **Metric** | **Teacher (Original)** | **Student (-2L pruned)** | **Student (-2L, 500 samples)** | **Student (-2L, 15K samples)** | **Final Performance vs Original** |\n",
        "|------------|------------------------|--------------------------|--------------------------------|--------------------------------|-----------------------------------|\n",
        "| **Parameters** | 268,098,176 | 256,950,912 | 256,950,912 | 256,950,912 | **-4.16%** |\n",
        "| **arc_easy** (acc) | 0.5500 | 0.4600 | 0.4700 | **0.5200** | **94.5%** of original |\n",
        "| **hellaswag** (acc) | 0.4200 | - | - | **0.4100** | **97.6%** of original |\n",
        "| **lambada_openai** (acc) | 0.4200 | 0.3400 | 0.2600 | **0.3900** | **92.9%** of original |\n",
        "| **winogrande** (acc) | 0.6000 | 0.4800 | 0.5800 | **0.5800** | **96.7%** of original |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo20AhGfWwiO"
      },
      "source": [
        "## Results with 3 layers pruned\n",
        "| Metric | Teacher (Original) | Student (-3L pruned) | Student (-3L, 5K samples) | Final Performance vs Original |\n",
        "|:-------|:-------------------|:---------------------|:--------------------------|:------------------------------|\n",
        "| **Parameters** | 268,098,176 | 251,377,280 | 251,377,280 | **-6.24%** |\n",
        "| **arc_easy** (acc) | 0.5500 | 0.3800 | **0.4700** | **85.5%** of original |\n",
        "| **winogrande** (acc) | 0.6000 | 0.5400 | **0.5800** | **96.7%** of original |\n",
        "| **boolq** (acc) | 0.6600 | 0.7000 | **0.6700** | **101.5%** of original |\n",
        "| **lambada_openai** (acc) | 0.4200 | 0.1800 | **0.3300** | **78.6%** of original |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "z0k7tkDeG5TO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaVoPbU8V95K"
      },
      "source": [
        "## Results with 4 layers pruned\n",
        "\n",
        "| **Metric** | **Teacher (Original)** | **Student (-4L pruned)** | **Student (-4L, 5K samples)** | **Student (-4L, 15K samples)** | **Final Performance vs Original** |\n",
        "|------------|------------------------|--------------------------|--------------------------------|--------------------------------|-----------------------------------|\n",
        "| **Parameters** | 268,098,176 | 245,803,648 | 245,803,648 | 245,803,648 | **-8.31%** |\n",
        "| **arc_easy** (acc) | 0.5500 | 0.4100 | **0.4800** | **0.4300** | **78.2%** of original |\n",
        "| **hellaswag** (acc) | 0.4200 | 0.3100 | **0.3800** | **0.3600** | **85.7%** of original |\n",
        "| **lambada_openai** (acc) | 0.4200 | 0.2100 | **0.2700** | **0.3300** | **78.6%** of original |\n",
        "| **winogrande** (acc) | 0.6000 | 0.5300 | **0.5800** | **0.6000** | **100.0%** of original |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPOu-8o5rJN5"
      },
      "source": [
        "### LLaMA-3.2-1B (-2 Layers) Knowledge Recovery\n",
        "\n",
        "| Metric | Teacher (Original) | Student (Pruned) | Student (500 samples) | Final Performance |\n",
        "|:-------|:-------------------|:-----------------|:---------------------|:------------------|\n",
        "| **Parameters** | 1,235,814,400 | 1,117,171,392 | 1,117,171,392 | **Maintained** |\n",
        "| **arc_easy** (acc) | 0.6600 | 0.4800 | **0.5800** | **87.9%** of original |\n",
        "| **winogrande** (acc) | 0.6000 | 0.5400 | 0.5500 | **91.7%** of original |\n",
        "| **boolq** (acc) | 0.6700 | 0.7000 | **0.5400** | **80.6%** of original |\n",
        "| **lambada_openai** (acc) | 0.5700 | 0.1700 | 0.1800 | **31.6%** of original |\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}