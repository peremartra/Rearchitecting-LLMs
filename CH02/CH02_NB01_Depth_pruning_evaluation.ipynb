{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH02/CH02_NB01_Depth_pruning_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table style=\"width:100%; border:none; background:none;\">\n",
        "  <tr style=\"border:none;\">\n",
        "    <td style=\"border:none; vertical-align:middle; text-align:left; width: 120px;\">\n",
        "      <a href=\"https://hubs.la/Q040tvsK0\"><img src=\"https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/Images/cover.png\" width=\"100px\" style=\"border-radius: 4px;\"></a>\n",
        "    </td>\n",
        "    <td style=\"border:none; vertical-align:middle; text-align:left;\">\n",
        "      <p style=\"margin: 0; font-size: 14px;\">\n",
        "        Supplementary code for the <a href=\"https://hubs.la/Q040tvsK0\">Rearchitecting LLMs</a> book by <a href=\"https://github.com/peremartra\">Pere Martra</a>.<br>\n",
        "        <br>\n",
        "        <strong>MEAP Special:</strong> Get it at 50% off with code <strong>MLMartra</strong> (valid until Feb 26th).<br>\n",
        "        <br>\n",
        "        Code repository: <a href=\"https://github.com/peremartra/Rearchitecting-LLMs\">https://github.com/peremartra/Rearchitecting-LLMs</a>\n",
        "      </p>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnrz5pV3ZdAs"
      },
      "source": [
        "# Rearchitecting LLMs\n",
        "## Structural techniques for efficient models\n",
        "\n",
        "### Chapter 2: Rearchitecting an LLM: A hands-on introduction\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white)](https://x.com/PereMartra) [![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "- Models recommended: google/gemma-3-270m\n",
        "\n",
        "- Tested with: meta-llama/Llama-3.2-1B\n",
        "_____\n",
        "\n",
        "Welcome to your first hands-on model tailoring project. In this notebook, we will follow the first two steps of a typical optimization workflow:\n",
        "\n",
        "* Establish a Baseline: We will measure the performance of a standard, pre-trained model on a few key metrics.\n",
        "\n",
        "* Perform the Surgery: We will surgically remove entire layers from the model's architecture (a technique known as depth pruning).\n",
        "\n",
        "* Evaluate the Impact: We will re-run our baseline evaluation to precisely quantify the effect of our intervention.\n",
        "\n",
        "This notebook sets the stage for the next step, covered in the book and the following notebook: recovering the model's lost knowledge through fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F1PYtSHTtgJ"
      },
      "source": [
        "# 2.1 Setting up the project and establising the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bth88f19fjsi"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRPeRaIXEwWW",
        "outputId": "1657a0c9-9eef-4fef-a858-ae0abadbe7b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m483.4/483.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "      \"lm_eval==0.4.9.1\" \\\n",
        "      \"sentencepiece==0.2.1\" \\\n",
        "      \"sentence-transformers==5.1.0\" \\\n",
        "      \"optipfair==0.1.4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgSLeGGacKO8",
        "outputId": "b07c0310-8d10-4c8a-99b5-c89eb9c06158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair.pruning.depth import prune_model_depth\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6gVAERdfq1I"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scqxXETCfpdp"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"google/gemma-3-270m\"\n",
        "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
        "MAX_NEW_TOKENS = 50\n",
        "LAYERS_TO_REMOVE = 2\n",
        "TEST_PROMPT = \"Paris is the capital of\"\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kppPYIb0MOms"
      },
      "source": [
        "## Model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Ea5mzsf0mw",
        "outputId": "7a50ad30-a825-4ab6-d949-95688cd7846f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 268098176\n",
            "Original layers: 18\n",
            "====================\n",
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "original_params= sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {original_params}\")\n",
        "print(f\"Original layers: {len(model.model.layers)}\")\n",
        "\n",
        "print(\"=\" * 20)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buqunQ8EhURh"
      },
      "source": [
        "## Basic generation test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTagr1rMhCEp"
      },
      "source": [
        "### Support functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvhT7W_AYORM",
        "outputId": "88a3ddf1-eba0-411e-e33b-e4c2eca41145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Model generation config cleaned\n",
            "New config: GenerationConfig {}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Clean conflicting configuration after loading the model\n",
        "from transformers import GenerationConfig\n",
        "clean_config = GenerationConfig(\n",
        "    max_length=model.generation_config.max_length,\n",
        "    pad_token_id=model.generation_config.pad_token_id,\n",
        "    eos_token_id=model.generation_config.eos_token_id,\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    early_stopping=False\n",
        ")\n",
        "model.generation_config = clean_config\n",
        "\n",
        "print(\"âœ“ Model generation config cleaned\")\n",
        "print(f\"New config: {model.generation_config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rbKhNpU24nUt"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total parameters in model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
        "    \"\"\"Generate text with the model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            #early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9YeWZVANzEs"
      },
      "source": [
        "### Generating response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXw_Ze4AhNgx",
        "outputId": "6f666887-b46c-429a-9a18-08c5fdb7fe9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated Text: 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n"
          ]
        }
      ],
      "source": [
        "# A simple prompt to test basic knowledge and coherence\n",
        "prompt = TEST_PROMPT\n",
        "generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated Text: '{generated_text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9tfwHi9xpEq"
      },
      "source": [
        "## Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KqfHsXnzgC03"
      },
      "outputs": [],
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, limit=100):\n",
        "    \"\"\"\n",
        "    Runs lm-eval on a PyTorch model object already in memory.\n",
        "\n",
        "    Args:\n",
        "        model_obj: The PyTorch model object to evaluate.\n",
        "        tokenizer: The tokenizer object.\n",
        "        tasks (list): A list of task names.\n",
        "        limit (int): The number of samples per task.\n",
        "    \"\"\"\n",
        "    print(f\"Starting lm-eval on model '{model_obj.config._name_or_path}' for tasks: {tasks}\")\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(device)\n",
        "    )\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=model_wrapper,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        device=str(device),\n",
        "    )\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in results[\"results\"].items():\n",
        "        # Look for accuracy ('acc') first, then perplexity ('ppl')\n",
        "        if 'acc,none' in res:\n",
        "            metric_val = res.get('acc,none', 0)\n",
        "        elif 'ppl,none' in res:\n",
        "             metric_val = res.get('ppl,none', 0)\n",
        "        else:\n",
        "            metric_val = 0 # Fallback\n",
        "\n",
        "        formatted_results[task_name] = f\"{metric_val:.4f}\"\n",
        "\n",
        "    print(json.dumps(formatted_results, indent=2))\n",
        "    return formatted_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGokGEyLg-rQ"
      },
      "outputs": [],
      "source": [
        "# Define the benchmark suite for our diagnostic\n",
        "benchmark_tasks = ['arc_easy', 'winogrande', 'hellaswag', 'lambada_openai']\n",
        "\n",
        "# Run the evaluation\n",
        "baseline_results = model_evaluation(model, tokenizer, benchmark_tasks, limit=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbXRNOFtUO7h",
        "outputId": "2f73db63-9809-4efd-d140-6af9d7ab7526"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'arc_easy': '0.5500',\n",
              " 'hellaswag': '0.4200',\n",
              " 'lambada_openai': '0.4200',\n",
              " 'winogrande': '0.6000'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWixoBKpO00u"
      },
      "source": [
        "## Inference time response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q4gAdU5yIe6u"
      },
      "outputs": [],
      "source": [
        "#### Inference Speed Measurement\n",
        "\n",
        "import gc\n",
        "def clear_gpu_cache():\n",
        "    \"\"\"Clean GPU Cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "def measure_inference_time(model, tokenizer, prompts, num_runs=5):\n",
        "    \"\"\"Measure average inference time across multiple runs\"\"\"\n",
        "    times = []\n",
        "\n",
        "    # Warm up GPU\n",
        "    print(\"Warming up base model...\")\n",
        "    clear_gpu_cache()\n",
        "    for _ in range(20):\n",
        "        _ = generate_text(model, tokenizer, \"warmup\", max_new_tokens=50)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        for prompt in prompts:\n",
        "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "            with torch.no_grad():\n",
        "                _ = model.generate(\n",
        "                    inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_new_tokens=50,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "\n",
        "    return {\n",
        "        'mean_time': np.mean(times),\n",
        "        'std_time': np.std(times),\n",
        "        'all_times': times\n",
        "    }\n",
        "\n",
        "# Test prompts for speed measurement\n",
        "speed_test_prompts = [\n",
        "    \"The capital of France is\",\n",
        "    \"Machine learning is\",\n",
        "    \"Climate change refers to\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "salZJDfq33Ji",
        "outputId": "fe8dad4f-c600-4b7a-d69f-ea9f3474e19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Measuring original model...\n",
            "Warming up base model...\n"
          ]
        }
      ],
      "source": [
        "# Measure original model\n",
        "print(\"Measuring original model...\")\n",
        "original_timing = measure_inference_time(model, tokenizer, speed_test_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30H6oRtm4D33",
        "outputId": "c988e289-a48c-4736-d11d-14a70f64538e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Inference Speed Results:\n",
            "   Original model: 4.959s Â± 0.207s\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nðŸ“Š Inference Speed Results:\")\n",
        "print(f\"   Original model: {original_timing['mean_time']:.3f}s Â± {original_timing['std_time']:.3f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sjfYCuGAIe6"
      },
      "source": [
        "# 2.2 Applying depth pruning to the model\n",
        "Let's remove the last layers of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9STV8NaGC1gJ",
        "outputId": "9fd0ae2c-4770-47be-e3fa-d6a7a005d16f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model structure:\n",
            "  - Total layers: 18\n",
            "  - Layer indices: 0 to 17\n",
            "  - Parameters: 268,098,176\n"
          ]
        }
      ],
      "source": [
        "# Create a copy of the original model for manual pruning\n",
        "import copy\n",
        "pruned_model = copy.deepcopy(model)\n",
        "print(f\"Original model structure:\")\n",
        "print(f\"  - Total layers: {len(pruned_model.model.layers)}\")\n",
        "print(f\"  - Layer indices: 0 to {len(pruned_model.model.layers)-1}\")\n",
        "print(f\"  - Parameters: {count_parameters(pruned_model):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSf2H_9hC09q",
        "outputId": "1b49b870-b3b2-4e0d-b4cf-7dbed2a92a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Manually removing last 2 layers...\n"
          ]
        }
      ],
      "source": [
        "# Manual layer removal - remove last N layers\n",
        "print(f\"\\nManually removing last {LAYERS_TO_REMOVE} layers...\")\n",
        "original_layers_count = len(pruned_model.model.layers)\n",
        "new_layers_count = original_layers_count - LAYERS_TO_REMOVE\n",
        "# Create new layer list excluding the last LAYERS_TO_REMOVE layers\n",
        "new_layers = pruned_model.model.layers[:new_layers_count]\n",
        "pruned_model.model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "# Update model configuration to reflect the change\n",
        "pruned_model.config.num_hidden_layers = len(pruned_model.model.layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jndg8UiyDNyb",
        "outputId": "36bbf50e-fe9a-4f22-94de-e4f3d4960779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual pruning results:\n",
            "  - New layer count: 16\n",
            "  - New parameter count: 256,950,912\n",
            "  - Parameters removed: 11,147,264\n",
            "  - Reduction: 4.16%\n"
          ]
        }
      ],
      "source": [
        "# Verify the manual pruning\n",
        "manual_params = count_parameters(pruned_model)\n",
        "manual_layers = len(pruned_model.model.layers)\n",
        "\n",
        "print(f\"Manual pruning results:\")\n",
        "print(f\"  - New layer count: {manual_layers}\")\n",
        "print(f\"  - New parameter count: {manual_params:,}\")\n",
        "print(f\"  - Parameters removed: {original_params - manual_params:,}\")\n",
        "print(f\"  - Reduction: {((original_params - manual_params) / original_params * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGhXGUhCUgId",
        "outputId": "39d57e22-55f7-4050-fa55-6cbe1eeeed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Gemma3ForCausalLM(\n",
              "  (model): Gemma3TextModel(\n",
              "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x Gemma3DecoderLayer(\n",
              "        (self_attn): Gemma3Attention(\n",
              "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
              "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
              "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Gemma3MLP(\n",
              "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "    (rotary_emb): Gemma3RotaryEmbedding()\n",
              "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pruned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEdo19XiHQ3n"
      },
      "source": [
        "## Basic pruned generation test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhAnW0MCMAdR",
        "outputId": "670b97a6-cc46-443c-a7b3-b993b2ab1adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unloading the original model from VRAM to ensure isolated measurement...\n"
          ]
        }
      ],
      "source": [
        "print(\"Unloading the original model from VRAM to ensure isolated measurement...\")\n",
        "del model\n",
        "clear_gpu_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N05taPhlEXM7",
        "outputId": "904c5235-f5bf-4ab4-deb5-f267e9394a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated Text Pruned: 'Paris is the capital of France and one of the largest cities in Europe. It occupies approximately 2.5 million hectares of land surrounded by mountains and forests. Parisians love to travel abroad because they enjoy sightseeing tours abroad. Tourists visiting Paris visit museums, monuments, theaters,'\n",
            "Generated Text Base  : 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n"
          ]
        }
      ],
      "source": [
        "# A simple prompt to test basic knowledge and coherence\n",
        "generated_text_pruned = generate_text(pruned_model, tokenizer, TEST_PROMPT)\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated Text Pruned: '{generated_text_pruned}'\")\n",
        "print(f\"Generated Text Base  : '{generated_text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCuzNq7lHxY6"
      },
      "source": [
        "## Pruned Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXMBDkoJGHYA",
        "outputId": "e7e82584-59f2-493d-f002-228f1531fcd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating Pruned Model ---\n",
            "Starting lm-eval on model 'google/gemma-3-270m' for tasks: ['arc_easy', 'winogrande', 'hellaswag', 'lambada_openai']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of hellaswag from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of arc_easy from None to 0\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 541.43it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2459.28it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 94296.40it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1026.49it/s]\n",
            "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1099/1099 [00:41<00:00, 26.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bootstrapping for stddev: perplexity\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 104.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"arc_easy\": \"0.4600\",\n",
            "  \"hellaswag\": \"0.3400\",\n",
            "  \"lambada_openai\": \"0.3400\",\n",
            "  \"winogrande\": \"0.4800\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "## Evaluation with lm-eval\n",
        "# Run evaluation on the pruned model object we have in memory\n",
        "print(\"--- Evaluating Pruned Model ---\")\n",
        "\n",
        "# The 'pruned_model' variable holds the model we modified in Section 2.1\n",
        "pruned_results = model_evaluation(pruned_model, tokenizer, benchmark_tasks, limit=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "JN0ymC8FIKP9",
        "outputId": "885f8f74-536e-41ef-b8c7-21527e7fb2c2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "## Performance Impact Analysis\n",
              "\n",
              "| Metric                  | Original Model (`gemma-3-270m`) | Pruned Model (-2 Layers) | Change          |\n",
              "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
              "| **Parameters**          | 268,098,176              | 256,950,912          | **-4.16%** |\n",
              "| **arc_easy** (acc)      | 0.5500   | 0.4600 | -16.36% |\n",
              "| **winogrande** (acc)    | 0.6000 | 0.4800 | -20.00% |\n",
              "| **hellaswag** (acc)         | 0.4200      | 0.3400      | -19.05% |\n",
              "| **lambada_openai** (acc)| 0.4200 | 0.3400 | -19.05% |\n",
              "\n",
              "### Key Insights:\n",
              "- **Efficiency Gain**: 4.2% parameter reduction â†’ faster inference & lower memory\n",
              "- **Performance Impact**: See individual benchmark changes above\n",
              "- **Next Step**: Knowledge recovery through fine-tuning (Section 2.3)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Calculate parameter reduction\n",
        "pruned_params = count_parameters(pruned_model)\n",
        "param_reduction_pct = (original_params - pruned_params) / original_params\n",
        "\n",
        "# Helper function to calculate percentage change\n",
        "def calculate_change(old, new):\n",
        "    old, new = float(old), float(new)\n",
        "    if old == 0:\n",
        "        return \"N/A\"\n",
        "    return f\"{(new - old) / old:+.2%}\"\n",
        "\n",
        "# Create the comparison table\n",
        "model_id = MODEL_NAME.split('/')[-1]  # Get just the model name without org\n",
        "\n",
        "markdown_table = f\"\"\"\n",
        "## Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`{model_id}`) | Pruned Model (-{LAYERS_TO_REMOVE} Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | {original_params:,}              | {pruned_params:,}          | **-{param_reduction_pct:.2%}** |\n",
        "| **arc_easy** (acc)      | {baseline_results['arc_easy']}   | {pruned_results['arc_easy']} | {calculate_change(baseline_results['arc_easy'], pruned_results['arc_easy'])} |\n",
        "| **winogrande** (acc)    | {baseline_results['winogrande']} | {pruned_results['winogrande']} | {calculate_change(baseline_results['winogrande'], pruned_results['winogrande'])} |\n",
        "| **hellaswag** (acc)         | {baseline_results['hellaswag']}      | {pruned_results['hellaswag']}      | {calculate_change(baseline_results['hellaswag'], pruned_results['hellaswag'])} |\n",
        "| **lambada_openai** (acc)| {baseline_results['lambada_openai']} | {pruned_results['lambada_openai']} | {calculate_change(baseline_results['lambada_openai'], pruned_results['lambada_openai'])} |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: {param_reduction_pct:.1%} parameter reduction â†’ faster inference & lower memory\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Next Step**: Knowledge recovery through fine-tuning (Section 2.3)\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(markdown_table))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Z0YSKnNENt"
      },
      "source": [
        "## Inference pruned time response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ih1d_UWZ8r2",
        "outputId": "72662d31-063f-46f6-c01a-e7fe70d64aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Measuring inference speed across 3 prompts, 5 runs each...\n",
            "Warming up base model...\n",
            "\n",
            "ðŸ“Š Inference Speed Results:\n",
            "   Original model: 4.959s Â± 0.207s\n",
            "   Pruned model:   4.515s Â± 0.260s\n",
            "   Speedup:        1.10x (+9.0% faster)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Measuring inference speed across {len(speed_test_prompts)} prompts, 5 runs each...\")\n",
        "\n",
        "# Measure pruned model\n",
        "pruned_timing = measure_inference_time(pruned_model, tokenizer, speed_test_prompts)\n",
        "\n",
        "# Calculate speedup\n",
        "speedup = original_timing['mean_time'] / pruned_timing['mean_time']\n",
        "time_reduction_pct = (original_timing['mean_time'] - pruned_timing['mean_time']) / original_timing['mean_time']\n",
        "\n",
        "print(f\"\\nðŸ“Š Inference Speed Results:\")\n",
        "print(f\"   Original model: {original_timing['mean_time']:.3f}s Â± {original_timing['std_time']:.3f}s\")\n",
        "print(f\"   Pruned model:   {pruned_timing['mean_time']:.3f}s Â± {pruned_timing['std_time']:.3f}s\")\n",
        "print(f\"   Speedup:        {speedup:.2f}x ({time_reduction_pct:+.1%} faster)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "gg5i8qOhK3ic",
        "outputId": "5c6dc237-2d70-4a53-9b71-d3a8be3a30d2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "## Performance Impact Analysis\n",
              "\n",
              "| Metric                  | Original Model (`gemma-3-270m`) | Pruned Model (-2 Layers) | Change          |\n",
              "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
              "| **Parameters**          | 268,098,176              | 256,950,912          | **-4.16%** |\n",
              "| **Inference Time**      | 4.959s | 4.515s | **+9.0%** |\n",
              "| **arc_easy** (acc)      | 0.5500   | 0.4600 | -16.36% |\n",
              "| **winogrande** (acc)    | 0.6000 | 0.4800 | -20.00% |\n",
              "| **hellaswag** (acc)         | 0.4200      | 0.3400      | -19.05% |\n",
              "| **lambada_openai** (acc)| 0.4200 | 0.3400 | -19.05% |\n",
              "\n",
              "### Key Insights:\n",
              "- **Efficiency Gain**: 4.2% parameter reduction + 1.10x speedup\n",
              "- **Performance Impact**: See individual benchmark changes above\n",
              "- **Trade-off**: 9.0% faster inference vs accuracy changes\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Update the comparison table to include measured speed\n",
        "markdown_table = f\"\"\"\n",
        "## Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`{model_id}`) | Pruned Model (-{LAYERS_TO_REMOVE} Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | {original_params:,}              | {pruned_params:,}          | **-{param_reduction_pct:.2%}** |\n",
        "| **Inference Time**      | {original_timing['mean_time']:.3f}s | {pruned_timing['mean_time']:.3f}s | **{time_reduction_pct:+.1%}** |\n",
        "| **arc_easy** (acc)      | {baseline_results['arc_easy']}   | {pruned_results['arc_easy']} | {calculate_change(baseline_results['arc_easy'], pruned_results['arc_easy'])} |\n",
        "| **winogrande** (acc)    | {baseline_results['winogrande']} | {pruned_results['winogrande']} | {calculate_change(baseline_results['winogrande'], pruned_results['winogrande'])} |\n",
        "| **hellaswag** (acc)         | {baseline_results['hellaswag']}      | {pruned_results['hellaswag']}      | {calculate_change(baseline_results['hellaswag'], pruned_results['hellaswag'])} |\n",
        "| **lambada_openai** (acc)| {baseline_results['lambada_openai']} | {pruned_results['lambada_openai']} | {calculate_change(baseline_results['lambada_openai'], pruned_results['lambada_openai'])} |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: {param_reduction_pct:.1%} parameter reduction + {speedup:.2f}x speedup\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Trade-off**: {time_reduction_pct:.1%} faster inference vs accuracy changes\n",
        "\"\"\"\n",
        "display(Markdown(markdown_table))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8msGXT-rb0eK"
      },
      "source": [
        "\n",
        "## Llama-3.2-1B Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`Llama-3.2-1B`) | Pruned Model (-2 Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | 1,235,814,400              | 1,117,171,392          | **-9.84%** |\n",
        "| **Inference Time**      | 6.635s | 4,752s | **+20.4%** |\n",
        "| **arc_easy** (acc)      | 0.6600   | 0.4800 | -27.27% |\n",
        "| **winogrande** (acc)    | 0.6000 | 0.5400 | -10% |\n",
        "| **boolq** (acc)         | 0.6700      | 0.7000      | +4.48% |\n",
        "| **lambada_openai** (acc)| 0.5700 | 0.1700 | -70.18% |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: 9.8 parameter reduction + 1.40x speedup\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Trade-off**: 28.4 faster inference vs accuracy changes\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
