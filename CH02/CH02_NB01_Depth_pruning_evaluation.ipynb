{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOudlk8H2dfn/y5I5I0Z9Iq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH02/CH02_NB01_Depth_pruning_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 2: Rearchitecting an LLM: A hands-on introduction\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "- Models recommended: google/gemma-3-270m\n",
        "\n",
        "- Tested with: meta-llama/Llama-3.2-1B\n",
        "_____\n",
        "\n",
        "Welcome to your first hands-on model tailoring project. In this notebook, we will follow the first two steps of a typical optimization workflow:\n",
        "\n",
        "* Establish a Baseline: We will measure the performance of a standard, pre-trained model on a few key metrics.\n",
        "\n",
        "* Perform the Surgery: We will surgically remove entire layers from the model's architecture (a technique known as depth pruning).\n",
        "\n",
        "* Evaluate the Impact: We will re-run our baseline evaluation to precisely quantify the effect of our intervention.\n",
        "\n",
        "This notebook sets the stage for the next step, covered in the book and the following notebook: recovering the model's lost knowledge through fine-tuning."
      ],
      "metadata": {
        "id": "rnrz5pV3ZdAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Setting up the project and establising the pipeline"
      ],
      "metadata": {
        "id": "_F1PYtSHTtgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "bth88f19fjsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "      \"torch==2.8.0+cu126\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "      \"lm_eval==0.4.9.1\" \\\n",
        "      \"sentencepiece==0.2.1\" \\\n",
        "      \"sentence-transformers==5.1.0\" \\\n",
        "      \"optipfair==0.1.4\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rRPeRaIXEwWW",
        "outputId": "4e44f323-7152-457a-f2ad-fdb55ff86ded"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair.pruning.depth import prune_model_depth\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CgSLeGGacKO8",
        "outputId": "c8b28e5c-c5d1-432b-fb6f-75e5c722f82d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "B6gVAERdfq1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"google/gemma-3-270m\"\n",
        "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
        "MAX_NEW_TOKENS = 50\n",
        "LAYERS_TO_REMOVE = 2\n",
        "TEST_PROMPT = \"Paris is the capital of\"\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "scqxXETCfpdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model structure"
      ],
      "metadata": {
        "id": "kppPYIb0MOms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_params= sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {original_params}\")\n",
        "print(f\"Original layers: {len(model.model.layers)}\")\n",
        "\n",
        "print(\"=\" * 20)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-1Ea5mzsf0mw",
        "outputId": "85340d62-0cec-40c1-88f9-08db2398b970"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 268098176\n",
            "Original layers: 18\n",
            "====================\n",
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic generation test"
      ],
      "metadata": {
        "id": "buqunQ8EhURh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support functions"
      ],
      "metadata": {
        "id": "QTagr1rMhCEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean conflicting configuration after loading the model\n",
        "from transformers import GenerationConfig\n",
        "clean_config = GenerationConfig(\n",
        "    max_length=model.generation_config.max_length,\n",
        "    pad_token_id=model.generation_config.pad_token_id,\n",
        "    eos_token_id=model.generation_config.eos_token_id,\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    early_stopping=False\n",
        ")\n",
        "model.generation_config = clean_config\n",
        "\n",
        "print(\"✓ Model generation config cleaned\")\n",
        "print(f\"New config: {model.generation_config}\")"
      ],
      "metadata": {
        "id": "pvhT7W_AYORM",
        "outputId": "557a26bf-e0cc-483d-d129-eab0e5bfd7d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model generation config cleaned\n",
            "New config: GenerationConfig {}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total parameters in model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
        "    \"\"\"Generate text with the model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            #early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "rbKhNpU24nUt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating response"
      ],
      "metadata": {
        "id": "y9YeWZVANzEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple prompt to test basic knowledge and coherence\n",
        "prompt = TEST_PROMPT\n",
        "generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated Text: '{generated_text}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NXw_Ze4AhNgx",
        "outputId": "0b7a0ac4-3ba2-4053-ea02-f9fe841dabc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated Text: 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Evaluation"
      ],
      "metadata": {
        "id": "n9tfwHi9xpEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, limit=100):\n",
        "    \"\"\"\n",
        "    Runs lm-eval on a PyTorch model object already in memory.\n",
        "\n",
        "    Args:\n",
        "        model_obj: The PyTorch model object to evaluate.\n",
        "        tokenizer: The tokenizer object.\n",
        "        tasks (list): A list of task names.\n",
        "        limit (int): The number of samples per task.\n",
        "    \"\"\"\n",
        "    print(f\"Starting lm-eval on model '{model_obj.config._name_or_path}' for tasks: {tasks}\")\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(device)\n",
        "    )\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=model_wrapper,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        device=str(device),\n",
        "    )\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in results[\"results\"].items():\n",
        "        # Look for accuracy ('acc') first, then perplexity ('ppl')\n",
        "        if 'acc,none' in res:\n",
        "            metric_val = res.get('acc,none', 0)\n",
        "        elif 'ppl,none' in res:\n",
        "             metric_val = res.get('ppl,none', 0)\n",
        "        else:\n",
        "            metric_val = 0 # Fallback\n",
        "\n",
        "        formatted_results[task_name] = f\"{metric_val:.4f}\"\n",
        "\n",
        "    print(json.dumps(formatted_results, indent=2))\n",
        "    return formatted_results\n"
      ],
      "metadata": {
        "id": "KqfHsXnzgC03"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the benchmark suite for our diagnostic\n",
        "benchmark_tasks = ['arc_easy', 'winogrande', 'boolq', 'lambada_openai']\n",
        "\n",
        "# Run the evaluation\n",
        "baseline_results = model_evaluation(model, tokenizer, benchmark_tasks, limit=100)\n"
      ],
      "metadata": {
        "id": "SGokGEyLg-rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AbXRNOFtUO7h",
        "outputId": "f3a51ed2-4c01-4de4-cd8e-00ebdd6e8989"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'arc_easy': '0.5500',\n",
              " 'boolq': '0.6600',\n",
              " 'lambada_openai': '0.4200',\n",
              " 'winogrande': '0.6000'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference time response"
      ],
      "metadata": {
        "id": "pWixoBKpO00u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Inference Speed Measurement\n",
        "\n",
        "import gc\n",
        "def clear_gpu_cache():\n",
        "    \"\"\"Limpia completamente la cache de GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "def measure_inference_time(model, tokenizer, prompts, num_runs=5):\n",
        "    \"\"\"Measure average inference time across multiple runs\"\"\"\n",
        "    times = []\n",
        "\n",
        "    # Warm up GPU\n",
        "    print(\"Warming up base model...\")\n",
        "    clear_gpu_cache()\n",
        "    for _ in range(20):\n",
        "        _ = generate_text(model, tokenizer, \"warmup\", max_new_tokens=50)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        for prompt in prompts:\n",
        "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "            with torch.no_grad():\n",
        "                _ = model.generate(\n",
        "                    inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_new_tokens=50,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "\n",
        "    return {\n",
        "        'mean_time': np.mean(times),\n",
        "        'std_time': np.std(times),\n",
        "        'all_times': times\n",
        "    }\n",
        "\n",
        "# Test prompts for speed measurement\n",
        "speed_test_prompts = [\n",
        "    \"The capital of France is\",\n",
        "    \"Machine learning is\",\n",
        "    \"Climate change refers to\"\n",
        "]"
      ],
      "metadata": {
        "id": "q4gAdU5yIe6u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure original model\n",
        "print(\"Measuring original model...\")\n",
        "original_timing = measure_inference_time(model, tokenizer, speed_test_prompts)"
      ],
      "metadata": {
        "id": "salZJDfq33Ji",
        "outputId": "b241b7c5-244a-45fc-af53-6fe53386723c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring original model...\n",
            "Warming up base model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n📊 Inference Speed Results:\")\n",
        "print(f\"   Original model: {original_timing['mean_time']:.3f}s ± {original_timing['std_time']:.3f}s\")"
      ],
      "metadata": {
        "id": "30H6oRtm4D33",
        "outputId": "b4c273c4-626f-455a-8a64-8b4f53613c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Inference Speed Results:\n",
            "   Original model: 4.748s ± 0.291s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Applying depth pruning to the model\n",
        "Let's remove the last layers of the model, based on the results from the paper:\n",
        "* Kim, B.-K., Kim, G., Kim, T.-H., Castells, T., Choi, S., Shin, J., & Song, H.-K. (2024). Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods. http://arxiv.org/abs/2402.02834"
      ],
      "metadata": {
        "id": "_sjfYCuGAIe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original model for manual pruning\n",
        "import copy\n",
        "pruned_model = copy.deepcopy(model)\n",
        "print(f\"Original model structure:\")\n",
        "print(f\"  - Total layers: {len(pruned_model.model.layers)}\")\n",
        "print(f\"  - Layer indices: 0 to {len(pruned_model.model.layers)-1}\")\n",
        "print(f\"  - Parameters: {count_parameters(pruned_model):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9STV8NaGC1gJ",
        "outputId": "c1d888d0-66a8-4c07-8f57-5b4080f2a066"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model structure:\n",
            "  - Total layers: 18\n",
            "  - Layer indices: 0 to 17\n",
            "  - Parameters: 268,098,176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual layer removal - remove last N layers\n",
        "print(f\"\\nManually removing last {LAYERS_TO_REMOVE} layers...\")\n",
        "original_layers_count = len(pruned_model.model.layers)\n",
        "new_layers_count = original_layers_count - LAYERS_TO_REMOVE\n",
        "# Create new layer list excluding the last LAYERS_TO_REMOVE layers\n",
        "new_layers = pruned_model.model.layers[:new_layers_count]\n",
        "pruned_model.model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "# Update model configuration to reflect the change\n",
        "pruned_model.config.num_hidden_layers = len(pruned_model.model.layers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hSf2H_9hC09q",
        "outputId": "0ab8efc1-1bfe-4dec-dd4b-c257e943360a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Manually removing last 2 layers...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the manual pruning\n",
        "manual_params = count_parameters(pruned_model)\n",
        "manual_layers = len(pruned_model.model.layers)\n",
        "\n",
        "print(f\"Manual pruning results:\")\n",
        "print(f\"  - New layer count: {manual_layers}\")\n",
        "print(f\"  - New parameter count: {manual_params:,}\")\n",
        "print(f\"  - Parameters removed: {original_params - manual_params:,}\")\n",
        "print(f\"  - Reduction: {((original_params - manual_params) / original_params * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Jndg8UiyDNyb",
        "outputId": "b1495c8d-5969-4419-8f56-5301d8fd5aff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual pruning results:\n",
            "  - New layer count: 16\n",
            "  - New parameter count: 256,950,912\n",
            "  - Parameters removed: 11,147,264\n",
            "  - Reduction: 4.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aGhXGUhCUgId",
        "outputId": "5851c512-a4f3-4cef-b7b9-8477af411a57"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma3ForCausalLM(\n",
              "  (model): Gemma3TextModel(\n",
              "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x Gemma3DecoderLayer(\n",
              "        (self_attn): Gemma3Attention(\n",
              "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
              "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
              "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Gemma3MLP(\n",
              "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
              "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
              "    (rotary_emb): Gemma3RotaryEmbedding()\n",
              "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic pruned generation test."
      ],
      "metadata": {
        "id": "IEdo19XiHQ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unloading the original model from VRAM to ensure isolated measurement...\")\n",
        "del model\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "qhAnW0MCMAdR",
        "outputId": "321eb9a1-ddc1-4a41-e895-60242a168083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unloading the original model from VRAM to ensure isolated measurement...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple prompt to test basic knowledge and coherence\n",
        "generated_text_pruned = generate_text(pruned_model, tokenizer, TEST_PROMPT)\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated Text Pruned: '{generated_text_pruned}'\")\n",
        "print(f\"Generated Text Base  : '{generated_text}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N05taPhlEXM7",
        "outputId": "dd1a13e3-95cd-4514-d826-6e2e1ff42fde"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated Text Pruned: 'Paris is the capital of France and one of the largest cities in Europe. It occupies approximately 2.5 million hectares of land surrounded by mountains and forests. Parisians love to travel abroad because they enjoy sightseeing tours abroad. Tourists visiting Paris visit museums, monuments, theaters,'\n",
            "Generated Text Base  : 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruned Evaluation"
      ],
      "metadata": {
        "id": "jCuzNq7lHxY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation with lm-eval\n",
        "# Run evaluation on the pruned model object we have in memory\n",
        "print(\"--- Evaluating Pruned Model ---\")\n",
        "\n",
        "# The 'pruned_model' variable holds the model we modified in Section 2.1\n",
        "pruned_results = model_evaluation(pruned_model, tokenizer, benchmark_tasks, limit=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HXMBDkoJGHYA",
        "outputId": "f5f8b95c-31ee-4e34-a24e-564d294af8f2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating Pruned Model ---\n",
            "Starting lm-eval on model 'google/gemma-3-270m' for tasks: ['arc_easy', 'winogrande', 'boolq', 'lambada_openai']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of arc_easy from None to 0\n",
            "100%|██████████| 100/100 [00:00<00:00, 558.76it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 1917.48it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 60436.66it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 1123.41it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 899/899 [00:26<00:00, 34.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bootstrapping for stddev: perplexity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 95.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"arc_easy\": \"0.4600\",\n",
            "  \"boolq\": \"0.4500\",\n",
            "  \"lambada_openai\": \"0.3400\",\n",
            "  \"winogrande\": \"0.4800\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Calculate parameter reduction\n",
        "pruned_params = count_parameters(pruned_model)\n",
        "param_reduction_pct = (original_params - pruned_params) / original_params\n",
        "\n",
        "# Helper function to calculate percentage change\n",
        "def calculate_change(old, new):\n",
        "    old, new = float(old), float(new)\n",
        "    if old == 0:\n",
        "        return \"N/A\"\n",
        "    return f\"{(new - old) / old:+.2%}\"\n",
        "\n",
        "# Create the comparison table\n",
        "model_id = MODEL_NAME.split('/')[-1]  # Get just the model name without org\n",
        "\n",
        "markdown_table = f\"\"\"\n",
        "## Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`{model_id}`) | Pruned Model (-{LAYERS_TO_REMOVE} Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | {original_params:,}              | {pruned_params:,}          | **-{param_reduction_pct:.2%}** |\n",
        "| **arc_easy** (acc)      | {baseline_results['arc_easy']}   | {pruned_results['arc_easy']} | {calculate_change(baseline_results['arc_easy'], pruned_results['arc_easy'])} |\n",
        "| **winogrande** (acc)    | {baseline_results['winogrande']} | {pruned_results['winogrande']} | {calculate_change(baseline_results['winogrande'], pruned_results['winogrande'])} |\n",
        "| **boolq** (acc)         | {baseline_results['boolq']}      | {pruned_results['boolq']}      | {calculate_change(baseline_results['boolq'], pruned_results['boolq'])} |\n",
        "| **lambada_openai** (acc)| {baseline_results['lambada_openai']} | {pruned_results['lambada_openai']} | {calculate_change(baseline_results['lambada_openai'], pruned_results['lambada_openai'])} |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: {param_reduction_pct:.1%} parameter reduction → faster inference & lower memory\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Next Step**: Knowledge recovery through fine-tuning (Section 2.3)\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(markdown_table))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "JN0ymC8FIKP9",
        "outputId": "ca686f09-1ff1-44da-915e-5af0d618a7f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Performance Impact Analysis\n\n| Metric                  | Original Model (`gemma-3-270m`) | Pruned Model (-2 Layers) | Change          |\n| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n| **Parameters**          | 268,098,176              | 256,950,912          | **-4.16%** |\n| **arc_easy** (acc)      | 0.5500   | 0.4600 | -16.36% |\n| **winogrande** (acc)    | 0.6000 | 0.4800 | -20.00% |\n| **boolq** (acc)         | 0.6600      | 0.4500      | -31.82% |\n| **lambada_openai** (acc)| 0.4200 | 0.3400 | -19.05% |\n\n### Key Insights:\n- **Efficiency Gain**: 4.2% parameter reduction → faster inference & lower memory\n- **Performance Impact**: See individual benchmark changes above\n- **Next Step**: Knowledge recovery through fine-tuning (Section 2.3)\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Measuring inference speed across {len(speed_test_prompts)} prompts, 5 runs each...\")\n",
        "\n",
        "# Measure pruned model\n",
        "pruned_timing = measure_inference_time(pruned_model, tokenizer, speed_test_prompts)\n",
        "\n",
        "# Calculate speedup\n",
        "speedup = original_timing['mean_time'] / pruned_timing['mean_time']\n",
        "time_reduction_pct = (original_timing['mean_time'] - pruned_timing['mean_time']) / original_timing['mean_time']\n",
        "\n",
        "print(f\"\\n📊 Inference Speed Results:\")\n",
        "print(f\"   Original model: {original_timing['mean_time']:.3f}s ± {original_timing['std_time']:.3f}s\")\n",
        "print(f\"   Pruned model:   {pruned_timing['mean_time']:.3f}s ± {pruned_timing['std_time']:.3f}s\")\n",
        "print(f\"   Speedup:        {speedup:.2f}x ({time_reduction_pct:+.1%} faster)\")"
      ],
      "metadata": {
        "id": "-ih1d_UWZ8r2",
        "outputId": "2418844b-e16a-4682-ee04-e15dbce00bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring inference speed across 3 prompts, 5 runs each...\n",
            "Warming up base model...\n",
            "\n",
            "📊 Inference Speed Results:\n",
            "   Original model: 4.651s ± 0.213s\n",
            "   Pruned model:   4.181s ± 0.163s\n",
            "   Speedup:        1.11x (+10.1% faster)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the comparison table to include measured speed\n",
        "markdown_table = f\"\"\"\n",
        "## Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`{model_id}`) | Pruned Model (-{LAYERS_TO_REMOVE} Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | {original_params:,}              | {pruned_params:,}          | **-{param_reduction_pct:.2%}** |\n",
        "| **Inference Time**      | {original_timing['mean_time']:.3f}s | {pruned_timing['mean_time']:.3f}s | **{time_reduction_pct:+.1%}** |\n",
        "| **arc_easy** (acc)      | {baseline_results['arc_easy']}   | {pruned_results['arc_easy']} | {calculate_change(baseline_results['arc_easy'], pruned_results['arc_easy'])} |\n",
        "| **winogrande** (acc)    | {baseline_results['winogrande']} | {pruned_results['winogrande']} | {calculate_change(baseline_results['winogrande'], pruned_results['winogrande'])} |\n",
        "| **boolq** (acc)         | {baseline_results['boolq']}      | {pruned_results['boolq']}      | {calculate_change(baseline_results['boolq'], pruned_results['boolq'])} |\n",
        "| **lambada_openai** (acc)| {baseline_results['lambada_openai']} | {pruned_results['lambada_openai']} | {calculate_change(baseline_results['lambada_openai'], pruned_results['lambada_openai'])} |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: {param_reduction_pct:.1%} parameter reduction + {speedup:.2f}x speedup\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Trade-off**: {time_reduction_pct:.1%} faster inference vs accuracy changes\n",
        "\"\"\"\n",
        "display(Markdown(markdown_table))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "gg5i8qOhK3ic",
        "outputId": "21ef80fd-5dfc-43f0-9274-875c9672707d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Performance Impact Analysis\n\n| Metric                  | Original Model (`gemma-3-270m`) | Pruned Model (-2 Layers) | Change          |\n| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n| **Parameters**          | 268,098,176              | 256,950,912          | **-4.16%** |\n| **Inference Time**      | 4.651s | 4.181s | **+10.1%** |\n| **arc_easy** (acc)      | 0.5500   | 0.4600 | -16.36% |\n| **winogrande** (acc)    | 0.6000 | 0.4800 | -20.00% |\n| **boolq** (acc)         | 0.6600      | 0.4500      | -31.82% |\n| **lambada_openai** (acc)| 0.4200 | 0.3400 | -19.05% |\n\n### Key Insights:\n- **Efficiency Gain**: 4.2% parameter reduction + 1.11x speedup\n- **Performance Impact**: See individual benchmark changes above\n- **Trade-off**: 10.1% faster inference vs accuracy changes\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Llama-3.2-1B Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`Llama-3.2-1B`) | Pruned Model (-2 Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | 1,235,814,400              | 1,117,171,392          | **-9.84%** |\n",
        "| **Inference Time**      | 6.635s | 4,752s | **+20.4%** |\n",
        "| **arc_easy** (acc)      | 0.6600   | 0.4800 | -27.27% |\n",
        "| **winogrande** (acc)    | 0.6000 | 0.5400 | -10% |\n",
        "| **boolq** (acc)         | 0.6700      | 0.7000      | +4.48% |\n",
        "| **lambada_openai** (acc)| 0.5700 | 0.1700 | -70.18% |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: 9.8 parameter reduction + 1.40x speedup\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Trade-off**: 28.4 faster inference vs accuracy changes\n"
      ],
      "metadata": {
        "id": "8msGXT-rb0eK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJlZ-oFV2O3D"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}