{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOSjlSgzagvKte/KcDVWpI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH02/CH02_NB01_Depth_pruning_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "###Â Chapter 2: Your First Model Tailoring Project\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "- Models recommended: google/gemma-3-270m\n",
        "\n",
        "- Tested with: meta-llama/Llama-3.2-1B\n",
        "_____\n",
        "\n",
        "Welcome to your first hands-on model tailoring project. In this notebook, we will follow the first two steps of a typical optimization workflow:\n",
        "\n",
        "* Establish a Baseline: We will measure the performance of a standard, pre-trained model on a few key metrics.\n",
        "\n",
        "* Perform the Surgery: We will surgically remove entire layers from the model's architecture (a technique known as depth pruning).\n",
        "\n",
        "* Evaluate the Impact: We will re-run our baseline evaluation to precisely quantify the effect of our intervention.\n",
        "\n",
        "This notebook sets the stage for the next step, covered in the book and the following notebook: recovering the model's lost knowledge through fine-tuning."
      ],
      "metadata": {
        "id": "rnrz5pV3ZdAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25LEJE2jZTTk",
        "outputId": "cc6e3792-90ff-4d51-f79f-b47fd0c32950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch optipfair lm-eval accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "bth88f19fjsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers torch optipfair lm-eval accelerate sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair.pruning.depth import prune_model_depth\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import time\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgSLeGGacKO8",
        "outputId": "5c4793e3-0a40-4f62-b99c-44c47afcb41f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "B6gVAERdfq1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"google/gemma-3-270m\"\n",
        "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
        "MAX_NEW_TOKENS = 50\n",
        "LAYERS_TO_REMOVE = 2\n",
        "TEST_PROMPT = \"Paris is the capital of\"\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "scqxXETCfpdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_params= sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {original_params}\")\n",
        "print(f\"Original layers: {len(model.model.layers)}\")\n",
        "\n",
        "print(\"=\" * 20)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Ea5mzsf0mw",
        "outputId": "5886eac6-4e39-4c9a-ecc8-a72b8b547f44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 268098176\n",
            "Original layers: 18\n",
            "====================\n",
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Test"
      ],
      "metadata": {
        "id": "buqunQ8EhURh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support functions"
      ],
      "metadata": {
        "id": "QTagr1rMhCEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean conflicting configuration after loading the model\n",
        "\n",
        "from transformers import GenerationConfig\n",
        "clean_config = GenerationConfig(\n",
        "    max_length=model.generation_config.max_length,\n",
        "    pad_token_id=model.generation_config.pad_token_id,\n",
        "    eos_token_id=model.generation_config.eos_token_id,\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    early_stopping=False\n",
        ")\n",
        "model.generation_config = clean_config\n",
        "\n",
        "print(\"âœ“ Model generation config cleaned\")\n",
        "print(f\"New config: {model.generation_config}\")"
      ],
      "metadata": {
        "id": "pvhT7W_AYORM",
        "outputId": "31bb4e72-8837-49d7-9349-144c801468b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model generation config cleaned\n",
            "New config: GenerationConfig {}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total parameters in model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
        "    \"\"\"Generate text with the model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "rbKhNpU24nUt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple prompt to test basic knowledge and coherence\n",
        "prompt = TEST_PROMPT\n",
        "generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated Text: '{generated_text}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXw_Ze4AhNgx",
        "outputId": "8acbbb2c-eb06-4918-c7fe-8f92f5ed10c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated Text: 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Evaluation"
      ],
      "metadata": {
        "id": "n9tfwHi9xpEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, limit=100):\n",
        "    \"\"\"\n",
        "    Runs lm-eval on a PyTorch model object already in memory.\n",
        "\n",
        "    Args:\n",
        "        model_obj: The PyTorch model object to evaluate.\n",
        "        tokenizer: The tokenizer object.\n",
        "        tasks (list): A list of task names.\n",
        "        limit (int): The number of samples per task.\n",
        "    \"\"\"\n",
        "    print(f\"Starting lm-eval on model '{model_obj.config._name_or_path}' for tasks: {tasks}\")\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(device)\n",
        "    )\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=model_wrapper,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        device=str(device),\n",
        "    )\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in results[\"results\"].items():\n",
        "        # Look for accuracy ('acc') first, then perplexity ('ppl')\n",
        "        if 'acc,none' in res:\n",
        "            metric_val = res.get('acc,none', 0)\n",
        "        elif 'ppl,none' in res:\n",
        "             metric_val = res.get('ppl,none', 0)\n",
        "        else:\n",
        "            metric_val = 0 # Fallback\n",
        "\n",
        "        formatted_results[task_name] = f\"{metric_val:.4f}\"\n",
        "\n",
        "    print(json.dumps(formatted_results, indent=2))\n",
        "    return formatted_results\n"
      ],
      "metadata": {
        "id": "KqfHsXnzgC03"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the benchmark suite for our diagnostic\n",
        "benchmark_tasks = ['arc_easy', 'winogrande', 'boolq', 'lambada_openai']\n",
        "\n",
        "# Run the evaluation\n",
        "baseline_results = model_evaluation(model, tokenizer, benchmark_tasks, limit=100)\n"
      ],
      "metadata": {
        "id": "SGokGEyLg-rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbXRNOFtUO7h",
        "outputId": "22badd5c-86da-4cc8-be1a-a1bdca5f4dff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'arc_easy': '0.5500',\n",
              " 'boolq': '0.6600',\n",
              " 'lambada_openai': '0.4200',\n",
              " 'winogrande': '0.6000'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depth Pruning\n",
        "Vamos a eliminar las Ãºltimas capas del modelo, basandonos en los resultados del paper:\n",
        "* Kim, B.-K., Kim, G., Kim, T.-H., Castells, T., Choi, S., Shin, J., & Song, H.-K. (2024). Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods. http://arxiv.org/abs/2402.02834"
      ],
      "metadata": {
        "id": "_sjfYCuGAIe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch"
      ],
      "metadata": {
        "id": "13xmiJ6_D9jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original model for manual pruning\n",
        "import copy\n",
        "model_manual = copy.deepcopy(model)\n",
        "print(f\"Original model structure:\")\n",
        "print(f\"  - Total layers: {len(model_manual.model.layers)}\")\n",
        "print(f\"  - Layer indices: 0 to {len(model_manual.model.layers)-1}\")\n",
        "print(f\"  - Parameters: {count_parameters(model_manual):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9STV8NaGC1gJ",
        "outputId": "f9925f75-5fd6-48f4-b790-52b09d318217"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model structure:\n",
            "  - Total layers: 18\n",
            "  - Layer indices: 0 to 17\n",
            "  - Parameters: 268,098,176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual layer removal - remove last N layers\n",
        "print(f\"\\nManually removing last {LAYERS_TO_REMOVE} layers...\")\n",
        "original_layers_count = len(model_manual.model.layers)\n",
        "new_layers_count = original_layers_count - LAYERS_TO_REMOVE\n",
        "# Create new layer list excluding the last LAYERS_TO_REMOVE layers\n",
        "new_layers = model_manual.model.layers[:new_layers_count]\n",
        "model_manual.model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "# Update model configuration to reflect the change\n",
        "model_manual.config.num_hidden_layers = len(model_manual.model.layers)\n",
        "\n",
        "# Verify the manual pruning\n",
        "manual_params = count_parameters(model_manual)\n",
        "manual_layers = len(model_manual.model.layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSf2H_9hC09q",
        "outputId": "56fc7b8b-72c9-45a7-f63f-76da098b8e77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Manually removing last 2 layers...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Manual pruning results:\")\n",
        "print(f\"  - New layer count: {manual_layers}\")\n",
        "print(f\"  - New parameter count: {manual_params:,}\")\n",
        "print(f\"  - Parameters removed: {original_params - manual_params:,}\")\n",
        "print(f\"  - Reduction: {((original_params - manual_params) / original_params * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jndg8UiyDNyb",
        "outputId": "5ac82021-41fe-492e-f84c-ab7a4a34c3eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual pruning results:\n",
            "  - New layer count: 16\n",
            "  - New parameter count: 256,950,912\n",
            "  - Parameters removed: 11,147,264\n",
            "  - Reduction: 4.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optiPfair"
      ],
      "metadata": {
        "id": "soP2xbfAEA1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another copy for optipfair pruning\n",
        "model_optipfair = copy.deepcopy(model)\n",
        "\n",
        "# Apply depth pruning using optipfair\n",
        "model_optipfair_pruned = prune_model_depth(\n",
        "    model=model_optipfair,\n",
        "    num_layers_to_remove=LAYERS_TO_REMOVE,\n",
        "    layer_selection_method=\"last\",  # Remove last layers (same as manual)\n",
        "    show_progress=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mZ1UUiADYQj",
        "outputId": "686c1e89-0d7d-4d1c-e6fa-cf2aaf8b2fdf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 269633.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify optipfair results\n",
        "optipfair_params = count_parameters(model_optipfair_pruned)\n",
        "optipfair_layers = len(model_optipfair_pruned.model.layers)\n",
        "\n",
        "print(f\"\\noptipfair pruning results:\")\n",
        "print(f\"  - New layer count: {optipfair_layers}\")\n",
        "print(f\"  - New parameter count: {optipfair_params:,}\")\n",
        "print(f\"  - Parameters removed: {original_params - optipfair_params:,}\")\n",
        "print(f\"  - Reduction: {((original_params - optipfair_params) / original_params * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xsOgRSMEoZb",
        "outputId": "8f0e1889-c5ec-4c5f-c5bf-bb8ab3e516de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "optipfair pruning results:\n",
            "  - New layer count: 16\n",
            "  - New parameter count: 256,950,912\n",
            "  - Parameters removed: 11,147,264\n",
            "  - Reduction: 4.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify they are identical\n",
        "# Compare the results\n",
        "print(\"Comparing manual vs optipfair results:\")\n",
        "print(f\"Manual method Parameters: {manual_params:,}\")\n",
        "print(f\"optipfair  Parameters: {optipfair_params:,}\")\n",
        "params_match = manual_params == optipfair_params\n",
        "\n",
        "print(f\"\\nResults verification:\")\n",
        "print(f\"  âœ“ Parameter counts match: {params_match}\")\n",
        "\n",
        "if params_match:\n",
        "    print(f\"  SUCCESS: Both methods produce identical models!\")\n",
        "    print(f\"     - From {original_params:,} â†’ {manual_params:,} parameters\")\n",
        "else:\n",
        "    print(f\"  WARNING: Methods produced different results\")\n",
        "\n",
        "# Use the optipfair model for the rest of the notebook (better validated)\n",
        "pruned_model = model_optipfair_pruned\n",
        "\n",
        "print(f\"\\nContinuing with optipfair-pruned model for evaluation...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nq-KI3ESq0",
        "outputId": "29b59f87-2144-4a5c-f784-03de2c0a6c4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing manual vs optipfair results:\n",
            "Manual method Parameters: 256,950,912\n",
            "optipfair  Parameters: 256,950,912\n",
            "\n",
            "Results verification:\n",
            "  âœ“ Parameter counts match: True\n",
            "  SUCCESS: Both methods produce identical models!\n",
            "     - From 268,098,176 â†’ 256,950,912 parameters\n",
            "\n",
            "Continuing with optipfair-pruned model for evaluation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualitative check."
      ],
      "metadata": {
        "id": "IEdo19XiHQ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple prompt to test basic knowledge and coherence\n",
        "generated_text = generate_text(pruned_model, tokenizer, TEST_PROMPT)\n",
        "generated_text_base = generate_text(model, tokenizer, TEST_PROMPT)\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generated Text Pruned: '{generated_text}'\")\n",
        "print(f\"Generated Text Base  : '{generated_text_base}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N05taPhlEXM7",
        "outputId": "519c1a10-4a2c-42b0-c80d-ee2e11c5ae48"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated Text Pruned: 'Paris is the capital of France and one of the largest cities in Europe. It occupies approximately 2.5 million hectares of land surrounded by mountains and forests. Parisians love to travel abroad because they enjoy sightseeing tours abroad. Tourists visiting Paris visit museums, monuments, theaters,'\n",
            "Generated Text Base  : 'Paris is the capital of France. It is located in the middle of the country and has a population of about 10 million people. Paris is a city with a rich history and culture. The city is known for its beautiful architecture, art, and history. There are'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruned Evaluation"
      ],
      "metadata": {
        "id": "jCuzNq7lHxY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation with lm-eval\n",
        "# Run evaluation on the pruned model object we have in memory\n",
        "print(\"--- Evaluating Pruned Model ---\")\n",
        "\n",
        "# The 'pruned_model' variable holds the model we modified in Section 2.1\n",
        "pruned_results = model_evaluation(pruned_model, tokenizer, benchmark_tasks, limit=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXMBDkoJGHYA",
        "outputId": "fbb070e8-fba9-42bd-a1d1-b19597e5f7b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating Pruned Model ---\n",
            "Starting lm-eval on model 'google/gemma-3-270m' for tasks: ['arc_easy', 'winogrande', 'boolq', 'lambada_openai']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of arc_easy from None to 0\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 158.25it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 501.33it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 18209.19it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 260.48it/s]\n",
            "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899/899 [00:43<00:00, 20.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bootstrapping for stddev: perplexity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 54.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"arc_easy\": \"0.4600\",\n",
            "  \"boolq\": \"0.4500\",\n",
            "  \"lambada_openai\": \"0.3400\",\n",
            "  \"winogrande\": \"0.4800\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Calculate parameter reduction\n",
        "pruned_params = count_parameters(pruned_model)\n",
        "param_reduction_pct = (original_params - pruned_params) / original_params\n",
        "\n",
        "# Helper function to calculate percentage change\n",
        "def calculate_change(old, new):\n",
        "    old, new = float(old), float(new)\n",
        "    if old == 0:\n",
        "        return \"N/A\"\n",
        "    return f\"{(new - old) / old:+.2%}\"\n",
        "\n",
        "# Create the comparison table\n",
        "model_id = MODEL_NAME.split('/')[-1]  # Get just the model name without org\n",
        "\n",
        "markdown_table = f\"\"\"\n",
        "## Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`{model_id}`) | Pruned Model (-{LAYERS_TO_REMOVE} Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | {original_params:,}              | {pruned_params:,}          | **-{param_reduction_pct:.2%}** |\n",
        "| **arc_easy** (acc)      | {baseline_results['arc_easy']}   | {pruned_results['arc_easy']} | {calculate_change(baseline_results['arc_easy'], pruned_results['arc_easy'])} |\n",
        "| **winogrande** (acc)    | {baseline_results['winogrande']} | {pruned_results['winogrande']} | {calculate_change(baseline_results['winogrande'], pruned_results['winogrande'])} |\n",
        "| **boolq** (acc)         | {baseline_results['boolq']}      | {pruned_results['boolq']}      | {calculate_change(baseline_results['boolq'], pruned_results['boolq'])} |\n",
        "| **lambada_openai** (acc)| {baseline_results['lambada_openai']} | {pruned_results['lambada_openai']} | {calculate_change(baseline_results['lambada_openai'], pruned_results['lambada_openai'])} |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: {param_reduction_pct:.1%} parameter reduction â†’ faster inference & lower memory\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Next Step**: Knowledge recovery through fine-tuning (Section 2.3)\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(markdown_table))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "JN0ymC8FIKP9",
        "outputId": "e8ae0f86-7acf-4113-aac8-3f517919d62d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Performance Impact Analysis\n\n| Metric                  | Original Model (`gemma-3-270m`) | Pruned Model (-2 Layers) | Change          |\n| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n| **Parameters**          | 268,098,176              | 256,950,912          | **-4.16%** |\n| **arc_easy** (acc)      | 0.5500   | 0.4600 | -16.36% |\n| **winogrande** (acc)    | 0.6000 | 0.4800 | -20.00% |\n| **boolq** (acc)         | 0.6600      | 0.4500      | -31.82% |\n| **lambada_openai** (acc)| 0.4200 | 0.3400 | -19.05% |\n\n### Key Insights:\n- **Efficiency Gain**: 4.2% parameter reduction â†’ faster inference & lower memory\n- **Performance Impact**: See individual benchmark changes above\n- **Next Step**: Knowledge recovery through fine-tuning (Section 2.3)\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2.2.3 Inference Speed Measurement\n",
        "\n",
        "print(\"\\n--- 2.2.3 Measuring Actual Inference Speed ---\")\n",
        "print(\"Let's measure the real speed improvements from removing layers\")\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def measure_inference_time(model, tokenizer, prompts, num_runs=5):\n",
        "    \"\"\"Measure average inference time across multiple runs\"\"\"\n",
        "    times = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        for prompt in prompts:\n",
        "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "            with torch.no_grad():\n",
        "                _ = model.generate(\n",
        "                    inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_new_tokens=50,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "\n",
        "    return {\n",
        "        'mean_time': np.mean(times),\n",
        "        'std_time': np.std(times),\n",
        "        'all_times': times\n",
        "    }\n",
        "\n",
        "# Test prompts for speed measurement\n",
        "speed_test_prompts = [\n",
        "    \"The capital of France is\",\n",
        "    \"Machine learning is\",\n",
        "    \"Climate change refers to\"\n",
        "]\n",
        "\n",
        "print(f\"Measuring inference speed across {len(speed_test_prompts)} prompts, 5 runs each...\")\n",
        "\n",
        "# Warm up GPU\n",
        "print(\"Warming up models...\")\n",
        "for _ in range(2):\n",
        "    _ = generate_text(model, tokenizer, \"warmup\", max_new_tokens=10)\n",
        "    _ = generate_text(pruned_model, tokenizer, \"warmup\", max_new_tokens=10)\n",
        "\n",
        "# Measure original model\n",
        "print(\"Measuring original model...\")\n",
        "original_timing = measure_inference_time(model, tokenizer, speed_test_prompts)\n",
        "\n",
        "# Measure pruned model\n",
        "print(\"Measuring pruned model...\")\n",
        "pruned_timing = measure_inference_time(pruned_model, tokenizer, speed_test_prompts)\n",
        "\n",
        "# Calculate speedup\n",
        "speedup = original_timing['mean_time'] / pruned_timing['mean_time']\n",
        "time_reduction_pct = (original_timing['mean_time'] - pruned_timing['mean_time']) / original_timing['mean_time']\n",
        "\n",
        "print(f\"\\nðŸ“Š Inference Speed Results:\")\n",
        "print(f\"   Original model: {original_timing['mean_time']:.3f}s Â± {original_timing['std_time']:.3f}s\")\n",
        "print(f\"   Pruned model:   {pruned_timing['mean_time']:.3f}s Â± {pruned_timing['std_time']:.3f}s\")\n",
        "print(f\"   Speedup:        {speedup:.2f}x ({time_reduction_pct:+.1%} faster)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4gAdU5yIe6u",
        "outputId": "0378ffd8-a1f3-456c-ce00-597e80abc1b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2.2.3 Measuring Actual Inference Speed ---\n",
            "Let's measure the real speed improvements from removing layers\n",
            "Measuring inference speed across 3 prompts, 5 runs each...\n",
            "Warming up models...\n",
            "Measuring original model...\n",
            "Measuring pruned model...\n",
            "\n",
            "ðŸ“Š Inference Speed Results:\n",
            "   Original model: 10.324s Â± 6.431s\n",
            "   Pruned model:   4.322s Â± 0.171s\n",
            "   Speedup:        2.39x (+58.1% faster)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the comparison table to include measured speed\n",
        "markdown_table = f\"\"\"\n",
        "## Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`{model_id}`) | Pruned Model (-{LAYERS_TO_REMOVE} Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | {original_params:,}              | {pruned_params:,}          | **-{param_reduction_pct:.2%}** |\n",
        "| **Inference Time**      | {original_timing['mean_time']:.3f}s | {pruned_timing['mean_time']:.3f}s | **{time_reduction_pct:+.1%}** |\n",
        "| **arc_easy** (acc)      | {baseline_results['arc_easy']}   | {pruned_results['arc_easy']} | {calculate_change(baseline_results['arc_easy'], pruned_results['arc_easy'])} |\n",
        "| **winogrande** (acc)    | {baseline_results['winogrande']} | {pruned_results['winogrande']} | {calculate_change(baseline_results['winogrande'], pruned_results['winogrande'])} |\n",
        "| **boolq** (acc)         | {baseline_results['boolq']}      | {pruned_results['boolq']}      | {calculate_change(baseline_results['boolq'], pruned_results['boolq'])} |\n",
        "| **lambada_openai** (acc)| {baseline_results['lambada_openai']} | {pruned_results['lambada_openai']} | {calculate_change(baseline_results['lambada_openai'], pruned_results['lambada_openai'])} |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: {param_reduction_pct:.1%} parameter reduction + {speedup:.2f}x speedup\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Trade-off**: {time_reduction_pct:.1%} faster inference vs accuracy changes\n",
        "\"\"\"\n",
        "display(Markdown(markdown_table))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "gg5i8qOhK3ic",
        "outputId": "19173528-1e04-4856-8607-e37051ef8264"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Performance Impact Analysis\n\n| Metric                  | Original Model (`gemma-3-270m`) | Pruned Model (-2 Layers) | Change          |\n| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n| **Parameters**          | 268,098,176              | 256,950,912          | **-4.16%** |\n| **Inference Time**      | 10.324s | 4.322s | **+58.1%** |\n| **arc_easy** (acc)      | 0.5500   | 0.4600 | -16.36% |\n| **winogrande** (acc)    | 0.6000 | 0.4800 | -20.00% |\n| **boolq** (acc)         | 0.6600      | 0.4500      | -31.82% |\n| **lambada_openai** (acc)| 0.4200 | 0.3400 | -19.05% |\n\n### Key Insights:\n- **Efficiency Gain**: 4.2% parameter reduction + 2.39x speedup\n- **Performance Impact**: See individual benchmark changes above\n- **Trade-off**: 58.1% faster inference vs accuracy changes\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Llama-3.2-1B Performance Impact Analysis\n",
        "\n",
        "| Metric                  | Original Model (`{model_id}`) | Pruned Model (-{LAYERS_TO_REMOVE} Layers) | Change          |\n",
        "| :---------------------- | :----------------------------- | :----------------------- | :-------------- |\n",
        "| **Parameters**          | 1,235,814,400              | 1,117,171,392          | **-9.84%** |\n",
        "| **Inference Time**      | 6.635s | 4,752s | **+28.4%** |\n",
        "| **arc_easy** (acc)      | 0.6600   | 0.4800 | -27.27% |\n",
        "| **winogrande** (acc)    | 0.6000 | 0.5400 | -10% |\n",
        "| **boolq** (acc)         | 0.6700      | 0.7000      | +4.48% |\n",
        "| **lambada_openai** (acc)| 0.5700 | 0.1700 | -70.18% |\n",
        "\n",
        "### Key Insights:\n",
        "- **Efficiency Gain**: 9.8 parameter reduction + 1.40x speedup\n",
        "- **Performance Impact**: See individual benchmark changes above\n",
        "- **Trade-off**: 28.4 faster inference vs accuracy changes\n"
      ],
      "metadata": {
        "id": "8msGXT-rb0eK"
      }
    }
  ]
}