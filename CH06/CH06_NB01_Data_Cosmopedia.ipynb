{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/main/CH06/CH06_NB01_Data_Cosmopedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7hULT_n96KoD"
            },
            "source": [
                "# **Tailoring LLM Architectures**\n",
                "## **Chapter 6: The Fuel ‚Äî Data Strategy for Knowledge Recovery**\n",
                "\n",
                "### **Notebook 1: Quality vs Quantity (Cosmopedia)**\n",
                "by [Pere Martra](https://github.com/peremartra)\n",
                "\n",
                "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![Hugging Face](https://img.shields.io/badge/ü§ó%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
                "\n",
                "___\n",
                "\n",
                "**Colab Environment:** GPU T4  \n",
                "- **Models recommended:** google/gemma-3-270m  \n",
                "- **Tested with:** meta-llama/Llama-3.2-1B using a **GPU A100**\n",
                "\n",
                "---\n",
                "\n",
                "In this notebook, we challenge the standard. We will use only 15,000 high-quality data samples (Cosmopedia) to attempt to beat the results we obtained in Chapter 2 with 30,000 web crawl samples (SlimPajama).\n",
                "\n",
                "**What we'll accomplish:**\n",
                "- **Load our models**: Original (teacher) and pruned (student) (Reusing logic from Ch2)\n",
                "- **Prepare High-Quality Data**: Load synthetic textbooks from Cosmopedia\n",
                "- **Apply Knowledge Distillation**: Train the pruned model with high-quality data\n",
                "- **Measure recovery**: Compare against the baseline set in Chapter 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "d0OhWq985W0i",
                "outputId": "d9fa1cf3-3b8c-4342-dc49-43029b2aa335"
            },
            "outputs": [],
            "source": [
                "# Libraries\n",
                "# Install required packages\n",
                "!pip install -q transformers torch optipfair datasets accelerate sentencepiece lm-eval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "baseline_results_cell"
            },
            "outputs": [],
            "source": [
                "# --- BASELINE RESULTS (FROM CHAPTER 2) ---\n",
                "# We use the results obtained in CH02_NB02 as our baseline to beat.\n",
                "# Dataset: SlimPajama (Web Crawl)\n",
                "# Size: 30,000 samples\n",
                "# Training: 3 Epochs\n",
                "baseline_metrics = {\n",
                "    \"name\": \"SlimPajama (30k samples)\",\n",
                "    \"arc_easy\": 0.5800,      # Value from Ch2\n",
                "    \"winogrande\": 0.5500,    # Value from Ch2\n",
                "    \"boolq\": 0.5400          # Value from Ch2\n",
                "}\n",
                "\n",
                "print(f\"üéØ Target to beat: ARC-Easy {baseline_metrics['arc_easy']:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "pl-L82JK9BhG",
                "outputId": "941eaea5-45fe-4d68-ba8a-69dd5a602f57"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from torch.optim import AdamW\n",
                "from optipfair import prune_model\n",
                "from datasets import load_dataset\n",
                "from torch.nn import functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from lm_eval import evaluator\n",
                "from lm_eval.models.huggingface import HFLM\n",
                "import time\n",
                "import json\n",
                "from typing import Dict, List, Any\n",
                "import copy\n",
                "\n",
                "# Check device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LRscks9QSEqt"
            },
            "source": [
                "# 2.4 Recovering knowledge with distillation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MRj--518-Vkv"
            },
            "source": [
                "## Load Libraries & Models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0Js9yx_29Uv2"
            },
            "outputs": [],
            "source": [
                "# Model configuration (consistent with previous notebook)\n",
                "MODEL_NAME = \"google/gemma-3-270m\"\n",
                "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
                "MAX_NEW_TOKENS = 50\n",
                "LAYERS_TO_REMOVE = 2 #Try removing 4, 6, or even 8 layers\n",
                "TEST_PROMPT = \"Paris is the capital of\"\n",
                "\n",
                "print(f\"Loading base model: {MODEL_NAME}\")\n",
                "\n",
                "# Load the original model (this will be our TEACHER)\n",
                "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# Clean generation config (same as previous notebook)\n",
                "from transformers import GenerationConfig\n",
                "clean_config = GenerationConfig(\n",
                "    max_length=teacher_model.generation_config.max_length,\n",
                "    pad_token_id=teacher_model.generation_config.pad_token_id,\n",
                "    eos_token_id=teacher_model.generation_config.eos_token_id,\n",
                "    do_sample=False,\n",
                "    num_beams=1,\n",
                "    early_stopping=False\n",
                ")\n",
                "teacher_model.generation_config = clean_config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "9sNjhJ_X-LFJ",
                "outputId": "fb75e9af-de4b-4b87-e056-12c600e32b26"
            },
            "outputs": [],
            "source": [
                "original_params = sum(p.numel() for p in teacher_model.parameters())\n",
                "print(f\"Teacher model parameters: {original_params:,}\")\n",
                "print(f\"Teacher model layers: {len(teacher_model.model.layers)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "heQC0dTP_UtK"
            },
            "source": [
                "### Create Student Model with optiPfair\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "oPdWmAal-STI",
                "outputId": "16ad7bd0-d366-49bd-8dec-d0c04573fb48"
            },
            "outputs": [],
            "source": [
                "# Create the STUDENT model by pruning (same process as previous notebook)\n",
                "print(f\"\\nCreating student model by removing {LAYERS_TO_REMOVE} layers...\")\n",
                "\n",
                "# Apply depth pruning using optipfair\n",
                "student_model = prune_model(\n",
                "    model=copy.deepcopy(teacher_model),\n",
                "    pruning_type=\"DEPTH\",\n",
                "    num_layers_to_remove=LAYERS_TO_REMOVE,\n",
                "    layer_selection_method=\"last\",\n",
                "    show_progress=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "mpaF1YAm_Lnl",
                "outputId": "1fe1a975-b2e3-484d-cbc4-697db407bf05"
            },
            "outputs": [],
            "source": [
                "student_params = sum(p.numel() for p in student_model.parameters())\n",
                "param_reduction = (original_params - student_params) / original_params\n",
                "\n",
                "print(f\"Student model parameters: {student_params:,}\")\n",
                "print(f\"Parameter reduction: {param_reduction:.1%}\")\n",
                "print(f\"Student model layers: {len(student_model.model.layers)}\")\n",
                "\n",
                "student_model.gradient_checkpointing_enable()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-CKJcRUE_1LK"
            },
            "source": [
                "## Support Functions & Basic Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "8OWwlMgJHw49"
            },
            "outputs": [],
            "source": [
                "def model_evaluation(model_obj, tokenizer, tasks, limit=100):\n",
                "    \"\"\"\n",
                "    Runs lm-eval on a PyTorch model object already in memory.\n",
                "\n",
                "    Args:\n",
                "        model_obj: The PyTorch model object to evaluate.\n",
                "        tokenizer: The tokenizer object.\n",
                "        tasks (list): A list of task names.\n",
                "        limit (int): The number of samples per task.\n",
                "    \"\"\"\n",
                "    print(f\"Starting lm-eval on model '{model_obj.config._name_or_path}' for tasks: {tasks}\")\n",
                "\n",
                "    # Wrap the local model object and tokenizer for lm-eval\n",
                "    model_wrapper = HFLM(\n",
                "        pretrained=model_obj,\n",
                "        tokenizer=tokenizer,\n",
                "        device=str(device)\n",
                "    )\n",
                "\n",
                "    results = evaluator.simple_evaluate(\n",
                "        model=model_wrapper,\n",
                "        tasks=tasks,\n",
                "        num_fewshot=0,\n",
                "        limit=limit,\n",
                "        device=str(device),\n",
                "    )\n",
                "\n",
                "    # Format results for clean display\n",
                "    formatted_results = {}\n",
                "    for task_name, res in results[\"results\"].items():\n",
                "        # Look for accuracy ('acc') first, then perplexity ('ppl')\n",
                "        if 'acc,none' in res:\n",
                "            metric_val = res.get('acc,none', 0)\n",
                "        elif 'ppl,none' in res:\n",
                "             metric_val = res.get('ppl,none', 0)\n",
                "        else:\n",
                "            metric_val = 0 # Fallback\n",
                "\n",
                "        formatted_results[task_name] = f\"{metric_val:.4f}\"\n",
                "\n",
                "    print(json.dumps(formatted_results, indent=2))\n",
                "    return float(formatted_results.get(tasks[0], 0))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "lIFKAxxc_wTw",
                "outputId": "6eacc005-2f04-4172-e14b-26162aea68ff"
            },
            "outputs": [],
            "source": [
                "# Quick baseline test - confirm degradation from previous notebook\n",
                "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
                "    \"\"\"Generate text with the model (same function as previous notebook)\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            inputs['input_ids'],\n",
                "            attention_mask=inputs['attention_mask'],\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            num_return_sequences=1,\n",
                "            pad_token_id=tokenizer.pad_token_id,\n",
                "            do_sample=False,\n",
                "            num_beams=3,\n",
                "            early_stopping=True,\n",
                "            no_repeat_ngram_size=2\n",
                "        )\n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Test both models with the same prompt\n",
                "print(f\"\\n--- Baseline Test: '{TEST_PROMPT}' ---\")\n",
                "teacher_output = generate_text(teacher_model, tokenizer, TEST_PROMPT)\n",
                "student_output = generate_text(student_model, tokenizer, TEST_PROMPT)\n",
                "\n",
                "print(f\"Teacher: '{teacher_output}'\")\n",
                "print(f\"Student: '{student_output}'\")\n",
                "print(\"\\nReady for knowledge recovery...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "EJQW3EJ4Csol"
            },
            "source": [
                "## Dataset Preparation: Cosmopedia\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dataset_loading_change"
            },
            "outputs": [],
            "source": [
                "# --- DATASET: COSMOPEDIA (High Quality) ---\n",
                "# We use the 'stories' subset which contains synthetic textbooks and stories.\n",
                "print(\"Loading Cosmopedia (Textbook Quality)...\")\n",
                "\n",
                "dataset_name = \"HuggingFaceTB/cosmopedia\"\n",
                "subset = \"stories\"\n",
                "num_samples = 15000  # HALF the size of Chapter 2!\n",
                "\n",
                "dataset = load_dataset(dataset_name, subset, split=\"train\", streaming=True)\n",
                "\n",
                "# Tokenization pipeline (Igual que en Ch2 pero adaptado a la columna 'text')\n",
                "def get_data_loader(dataset, num_samples, batch_size=8):\n",
                "    data = []\n",
                "    # Add shuffling with buffer for streaming\n",
                "    dataset = dataset.shuffle(seed=42, buffer_size=1000)\n",
                "    \n",
                "    for i, sample in enumerate(dataset):\n",
                "        if i >= num_samples: break\n",
                "        \n",
                "        # Cosmopedia stories normally use 'text' column, strictly checking just in case\n",
                "        text = sample.get('text', sample.get('prompt', '') + ' ' + sample.get('completion', ''))\n",
                "        \n",
                "        tokenized = tokenizer(\n",
                "            text,\n",
                "            truncation=True,\n",
                "            padding=\"max_length\",\n",
                "            max_length=128, # Short context is enough for this demo\n",
                "            return_tensors=\"pt\"\n",
                "        )\n",
                "        data.append({\n",
                "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
                "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
                "        })\n",
                "    \n",
                "    return DataLoader(data, batch_size=batch_size, shuffle=True)\n",
                "\n",
                "kd_dataloader = get_data_loader(dataset, num_samples)\n",
                "print(f\"Data ready: {num_samples} high-quality samples.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "mC38ykgpCuy9",
                "outputId": "232bd4d0-526c-494f-f375-e135ec8f3b3d"
            },
            "outputs": [],
            "source": [
                "# Move models to device and set appropriate modes\n",
                "teacher_model.to(device)\n",
                "student_model.to(device)\n",
                "\n",
                "# Teacher stays in eval mode - we don't train it\n",
                "teacher_model.eval()\n",
                "\n",
                "# Student will be trained\n",
                "student_model.train()\n",
                "\n",
                "# KD Hyperparameters\n",
                "TEMPERATURE = 2.0      # Softens probability distributions\n",
                "ALPHA = 1.0           # Weight for distillation loss\n",
                "NUM_EPOCHS = 3        # Conservative for demo\n",
                "LEARNING_RATE = 1e-5  # Lower LR for stability\n",
                "ACCUMULATION_STEPS = 4  # Effective batch size = 4 * 8 = 32\n",
                "\n",
                "# Optimizer for student model only\n",
                "optimizer = AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "print(f\"Knowledge Distillation Configuration:\")\n",
                "print(f\"  Temperature: {TEMPERATURE}\")\n",
                "print(f\"  Alpha: {ALPHA}\")\n",
                "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
                "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
                "print(f\"  Effective Batch Size: {4 * ACCUMULATION_STEPS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "visualizing_temp"
            },
            "outputs": [],
            "source": [
                "# --- SIDEBAR: VISUALIZING TEMPERATURE ---\n",
                "# Before training, let's understand the T parameter we are about to use.\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "def plot_temperature_scaling(logits, temperatures=[1.0, 2.0, 4.0]):\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    tokens = ['London', 'Paris', 'Madrid', 'Rome'] # Example tokens\n",
                "    \n",
                "    for T in temperatures:\n",
                "        # Softmax formula with T\n",
                "        exp_logits = np.exp(np.array(logits) / T)\n",
                "        probs = exp_logits / np.sum(exp_logits)\n",
                "        \n",
                "        plt.plot(tokens, probs, marker='o', label=f'T={T}')\n",
                "        \n",
                "    plt.title(\"How Temperature affects 'Dark Knowledge'\")\n",
                "    plt.ylabel(\"Probability\")\n",
                "    plt.legend()\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.show()\n",
                "\n",
                "# Simulating logits where Index 1 (Paris) is the correct answer\n",
                "dummy_logits = [2.0, 8.0, 4.5, 1.0] \n",
                "print(\"Visualizing Temperature Scaling (T=2.0 is our choice):\")\n",
                "plot_temperature_scaling(dummy_logits)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "DKAKi0K8B95H",
                "outputId": "fe525e34-a1be-4c9f-b0f9-f5caaef7236a"
            },
            "outputs": [],
            "source": [
                "print(f\"\\n>>> STARTING TRAINING WITH COSMOPEDIA ({num_samples} samples) <<<\")\n",
                "print(f\"Training student model to mimic teacher behavior \\n\")\n",
                "\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "  student_model.train()\n",
                "  total_loss = 0\n",
                "  num_batches = 0\n",
                "  for batch_idx, batch in enumerate(kd_dataloader):\n",
                "    # ###Step 1: Move batch to device###\n",
                "    input_ids = batch['input_ids'].to(device)\n",
                "    attention_mask = batch['attention_mask'].to(device)\n",
                "\n",
                "    # Move teacher model to device, perform inference, and move back to CPU\n",
                "    teacher_model.to(device)\n",
                "\n",
                "    # ###Step 2: The Master is asked to generate its logits###\n",
                "    with torch.no_grad():\n",
                "      teacher_outputs = teacher_model(\n",
                "        input_ids=input_ids,\n",
                "        attention_mask=attention_mask\n",
                "      )\n",
                "      teacher_logits = teacher_outputs.logits / TEMPERATURE\n",
                "\n",
                "    # Moving teacher model to CPU to save memory.\n",
                "    teacher_model.cpu()\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "    # ###Step 3: Get Student's \"thoughts\" (logits) with training enabled###\n",
                "    # Student inference (with gradients)\n",
                "    student_outputs = student_model(\n",
                "      input_ids=input_ids,\n",
                "      attention_mask=attention_mask\n",
                "    )\n",
                "    student_logits = student_outputs.logits / TEMPERATURE\n",
                "\n",
                "    # Compute Knowledge Distillation loss\n",
                "    teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
                "    student_log_probs = F.log_softmax(student_logits, dim=-1)\n",
                "\n",
                "    # ###Step 4:\n",
                "    # KL Divergence loss\n",
                "    kd_loss = F.kl_div(\n",
                "      student_log_probs,\n",
                "      teacher_probs,\n",
                "      reduction='batchmean'\n",
                "    )\n",
                "\n",
                "    # Scale loss for gradient accumulation\n",
                "    loss = kd_loss / ACCUMULATION_STEPS\n",
                "    loss.backward()\n",
                "\n",
                "    # Gradient accumulation\n",
                "    if (batch_idx + 1) % ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(kd_dataloader):\n",
                "      optimizer.step()\n",
                "      optimizer.zero_grad()\n",
                "\n",
                "    total_loss += loss.item() * ACCUMULATION_STEPS\n",
                "    num_batches += 1\n",
                "    # Progress update\n",
                "\n",
                "    if (batch_idx + 1) % 100 == 0:\n",
                "      avg_loss = total_loss / num_batches\n",
                "      print(f'Epoch {epoch + 1}/{NUM_EPOCHS} | Batch {batch_idx + 1} | Loss: {avg_loss:.4f}')\n",
                "\n",
                "  # Epoch summary\n",
                "  avg_epoch_loss = total_loss / num_batches\n",
                "  print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} | Average Loss: {avg_epoch_loss:.4f}\")\n",
                "\n",
                "print(f\"\\nüéâ Knowledge Distillation completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jww6LbeRHC1K"
            },
            "source": [
                "## Basic Test generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "1W0-J7MuFqiO",
                "outputId": "97589fd2-38ed-46af-9684-c9b8cf7583cf"
            },
            "outputs": [],
            "source": [
                "# Set student model to evaluation mode\n",
                "teacher_model.to(device)\n",
                "student_model.eval()\n",
                "# Test with the same prompt used in baseline\n",
                "print(f\"--- Qualitative Test: '{TEST_PROMPT}' ---\")\n",
                "\n",
                "# Generate with all three models for comparison\n",
                "teacher_output = generate_text(teacher_model, tokenizer, TEST_PROMPT)\n",
                "student_baseline_output = generate_text(student_model, tokenizer, TEST_PROMPT)\n",
                "\n",
                "print(f\"Teacher (Original):    '{teacher_output}'\")\n",
                "print(f\"Student (Post-KD):     '{student_baseline_output}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-3Wi3YX5H9nQ"
            },
            "source": [
                "## Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "xhPEAh9LHWl5"
            },
            "outputs": [],
            "source": [
                "# Define the benchmark suite for our diagnostic\n",
                "benchmark_tasks = ['arc_easy', 'winogrande', 'hellaswag', 'lambada_openai']\n",
                "#student_recovered_results = model_evaluation(student_model, tokenizer, benchmark_tasks, limit=100)\n",
                "\n",
                "# --- FINAL COMPARISON ---\n",
                "print(\"Evaluating Cosmopedia Student...\")\n",
                "# Run evaluation (reuse your evaluate function)\n",
                "cosmo_acc = model_evaluation(student_model, tokenizer, ['arc_easy'], limit=100)\n",
                "\n",
                "print(\"\\nüèÜ QUALITY VS QUANTITY SHOWDOWN üèÜ\")\n",
                "print(f\"{'Dataset':<25} | {'Samples':<10} | {'ARC-Easy Acc':<15} | {'Result'}\")\n",
                "print(\"-\" * 65)\n",
                "print(f\"{baseline_metrics['name']:<25} | {'30,000':<10} | {baseline_metrics['arc_easy']:.4f}          | üõë Baseline\")\n",
                "print(f\"{'Cosmopedia (Stories)':<25} | {'15,000':<10} | {cosmo_acc:.4f}          | {'‚úÖ WINNER' if cosmo_acc > baseline_metrics['arc_easy'] else '‚ùå LOWER'}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "authorship_tag": "ABX9TyOjnnuvcG46eKyBqkLskokR",
            "gpuType": "L4",
            "include_colab_link": true,
            "machine_shape": "hm",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
