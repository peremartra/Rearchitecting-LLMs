{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/CH06/CH06/CH06_NB01_Data_Cosmopedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hULT_n96KoD"
      },
      "source": [
        "# **Tailoring LLM Architectures**\n",
        "## **Chapter 6: The Fuel â€” Data Strategy for Knowledge Recovery**\n",
        "\n",
        "### **Notebook 1: Quality vs Quantity (Cosmopedia)**\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "\n",
        "___\n",
        "\n",
        "**Colab Environment:** GPU T4  \n",
        "- **Models recommended:** google/gemma-3-270m  \n",
        "- **Tested with:** meta-llama/Llama-3.2-1B using a **GPU A100**\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we challenge the standard. We will use only 15,000 high-quality data samples (Cosmopedia) to attempt to beat the results we obtained in Chapter 2 with 30,000 web crawl samples (SlimPajama).\n",
        "\n",
        "**What we'll accomplish:**\n",
        "- **Load our models**: Original (teacher) and pruned (student) (Reusing logic from Ch2)\n",
        "- **Prepare High-Quality Data**: Load synthetic textbooks from Cosmopedia\n",
        "- **Apply Knowledge Distillation**: Train the pruned model with high-quality data\n",
        "- **Measure recovery**: Compare against the baseline set in Chapter 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0OhWq985W0i",
        "outputId": "964fd608-d654-4b3d-a95f-346340f49c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Libraries\n",
        "# Install required packages\n",
        "!pip install -q transformers torch optipfair datasets accelerate sentencepiece lm-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "baseline_results_cell",
        "outputId": "c851e634-b5e4-4387-eae1-739ce19f76ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Target to beat: ARC-Easy 52.00%\n"
          ]
        }
      ],
      "source": [
        "# --- BASELINE RESULTS (FROM CHAPTER 2) ---\n",
        "# We use the results obtained in CH02_NB02 as our baseline to beat.\n",
        "# Dataset: SlimPajama (Web Crawl)\n",
        "# Size: 30,000 samples\n",
        "# Training: 3 Epochs\n",
        "baseline_metrics = {\n",
        "    \"name\": \"SlimPajama (15k samples)\",\n",
        "    \"arc_easy\": 0.5200,      # Value from Ch2\n",
        "    \"hellaswang\": 0.4100,      # Value from Ch2\n",
        "    \"lambada_openai\": 0.3900,    # Value from Ch2\n",
        "    \"winogrande\": 0.5800          # Value from Ch2\n",
        "}\n",
        "\n",
        "print(f\"ğŸ¯ Target to beat: ARC-Easy {baseline_metrics['arc_easy']:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl-L82JK9BhG",
        "outputId": "6e1aa2a0-44f0-4544-aa6c-f36af5da6380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "from optipfair import prune_model\n",
        "from datasets import load_dataset\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import time\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "import copy\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from transformers import set_seed\n",
        "\n",
        "def set_reproducibility(seed=42):\n",
        "    # 1. Semilla para Python y librerÃ­as bÃ¡sicas\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 2. Semilla para PyTorch (CPU y GPU)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # Para multi-GPU\n",
        "\n",
        "    # 3. Configurar cuDNN para que sea determinista\n",
        "    # Nota: Esto puede ralentizar un poco el entrenamiento\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 4. Semilla para Transformers (Hugging Face)\n",
        "    set_seed(seed)\n",
        "\n",
        "    print(f\"âœ… Seed {seed} established.\")\n",
        "\n",
        "set_reproducibility(42)"
      ],
      "metadata": {
        "id": "N3Lz3YfP8Lec",
        "outputId": "a0561a0a-f8f4-42fa-ac55-509706115be9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Seed 42 established.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}