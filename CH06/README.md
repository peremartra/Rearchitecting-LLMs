# Chapter 6: Universal Knowledge Distillation - Engineering Plan

| Technical Component | Purpose in Chapter 6 | SOTA Implementation (Industrial Level) | Technical Implementation Strategy |
| :--- | :--- | :--- | :--- |
| **1. Textbook-Quality Data** (Simplified) | **NEW:** Focus on high-quality input without complex curation steps. We assume the data is ready. | Use of "Textbook Quality" datasets (Microsoft Phi / NVIDIA approach) instead of raw web text. | Direct loading of **Cosmopedia** (Hugging Face) to ensure dense information density. No custom generation script. |
| **2. Hidden State Distillation** | Move beyond "imitation" (output) to "understanding" (process). Force the student to replicate internal representations. | Access `hidden_states`. Use **MSE Loss** to align the student's intermediate tensors with the teacher's. | Hook into `outputs.hidden_states`. Compute distance between selected layers. |
| **3. Intelligent Layer Mapping** | Solve the architectural mismatch caused by **Depth Pruning** (Student has fewer layers). | **Uniform Strategy** (skip every N layers) or **Last-Layer Strategy** (focus on deep reasoning). | A mapping function `map_layers(student_n, teacher_n)` that returns a dictionary `{student_idx: teacher_idx}`. |
| **4. Learnable Projections** | Solve the dimensional mismatch caused by **Width Pruning** (Student vectors are smaller). | Insert a Trainable Linear Layer ($W_{proj}$) that learns to translate "Student Space" to "Teacher Space". | `nn.Linear(student_dim, teacher_dim)` initialized specifically for the distiller. Only these weights (and the student's) are updated. |
| **5. Compound Loss Landscape** | Balance factual accuracy with reasoning emulation. | $L_{Total} = \alpha L_{Task} + \beta L_{KL} + \gamma L_{Hidden}$ | Custom `compute_loss` combining CrossEntropy (Hard Targets), KL Divergence (Soft Targets), and MSE (Features). |
| **6. Custom Trainer Loop** | Full control over the optimization process for non-standard architectures. | Subclassing Hugging Face `Trainer` to manage the projector's lifecycle. | Create `UniversalDistillationTrainer` class. Override `compute_loss` and ensure projectors are saved with the model. |
