{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"colab_type": "text", "id": "view-in-github"},
      "source": ["<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/main/CH06/CH06_NB02_Logits_KLD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "title_header"},
      "source": ["# **Tailoring LLM Architectures**\n", "## **Chapter 6: Knowledge Distillation with Logits Only**\n", "### Training a pruned model using only Logits and KL Divergence\n", "\n", "by [Pere Martra](https://github.com/peremartra)\n", "\n", "---\n", "\n", "**Hardware Environment:** NVIDIA A100 GPU\n", "- **Model:** google/gemma-3-270m (Teacher: 18 transformer blocks) → Pruned Student (14 transformer blocks)\n", "- **Dataset:** Cosmopedia (40,000 samples, 3 epochs)\n", "\n", "---\n", "\n", "**What we'll accomplish:**\n", "- Train a depth-pruned student model using **only logits and KL Divergence**\n", "- No hidden state alignment (simpler, faster training)\n", "- Save the trained model to Hugging Face as **gem-3-small**"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section0_header"},
      "source": ["## Section 0: Environment & Dependencies"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "mount_drive"},
      "outputs": [],
      "source": ["from google.colab import drive\n", "drive.mount('/content/drive')\n", "print(\"✓ Drive mounted\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "config_cell"},
      "outputs": [],
      "source": ["RECOVERY_SAMPLES = 40000\n", "EPOCHS=3\n", "LEARNING_RATE=4e-5\n", "BATCH_SIZE = 16\n", "BATCH_EVAL=\"auto\"\n", "RUN_FULL_BENCHMARKS = True\n", "BENCHMARK_LIMIT = None\n", "BENCHMARK_TASKS = [\"arc_easy\", \"winogrande\", \"hellaswag\", \"lambada_openai\", \"piqa\"]\n", "HF_MODEL_NAME = \"gem-3-small\""]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "install_cell"},
      "outputs": [],
      "source": ["!pip install -q transformers accelerate datasets\n", "!pip install -q optipfair matplotlib seaborn tqdm\n", "!pip install -q lm_eval langdetect codecarbon huggingface_hub"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "imports_cell"},
      "outputs": [],
      "source": ["import torch, gc\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader, TensorDataset, random_split\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from datasets import load_dataset, Dataset\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import numpy as np\n", "from tqdm.auto import tqdm\n", "from copy import deepcopy\n", "import warnings, time, json, os\n", "from datetime import datetime\n", "warnings.filterwarnings('ignore')\n", "\n", "print(f\"PyTorch version: {torch.__version__}\")\n", "print(f\"CUDA available: {torch.cuda.is_available()}\")\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Using device: {device}\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "seed_cell"},
      "outputs": [],
      "source": ["def set_seed(seed=42):\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n", "    np.random.seed(seed)\n", "set_seed(42)\n", "print(\"✓ Random seed set to 42\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "utils_download"},
      "outputs": [],
      "source": ["!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n", "from utils import evaluate_metrics, clear_gpu_cache, model_evaluation\n", "print(\"✓ utils.py loaded\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section1_header"},
      "source": ["## Section 1: Load Teacher Model"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "load_teacher"},
      "outputs": [],
      "source": ["MODEL_NAME = \"google/gemma-3-270m\"\n", "print(f\"Loading Teacher model: {MODEL_NAME}\")\n", "teacher_model = AutoModelForCausalLM.from_pretrained(\n", "    MODEL_NAME,\n", "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n", "    device_map=\"auto\" if torch.cuda.is_available() else None\n", ")\n", "teacher_model.eval()\n", "for param in teacher_model.parameters():\n", "    param.requires_grad = False\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n", "if tokenizer.pad_token is None:\n", "    tokenizer.pad_token = tokenizer.eos_token\n", "\n", "n_teacher_layers = len(teacher_model.model.layers)\n", "hidden_dim = teacher_model.config.hidden_size\n", "print(f\"Teacher: {n_teacher_layers} layers, {teacher_model.num_parameters():,} params\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section2_header"},
      "source": ["## Section 2: Prepare Training Dataset"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "load_dataset"},
      "outputs": [],
      "source": ["MAX_LENGTH = 512\n", "print(\"Loading Cosmopedia dataset...\")\n", "dataset_name = \"HuggingFaceTB/cosmopedia\"\n", "subsets = [\"stories\", \"wikihow\", \"openstax\", \"web_samples_v1\"]\n", "samples_per_subset = int(RECOVERY_SAMPLES / 4)\n", "\n", "all_samples = []\n", "for subset in subsets:\n", "    print(f\"  Loading {subset}...\")\n", "    subset_data = load_dataset(dataset_name, subset, split=\"train\", streaming=True)\n", "    subset_samples = list(subset_data.take(samples_per_subset))\n", "    all_samples.extend(subset_samples)\n", "    print(f\"    ✓ {len(subset_samples):,} samples\")\n", "\n", "distillation_dataset = Dataset.from_dict({'text': [s['text'] for s in all_samples]})\n", "print(f\"✓ Total samples: {len(distillation_dataset):,}\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "tokenize_data"},
      "outputs": [],
      "source": ["print(\"Tokenizing...\")\n", "texts = [item['text'] for item in distillation_dataset]\n", "tokenized_data = []\n", "for i in tqdm(range(0, len(texts), 1000), desc=\"Tokenizing\"):\n", "    batch = tokenizer(texts[i:i+1000], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_tensors=\"pt\")\n", "    tokenized_data.append(batch)\n", "\n", "input_ids = torch.cat([b['input_ids'] for b in tokenized_data], dim=0)\n", "attention_mask = torch.cat([b['attention_mask'] for b in tokenized_data], dim=0)\n", "full_dataset = TensorDataset(input_ids, attention_mask)\n", "\n", "generator = torch.Generator().manual_seed(42)\n", "train_size = int(0.8 * len(full_dataset))\n", "val_size = len(full_dataset) - train_size\n", "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n", "\n", "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n", "eval_dataloader_raw = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n", "\n", "class DictDataLoader:\n", "    def __init__(self, dl): self.dataloader = dl\n", "    def __iter__(self):\n", "        for input_ids, attention_mask in self.dataloader:\n", "            yield {'input_ids': input_ids, 'attention_mask': attention_mask}\n", "    def __len__(self): return len(self.dataloader)\n", "\n", "eval_dataloader = DictDataLoader(eval_dataloader_raw)\n", "print(f\"✓ Train: {len(train_dataset):,}, Val: {len(val_dataset):,}\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section3_header"},
      "source": ["## Section 3: Create Pruned Student Model"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "create_pruned"},
      "outputs": [],
      "source": ["import optipfair as opf\n", "student_model = deepcopy(teacher_model)\n", "importance_scores = opf.analyze_layer_importance(student_model, train_dataloader, show_progress=True)\n", "LAYERS_TO_REMOVE = sorted(importance_scores.keys(), key=lambda x: importance_scores[x])[:4]\n", "print(f\"Layers to remove: {LAYERS_TO_REMOVE}\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "prune_student"},
      "outputs": [],
      "source": ["student_model = opf.prune_model_depth(model=student_model, layer_indices=LAYERS_TO_REMOVE, show_progress=True)\n", "for param in student_model.parameters():\n", "    param.requires_grad = True\n", "n_student_layers = len(student_model.model.layers)\n", "print(f\"✓ Student: {n_student_layers} layers, {student_model.num_parameters():,} params\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "eval_baselines"},
      "outputs": [],
      "source": ["print(\"Evaluating Teacher...\")\n", "teacher_metrics = evaluate_metrics(teacher_model, eval_dataloader, device=device)\n", "teacher_ppl = teacher_metrics['perplexity']\n", "teacher_loss = teacher_metrics['loss']\n", "\n", "print(\"Evaluating Pruned Student...\")\n", "student_pruned_copy = deepcopy(student_model)\n", "student_metrics = evaluate_metrics(student_pruned_copy, eval_dataloader, device=device)\n", "student_ppl = student_metrics['perplexity']\n", "student_loss = student_metrics['loss']\n", "del student_pruned_copy\n", "clear_gpu_cache()\n", "\n", "print(f\"Teacher PPL: {teacher_ppl:.2f}, Student PPL: {student_ppl:.2f}\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "benchmark_baselines"},
      "outputs": [],
      "source": ["benchmark_results = {}\n", "if RUN_FULL_BENCHMARKS:\n", "    print(\"Benchmarking Teacher...\")\n", "    benchmark_results['teacher'] = model_evaluation(model_obj=teacher_model, tokenizer=tokenizer, tasks=BENCHMARK_TASKS, device=device, limit=BENCHMARK_LIMIT, batch_size=BATCH_EVAL)\n", "    print(\"Benchmarking Pruned Student...\")\n", "    student_pruned_copy = deepcopy(student_model)\n", "    benchmark_results['student_pruned'] = model_evaluation(model_obj=student_pruned_copy, tokenizer=tokenizer, tasks=BENCHMARK_TASKS, device=device, limit=BENCHMARK_LIMIT, batch_size=BATCH_EVAL)\n", "    del student_pruned_copy\n", "    clear_gpu_cache()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section4_header"},
      "source": ["## Section 4: Loss Function (Logits Only)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "loss_function"},
      "outputs": [],
      "source": ["def compute_logits_only_loss(student_logits, teacher_logits, labels, alpha=0.5, beta=0.5, temperature=2.0):\n", "    shift_logits = student_logits[..., :-1, :].contiguous()\n", "    shift_labels = labels[..., 1:].contiguous()\n", "    loss_task = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), ignore_index=-100)\n", "    \n", "    student_soft = F.log_softmax(student_logits / temperature, dim=-1)\n", "    teacher_soft = F.softmax(teacher_logits / temperature, dim=-1)\n", "    student_soft = student_soft[..., :-1, :].contiguous()\n", "    teacher_soft = teacher_soft[..., :-1, :].contiguous()\n", "    loss_logits = F.kl_div(student_soft.view(-1, student_soft.size(-1)), teacher_soft.view(-1, teacher_soft.size(-1)), reduction='batchmean') * (temperature ** 2)\n", "    \n", "    total_loss = alpha * loss_task + beta * loss_logits\n", "    return total_loss, {'total': total_loss.item(), 'task': loss_task.item(), 'logits': loss_logits.item()}"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section5_header"},
      "source": ["## Section 5: Training Loop"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "train_function"},
      "outputs": [],
      "source": ["def train_student_logits_only(student_model, teacher_model, dataloader, alpha=0.5, beta=0.5, temperature=2.0, epochs=3, learning_rate=4e-5, experiment_name=\"experiment\", accumulation_steps=4):\n", "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate)\n", "    student_model.train()\n", "    teacher_model.eval()\n", "    loss_history = {'total': [], 'task': [], 'logits': []}\n", "    epoch_times = []\n", "    print(f\"\\n{'='*60}\\nStarting: {experiment_name}\\nEpochs: {epochs}, LR: {learning_rate}, α={alpha}, β={beta}, T={temperature}\\nHidden states: DISABLED\\n{'='*60}\")\n", "    total_start = time.time()\n", "    for epoch in range(epochs):\n", "        epoch_start = time.time()\n", "        epoch_losses = {k: [] for k in loss_history}\n", "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n", "        accum_losses, accum_count = {k: 0.0 for k in loss_history}, 0\n", "        for batch_idx, (input_ids, attention_mask) in enumerate(progress):\n", "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n", "            labels = input_ids.clone()\n", "            student_out = student_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=False)\n", "            with torch.no_grad():\n", "                teacher_out = teacher_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=False)\n", "            loss, loss_dict = compute_logits_only_loss(student_out.logits, teacher_out.logits, labels, alpha, beta, temperature)\n", "            (loss / accumulation_steps).backward()\n", "            for k in accum_losses: accum_losses[k] += loss_dict[k]\n", "            accum_count += 1\n", "            if (batch_idx + 1) % accumulation_steps == 0:\n", "                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n", "                optimizer.step()\n", "                optimizer.zero_grad()\n", "                avg = {k: v/accum_count for k, v in accum_losses.items()}\n", "                for k in avg: epoch_losses[k].append(avg[k])\n", "                progress.set_postfix({'loss': f\"{avg['total']:.4f}\"})\n", "                accum_losses, accum_count = {k: 0.0 for k in loss_history}, 0\n", "        if accum_count > 0:\n", "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n", "            optimizer.step()\n", "            optimizer.zero_grad()\n", "        for k in epoch_losses:\n", "            if epoch_losses[k]: loss_history[k].append(np.mean(epoch_losses[k]))\n", "        epoch_time = time.time() - epoch_start\n", "        epoch_times.append(epoch_time)\n", "        print(f\"Epoch {epoch+1}: Total={loss_history['total'][-1]:.4f}, Task={loss_history['task'][-1]:.4f}, Logits={loss_history['logits'][-1]:.4f} [{epoch_time:.1f}s]\")\n", "    total_time = time.time() - total_start\n", "    loss_history['epoch_times_seconds'] = epoch_times\n", "    loss_history['total_time_seconds'] = total_time\n", "    print(f\"\\n✓ Training completed in {total_time:.1f}s ({total_time/60:.1f} min)\")\n", "    return student_model, loss_history"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section6_header"},
      "source": ["## Section 6: Train with Logits Only"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "train_model"},
      "outputs": [],
      "source": ["student_logits_only = deepcopy(student_model)\n", "student_trained, history = train_student_logits_only(\n", "    student_model=student_logits_only, teacher_model=teacher_model, dataloader=train_dataloader,\n", "    alpha=0.5, beta=0.5, temperature=2.0, epochs=EPOCHS, learning_rate=LEARNING_RATE,\n", "    experiment_name=\"Logits-Only KLD Training\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "eval_trained"},
      "outputs": [],
      "source": ["print(\"Evaluating Trained Student...\")\n", "trained_metrics = evaluate_metrics(student_trained, eval_dataloader, device=device)\n", "trained_ppl = trained_metrics['perplexity']\n", "trained_loss = trained_metrics['loss']\n", "print(f\"Trained PPL: {trained_ppl:.2f}\")\n", "\n", "degradation = student_ppl - teacher_ppl\n", "recovered = student_ppl - trained_ppl\n", "recovery_pct = (recovered / degradation) * 100 if degradation > 0 else 0\n", "print(f\"Recovery: {recovery_pct:.1f}%\")\n", "\n", "if RUN_FULL_BENCHMARKS:\n", "    print(\"Benchmarking Trained Student...\")\n", "    benchmark_results['logits_only_trained'] = model_evaluation(model_obj=student_trained, tokenizer=tokenizer, tasks=BENCHMARK_TASKS, device=device, limit=BENCHMARK_LIMIT, batch_size=BATCH_EVAL)"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section7_header"},
      "source": ["## Section 7: Results Visualization"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "plot_ppl"},
      "outputs": [],
      "source": ["# Perplexity Bar Chart\n", "models = ['Teacher\\n(Baseline)', 'Pruned\\n(No KD)', 'Trained\\n(Logits KD)']\n", "ppls = [teacher_ppl, student_ppl, trained_ppl]\n", "colors = ['#2ecc71', '#e74c3c', '#3498db']\n", "\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "bars = ax.bar(models, ppls, color=colors, edgecolor='black', linewidth=1.5)\n", "ax.set_ylabel('Perplexity (↓ lower is better)', fontsize=12)\n", "ax.set_title('Perplexity Comparison: Teacher vs Pruned vs Trained', fontsize=14, fontweight='bold')\n", "for bar, ppl in zip(bars, ppls):\n", "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{ppl:.2f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n", "ax.axhline(y=teacher_ppl, color='#2ecc71', linestyle='--', alpha=0.7, label=f'Teacher baseline: {teacher_ppl:.2f}')\n", "ax.legend()\n", "plt.tight_layout()\n", "plt.savefig('ppl_comparison.png', dpi=150, bbox_inches='tight')\n", "plt.show()\n", "print(f\"\\n✓ Recovery: {recovery_pct:.1f}% of degradation recovered\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "plot_benchmarks"},
      "outputs": [],
      "source": ["# Benchmark Bar Chart\n", "if RUN_FULL_BENCHMARKS and benchmark_results:\n", "    tasks = BENCHMARK_TASKS\n", "    x = np.arange(len(tasks))\n", "    width = 0.25\n", "    \n", "    teacher_scores = [benchmark_results['teacher'].get(t, {}).get('acc', 0) * 100 for t in tasks]\n", "    pruned_scores = [benchmark_results['student_pruned'].get(t, {}).get('acc', 0) * 100 for t in tasks]\n", "    trained_scores = [benchmark_results['logits_only_trained'].get(t, {}).get('acc', 0) * 100 for t in tasks]\n", "    \n", "    fig, ax = plt.subplots(figsize=(14, 7))\n", "    bars1 = ax.bar(x - width, teacher_scores, width, label='Teacher (Baseline)', color='#2ecc71', edgecolor='black')\n", "    bars2 = ax.bar(x, pruned_scores, width, label='Pruned (No KD)', color='#e74c3c', edgecolor='black')\n", "    bars3 = ax.bar(x + width, trained_scores, width, label='Trained (Logits KD)', color='#3498db', edgecolor='black')\n", "    \n", "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n", "    ax.set_title('Benchmark Comparison: Teacher vs Pruned vs Trained', fontsize=14, fontweight='bold')\n", "    ax.set_xticks(x)\n", "    ax.set_xticklabels([t.replace('_', '\\n') for t in tasks], fontsize=10)\n", "    ax.legend(loc='upper right')\n", "    ax.set_ylim(0, 100)\n", "    ax.grid(axis='y', alpha=0.3)\n", "    \n", "    def add_labels(bars):\n", "        for bar in bars:\n", "            h = bar.get_height()\n", "            ax.text(bar.get_x() + bar.get_width()/2, h + 1, f'{h:.1f}', ha='center', va='bottom', fontsize=8)\n", "    add_labels(bars1)\n", "    add_labels(bars2)\n", "    add_labels(bars3)\n", "    \n", "    plt.tight_layout()\n", "    plt.savefig('benchmark_comparison.png', dpi=150, bbox_inches='tight')\n", "    plt.show()\n", "    \n", "    # Print average recovery\n", "    avg_teacher = np.mean(teacher_scores)\n", "    avg_pruned = np.mean(pruned_scores)\n", "    avg_trained = np.mean(trained_scores)\n", "    bench_degradation = avg_teacher - avg_pruned\n", "    bench_recovered = avg_trained - avg_pruned\n", "    bench_recovery_pct = (bench_recovered / bench_degradation) * 100 if bench_degradation > 0 else 0\n", "    print(f\"\\nBenchmark Average: Teacher={avg_teacher:.1f}%, Pruned={avg_pruned:.1f}%, Trained={avg_trained:.1f}%\")\n", "    print(f\"Benchmark Recovery: {bench_recovery_pct:.1f}%\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "plot_training"},
      "outputs": [],
      "source": ["# Training Loss Curves\n", "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n", "for idx, (key, title) in enumerate([('total', 'Total Loss'), ('task', 'Task Loss (CE)'), ('logits', 'Logits Loss (KLD)')]):\n", "    axes[idx].plot(history[key], marker='o', linewidth=2, markersize=8)\n", "    axes[idx].set_title(title, fontsize=12)\n", "    axes[idx].set_xlabel('Epoch')\n", "    axes[idx].set_ylabel('Loss')\n", "    axes[idx].grid(True, alpha=0.3)\n", "plt.suptitle('Training Progress: Logits-Only Knowledge Distillation', fontsize=14, fontweight='bold')\n", "plt.tight_layout()\n", "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n", "plt.show()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section75_header"},
      "source": ["## Section 7.5: Save Experiment Results"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "save_results"},
      "outputs": [],
      "source": ["class NumpyEncoder(json.JSONEncoder):\n", "    def default(self, obj):\n", "        if isinstance(obj, np.integer): return int(obj)\n", "        if isinstance(obj, np.floating): return float(obj)\n", "        if isinstance(obj, np.ndarray): return obj.tolist()\n", "        return super().default(obj)\n", "\n", "results_data = {\n", "    \"metadata\": {\"experiment_name\": \"Logits-Only KD\", \"timestamp\": datetime.now().isoformat(), \"torch_version\": torch.__version__},\n", "    \"models\": {\"teacher\": MODEL_NAME, \"student\": f\"Gemma-270m-Pruned ({n_student_layers} layers)\"},\n", "    \"training_config\": {\"alpha\": 0.5, \"beta\": 0.5, \"temperature\": 2.0, \"epochs\": EPOCHS, \"learning_rate\": LEARNING_RATE, \"batch_size\": BATCH_SIZE, \"samples\": RECOVERY_SAMPLES},\n", "    \"results\": {\n", "        \"teacher\": {\"perplexity\": teacher_ppl, \"loss\": teacher_loss},\n", "        \"student_pruned\": {\"perplexity\": student_ppl, \"loss\": student_loss},\n", "        \"logits_only_trained\": {\"perplexity\": trained_ppl, \"loss\": trained_loss, \"recovery_pct\": recovery_pct, \"training_time\": history.get('total_time_seconds', 0)}\n", "    }\n", "}\n", "if RUN_FULL_BENCHMARKS:\n", "    for k, v in benchmark_results.items():\n", "        if k in results_data['results']: results_data['results'][k]['benchmarks'] = v\n", "\n", "json_path = \"/content/drive/MyDrive/ch06nb01/CH06_NB02_Logits_KLD_results.json\"\n", "os.makedirs(os.path.dirname(json_path), exist_ok=True)\n", "with open(json_path, 'w') as f: json.dump(results_data, f, indent=2, cls=NumpyEncoder)\n", "print(f\"✓ Results saved to: {json_path}\")\n", "with open(\"CH06_NB02_Logits_KLD_results.json\", 'w') as f: json.dump(results_data, f, indent=2, cls=NumpyEncoder)\n", "print(\"✓ Local backup saved\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "section8_header"},
      "source": ["## Section 8: Save Model to Hugging Face"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "hf_login"},
      "outputs": [],
      "source": ["from huggingface_hub import login, HfApi\n", "login()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "push_model"},
      "outputs": [],
      "source": ["print(f\"Saving model to HuggingFace as '{HF_MODEL_NAME}'...\")\n", "student_trained.push_to_hub(HF_MODEL_NAME, commit_message=\"Depth-pruned Gemma-3-270m with Logits-Only KD\")\n", "tokenizer.push_to_hub(HF_MODEL_NAME, commit_message=\"Tokenizer for gem-3-small\")\n", "print(f\"✓ Model saved: https://huggingface.co/{HF_MODEL_NAME}\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "cleanup"},
      "outputs": [],
      "source": ["del student_logits_only\n", "clear_gpu_cache()\n", "print(f\"\\n{'='*60}\\nTRAINING COMPLETE\\n{'='*60}\")\n", "print(f\"Model: https://huggingface.co/{HF_MODEL_NAME}\")\n", "print(f\"Results: {json_path}\")"]
    }
  ],
  "metadata": {"accelerator": "GPU", "colab": {"gpuType": "A100", "machine_shape": "hm", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}},
  "nbformat": 4,
  "nbformat_minor": 0
}
