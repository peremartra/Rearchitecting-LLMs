{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/main/CH06/CH06_NB02_Logits_KLD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "title_header"
            },
            "source": [
                "# **Tailoring LLM Architectures**\n",
                "## **Chapter 6: Knowledge Distillation with Logits Only**\n",
                "### Training a pruned model using only Logits and KL Divergence\n",
                "\n",
                "by [Pere Martra](https://github.com/peremartra)\n",
                "\n",
                "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
                "\n",
                "---\n",
                "\n",
                "**Hardware Environment:** NVIDIA A100 GPU\n",
                "- **Model:** google/gemma-3-270m (Teacher: 18 transformer blocks) â†’ Pruned Student (14 transformer blocks)\n",
                "- **Dataset:** Cosmopedia (40,000 samples, 3 epochs)\n",
                "\n",
                "---\n",
                "\n",
                "**What we'll accomplish:**\n",
                "- Train a depth-pruned student model using **only logits and KL Divergence**\n",
                "- No hidden state alignment (simpler, faster training)\n",
                "- Use the **Selected-Layer mapping strategy** (map to original indices before pruning)\n",
                "- Save the trained model to Hugging Face as **gem-3-small**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section0_header"
            },
            "source": [
                "## Section 0: Environment & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "lcv_ggOHJDLh"
            },
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "print(\"âœ“ Drive mounted\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "CGEiuvn09B4m"
            },
            "outputs": [],
            "source": [
                "#Samples in the training dataset.\n",
                "RECOVERY_SAMPLES = 40000\n",
                "\n",
                "EPOCHS=3\n",
                "LEARNING_RATE=4e-5\n",
                "BATCH_SIZE = 16\n",
                "\n",
                "# Define benchmark tasks\n",
                "BATCH_EVAL=\"auto\"\n",
                "RUN_FULL_BENCHMARKS = True\n",
                "BENCHMARK_LIMIT = None\n",
                "BENCHMARK_TASKS = [\n",
                "      \"arc_easy\",\n",
                "      \"winogrande\",\n",
                "      \"hellaswag\",\n",
                "      \"lambada_openai\",\n",
                "      \"piqa\"\n",
                "]\n",
                "\n",
                "# Hugging Face Model Name\n",
                "HF_MODEL_NAME = \"gem-3-small\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_cell"
            },
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q transformers accelerate datasets\n",
                "!pip install -q optipfair  # For creating pruned model on-the-fly\n",
                "!pip install -q matplotlib seaborn tqdm\n",
                "!pip install -q lm_eval\n",
                "!pip install -q langdetect\n",
                "!pip install -q codecarbon\n",
                "!pip install -q huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports_cell"
            },
            "outputs": [],
            "source": [
                "import torch, gc\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    TrainingArguments,\n",
                "    Trainer\n",
                ")\n",
                "from datasets import load_dataset\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "from copy import deepcopy\n",
                "import warnings\n",
                "import time\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Check PyTorch version and device\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "seed_cell"
            },
            "outputs": [],
            "source": [
                "def set_seed(seed=42):\n",
                "    \"\"\"Set random seed for reproducibility\"\"\"\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    np.random.seed(seed)\n",
                "\n",
                "set_seed(42)\n",
                "print(\"âœ“ Random seed set to 42\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "utils_download"
            },
            "outputs": [],
            "source": [
                "# Download utils.py from GitHub repository\n",
                "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
                "\n",
                "# Verify download\n",
                "import os\n",
                "if os.path.exists('utils.py'):\n",
                "    print(\"âœ… utils.py downloaded successfully\")\n",
                "else:\n",
                "    print(\"âŒ Failed to download utils.py\")\n",
                "\n",
                "from utils import (\n",
                "    evaluate_metrics,  # Loss & Perplexity\n",
                "    clear_gpu_cache\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section1_header"
            },
            "source": [
                "## Section 1: Load Models and Create Pruned Student"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_teacher"
            },
            "outputs": [],
            "source": [
                "MODEL_NAME = \"google/gemma-3-270m\"\n",
                "\n",
                "print(f\"Loading Teacher model: {MODEL_NAME}\")\n",
                "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
                "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
                ")\n",
                "\n",
                "# Freeze teacher (we never update it)\n",
                "teacher_model.eval()\n",
                "for param in teacher_model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# Print architecture info\n",
                "n_teacher_layers = len(teacher_model.model.layers)\n",
                "hidden_dim = teacher_model.config.hidden_size\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"Teacher Model: {MODEL_NAME}\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Total layers: {n_teacher_layers}\")\n",
                "print(f\"Hidden dimension: {hidden_dim}\")\n",
                "print(f\"Total parameters: {teacher_model.num_parameters():,}\")\n",
                "print(f\"Memory footprint: {teacher_model.get_memory_footprint() / 1e9:.2f} GB\")\n",
                "print(f\"{'='*60}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section2_header"
            },
            "source": [
                "## Section 2: Prepare Training Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cosmopedia_alt"
            },
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# Section 2: Prepare Training Dataset (COMPLETE VERSION)\n",
                "# ==============================================================================\n",
                "\n",
                "from datasets import load_dataset, Dataset\n",
                "from tqdm.auto import tqdm\n",
                "import torch\n",
                "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
                "\n",
                "# Define tokenization parameters\n",
                "MAX_LENGTH = 512\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    \"\"\"Tokenize text samples for training\"\"\"\n",
                "    texts = examples['text'] if isinstance(examples, dict) else examples\n",
                "    return tokenizer(\n",
                "        texts,\n",
                "        truncation=True,\n",
                "        padding=\"max_length\",\n",
                "        max_length=MAX_LENGTH,\n",
                "        return_tensors=\"pt\"\n",
                "    )\n",
                "\n",
                "print(f\"âœ“ Tokenization function ready (max_length={MAX_LENGTH})\")\n",
                "\n",
                "# Load Cosmopedia dataset\n",
                "print(\"\\nLoading Cosmopedia dataset...\")\n",
                "dataset_name = \"HuggingFaceTB/cosmopedia\"\n",
                "subsets = [\"stories\", \"wikihow\", \"openstax\", \"web_samples_v1\"]\n",
                "samples_per_subset = int(RECOVERY_SAMPLES / 4)\n",
                "num_samples = samples_per_subset * len(subsets)\n",
                "\n",
                "print(f\"Loading {len(subsets)} subsets with {samples_per_subset:,} samples each...\")\n",
                "\n",
                "all_samples = []\n",
                "for subset in subsets:\n",
                "    print(f\"  Loading {subset}...\")\n",
                "    subset_data = load_dataset(dataset_name, subset, split=\"train\", streaming=True)\n",
                "    subset_samples = list(subset_data.take(samples_per_subset))\n",
                "    all_samples.extend(subset_samples)\n",
                "    print(f\"    âœ“ {len(subset_samples):,} samples from {subset}\")\n",
                "\n",
                "print(f\"âœ“ Total samples loaded: {len(all_samples):,}\")\n",
                "\n",
                "# Convert to HuggingFace Dataset\n",
                "distillation_dataset = Dataset.from_dict({'text': [s['text'] for s in all_samples]})\n",
                "print(f\"âœ“ Cosmopedia dataset ready: {len(distillation_dataset):,} samples\")\n",
                "\n",
                "# ==============================================================================\n",
                "# Tokenization and Train/Val Split\n",
                "# ==============================================================================\n",
                "\n",
                "print(\"\\nPreparing DataLoader with Train/Val split...\")\n",
                "\n",
                "# 1. Tokenize all samples\n",
                "print(\"  Converting dataset to list...\")\n",
                "dataset_list = list(distillation_dataset)\n",
                "texts = [item['text'] for item in dataset_list]\n",
                "\n",
                "print(f\"  Tokenizing {len(texts):,} samples...\")\n",
                "tokenized_data = []\n",
                "batch_size = 1000\n",
                "for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
                "    batch_texts = texts[i:i+batch_size]\n",
                "    batch_tokens = tokenizer(\n",
                "        batch_texts,\n",
                "        truncation=True,\n",
                "        padding=\"max_length\",\n",
                "        max_length=MAX_LENGTH,\n",
                "        return_tensors=\"pt\"\n",
                "    )\n",
                "    tokenized_data.append(batch_tokens)\n",
                "\n",
                "# Combine all batches\n",
                "input_ids = torch.cat([batch['input_ids'] for batch in tokenized_data], dim=0)\n",
                "attention_mask = torch.cat([batch['attention_mask'] for batch in tokenized_data], dim=0)\n",
                "\n",
                "# 2. Create full dataset\n",
                "full_dataset = TensorDataset(input_ids, attention_mask)\n",
                "\n",
                "# 3. Split into Train (80%) and Validation (20%)\n",
                "generator = torch.Generator().manual_seed(42)\n",
                "train_size = int(0.8 * len(full_dataset))\n",
                "val_size = len(full_dataset) - train_size\n",
                "\n",
                "train_dataset, val_dataset = random_split(\n",
                "    full_dataset,\n",
                "    [train_size, val_size],\n",
                "    generator=generator\n",
                ")\n",
                "\n",
                "# 4. Create DataLoaders\n",
                "train_dataloader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "eval_dataloader_raw = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "print(f\"\\nâœ“ DataLoaders ready:\")\n",
                "print(f\"  Total samples: {len(full_dataset):,}\")\n",
                "print(f\"  - Training samples: {len(train_dataset):,} (80%) ({len(train_dataloader):,} batches)\")\n",
                "print(f\"  - Validation samples: {len(val_dataset):,} (20%) ({len(eval_dataloader_raw):,} batches)\")\n",
                "print(f\"  Batch size: {BATCH_SIZE}\")\n",
                "\n",
                "# ==============================================================================\n",
                "# Evaluation DataLoader Wrapper\n",
                "# ==============================================================================\n",
                "\n",
                "class DictDataLoader:\n",
                "    \"\"\"Wrapper to convert TensorDataset tuples to dictionaries\"\"\"\n",
                "    def __init__(self, dataloader):\n",
                "        self.dataloader = dataloader\n",
                "\n",
                "    def __iter__(self):\n",
                "        for input_ids, attention_mask in self.dataloader:\n",
                "            yield {\n",
                "                'input_ids': input_ids,\n",
                "                'attention_mask': attention_mask\n",
                "            }\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.dataloader)\n",
                "\n",
                "eval_dataloader = DictDataLoader(eval_dataloader_raw)\n",
                "print(\"âœ“ Evaluation dataloader ready (using validation split)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "pruned_model_header"
            },
            "source": [
                "## Section 3: Create Pruned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_pruned"
            },
            "outputs": [],
            "source": [
                "import optipfair as opf\n",
                "\n",
                "print(\"Creating Student model with depth pruning...\")\n",
                "\n",
                "# IMPORTANT: Use deepcopy to avoid modifying the original model\n",
                "student_model = deepcopy(teacher_model)\n",
                "importance_scores = opf.analyze_layer_importance(student_model, train_dataloader, show_progress=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "identify_layers"
            },
            "outputs": [],
            "source": [
                "LAYERS_TO_REMOVE = sorted(importance_scores.keys(), key=lambda x: importance_scores[x])[:4]\n",
                "print(f\"Layers to remove (least important): {LAYERS_TO_REMOVE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_student"
            },
            "outputs": [],
            "source": [
                "student_model = opf.prune_model_depth(\n",
                "    model=student_model,\n",
                "    layer_indices=LAYERS_TO_REMOVE,\n",
                "    show_progress=True,\n",
                ")\n",
                "\n",
                "# IMPORTANT: Unfreeze student parameters!\n",
                "for param in student_model.parameters():\n",
                "    param.requires_grad = True\n",
                "\n",
                "# Get student info\n",
                "n_student_layers = len(student_model.model.layers)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"Student Model (Depth Pruned)\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Total layers: {n_student_layers} (removed {LAYERS_TO_REMOVE})\")\n",
                "print(f\"Hidden dimension: {hidden_dim} (unchanged)\")\n",
                "print(f\"Total parameters: {student_model.num_parameters():,}\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "\n",
                "print(f\"âœ“ Student has {n_student_layers} layers vs Teacher's {n_teacher_layers} layers\")\n",
                "print(f\"âœ“ Student parameters unfrozen and ready for training\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "eval_teacher"
            },
            "outputs": [],
            "source": [
                "# Evaluate Teacher (baseline)\n",
                "print(\"Evaluating Teacher...\")\n",
                "teacher_metrics = evaluate_metrics(teacher_model, eval_dataloader, device=device)\n",
                "teacher_loss = teacher_metrics['loss']\n",
                "teacher_ppl = teacher_metrics['perplexity']\n",
                "\n",
                "if RUN_FULL_BENCHMARKS:\n",
                "  from utils import model_evaluation\n",
                "  benchmark_results = {}\n",
                "  results = model_evaluation(\n",
                "      model_obj=teacher_model,\n",
                "      tokenizer=tokenizer,\n",
                "      tasks=BENCHMARK_TASKS,\n",
                "      device=device,\n",
                "      limit=BENCHMARK_LIMIT,\n",
                "      batch_size=BATCH_EVAL\n",
                "      )\n",
                "  benchmark_results['teacher'] = results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "eval_pruned"
            },
            "outputs": [],
            "source": [
                "# Evaluate Pruned Student (no training)\n",
                "print(\"Evaluating Pruned Student (no training)...\")\n",
                "student_pruned = deepcopy(student_model)\n",
                "student_metrics = evaluate_metrics(student_pruned, eval_dataloader, device=device)\n",
                "student_loss = student_metrics['loss']\n",
                "student_ppl = student_metrics['perplexity']\n",
                "\n",
                "if RUN_FULL_BENCHMARKS:\n",
                "    results = model_evaluation(\n",
                "    model_obj=student_pruned,\n",
                "    tokenizer=tokenizer,\n",
                "    tasks=BENCHMARK_TASKS,\n",
                "    device=device,\n",
                "    limit=BENCHMARK_LIMIT,\n",
                "    batch_size=BATCH_EVAL\n",
                "    )\n",
                "\n",
                "    benchmark_results['student_pruned'] = results\n",
                "\n",
                "del student_pruned\n",
                "clear_gpu_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section4_header"
            },
            "source": [
                "## Section 4: Implement Compound Loss (Logits Only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "compound_loss"
            },
            "outputs": [],
            "source": [
                "def compute_logits_only_loss(\n",
                "    student_logits,\n",
                "    teacher_logits,\n",
                "    labels,\n",
                "    alpha=0.5,       # weight for task loss (Cross-Entropy)\n",
                "    beta=0.5,        # weight for logits loss (KL Divergence)\n",
                "    temperature=2.0\n",
                "):\n",
                "    \"\"\"\n",
                "    Compound loss using ONLY logits (no hidden states).\n",
                "    \n",
                "    Components:\n",
                "    1. Task Loss: Standard cross-entropy with hard labels\n",
                "    2. Logits Loss: KL Divergence between teacher and student soft labels\n",
                "    \"\"\"\n",
                "    # 1. TASK LOSS (Cross-Entropy)\n",
                "    shift_logits = student_logits[..., :-1, :].contiguous()\n",
                "    shift_labels = labels[..., 1:].contiguous()\n",
                "    loss_task = F.cross_entropy(\n",
                "        shift_logits.view(-1, shift_logits.size(-1)),\n",
                "        shift_labels.view(-1),\n",
                "        ignore_index=-100\n",
                "    )\n",
                "\n",
                "    # 2. LOGITS LOSS (KL Divergence)\n",
                "    student_soft = F.log_softmax(student_logits / temperature, dim=-1)\n",
                "    teacher_soft = F.softmax(teacher_logits / temperature, dim=-1)\n",
                "\n",
                "    student_soft = student_soft[..., :-1, :].contiguous()\n",
                "    teacher_soft = teacher_soft[..., :-1, :].contiguous()\n",
                "\n",
                "    loss_logits = F.kl_div(\n",
                "        student_soft.view(-1, student_soft.size(-1)),\n",
                "        teacher_soft.view(-1, teacher_soft.size(-1)),\n",
                "        reduction='batchmean'\n",
                "    ) * (temperature ** 2)\n",
                "\n",
                "    # COMBINE LOSSES\n",
                "    total_loss = alpha * loss_task + beta * loss_logits\n",
                "\n",
                "    loss_dict = {\n",
                "        'total': total_loss.item(),\n",
                "        'task': loss_task.item(),\n",
                "        'logits': loss_logits.item()\n",
                "    }\n",
                "\n",
                "    return total_loss, loss_dict"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section5_header"
            },
            "source": [
                "## Section 5: Training Loop (Logits Only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_function"
            },
            "outputs": [],
            "source": [
                "def train_student_logits_only(\n",
                "    student_model,\n",
                "    teacher_model,\n",
                "    dataloader,\n",
                "    alpha=0.5,\n",
                "    beta=0.5,\n",
                "    temperature=2.0,\n",
                "    epochs=3,\n",
                "    learning_rate=4e-5,\n",
                "    experiment_name=\"experiment\",\n",
                "    accumulation_steps=4\n",
                "):\n",
                "    \"\"\"\n",
                "    Train student model using ONLY logits (no hidden states).\n",
                "    This is faster and simpler than full knowledge distillation.\n",
                "    \"\"\"\n",
                "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate)\n",
                "\n",
                "    student_model.train()\n",
                "    teacher_model.eval()\n",
                "\n",
                "    loss_history = {'total': [], 'task': [], 'logits': []}\n",
                "    epoch_times = []\n",
                "\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Starting Training: {experiment_name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(f\"Epochs: {epochs}\")\n",
                "    print(f\"Learning rate: {learning_rate}\")\n",
                "    print(f\"Loss weights: Î±={alpha} (task), Î²={beta} (logits)\")\n",
                "    print(f\"Temperature: {temperature}\")\n",
                "    print(f\"Hidden states computation: DISABLED (Logits Only)\")\n",
                "    print(f\"Gradient Accumulation Steps: {accumulation_steps}\")\n",
                "    print(f\"Effective Batch Size: {dataloader.batch_size * accumulation_steps}\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "\n",
                "    total_start_time = time.time()\n",
                "\n",
                "    for epoch in range(epochs):\n",
                "        epoch_start_time = time.time()\n",
                "\n",
                "        epoch_losses = {k: [] for k in loss_history.keys()}\n",
                "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
                "\n",
                "        accumulated_losses = {k: 0.0 for k in loss_history.keys()}\n",
                "        accumulation_counter = 0\n",
                "\n",
                "        for batch_idx, (input_ids, attention_mask) in enumerate(progress_bar):\n",
                "            input_ids = input_ids.to(device)\n",
                "            attention_mask = attention_mask.to(device)\n",
                "            labels = input_ids.clone()\n",
                "\n",
                "            # Student forward pass (NO hidden states)\n",
                "            student_outputs = student_model(\n",
                "                input_ids=input_ids,\n",
                "                attention_mask=attention_mask,\n",
                "                output_hidden_states=False  # KEY: No hidden states\n",
                "            )\n",
                "\n",
                "            # Teacher forward pass (no gradients, NO hidden states)\n",
                "            with torch.no_grad():\n",
                "                teacher_outputs = teacher_model(\n",
                "                    input_ids=input_ids,\n",
                "                    attention_mask=attention_mask,\n",
                "                    output_hidden_states=False  # KEY: No hidden states\n",
                "                )\n",
                "\n",
                "            # Compute logits-only loss\n",
                "            loss, loss_dict = compute_logits_only_loss(\n",
                "                student_logits=student_outputs.logits,\n",
                "                teacher_logits=teacher_outputs.logits,\n",
                "                labels=labels,\n",
                "                alpha=alpha,\n",
                "                beta=beta,\n",
                "                temperature=temperature\n",
                "            )\n",
                "\n",
                "            scaled_loss = loss / accumulation_steps\n",
                "            scaled_loss.backward()\n",
                "\n",
                "            for key in accumulated_losses:\n",
                "                accumulated_losses[key] += loss_dict[key]\n",
                "            accumulation_counter += 1\n",
                "\n",
                "            if (batch_idx + 1) % accumulation_steps == 0:\n",
                "                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
                "                optimizer.step()\n",
                "                optimizer.zero_grad()\n",
                "\n",
                "                avg_losses = {k: v / accumulation_counter for k, v in accumulated_losses.items()}\n",
                "                for key in avg_losses:\n",
                "                    epoch_losses[key].append(avg_losses[key])\n",
                "\n",
                "                progress_bar.set_postfix({\n",
                "                    'loss': f\"{avg_losses['total']:.4f}\",\n",
                "                    'task': f\"{avg_losses['task']:.4f}\",\n",
                "                    'logits': f\"{avg_losses['logits']:.4f}\"\n",
                "                })\n",
                "\n",
                "                accumulated_losses = {k: 0.0 for k in loss_history.keys()}\n",
                "                accumulation_counter = 0\n",
                "\n",
                "        if accumulation_counter > 0:\n",
                "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
                "            optimizer.step()\n",
                "            optimizer.zero_grad()\n",
                "\n",
                "            avg_losses = {k: v / accumulation_counter for k, v in accumulated_losses.items()}\n",
                "            for key in avg_losses:\n",
                "                epoch_losses[key].append(avg_losses[key])\n",
                "\n",
                "        for key in epoch_losses:\n",
                "            if epoch_losses[key]:\n",
                "                avg_loss = np.mean(epoch_losses[key])\n",
                "                loss_history[key].append(avg_loss)\n",
                "\n",
                "        epoch_time = time.time() - epoch_start_time\n",
                "        epoch_times.append(epoch_time)\n",
                "\n",
                "        print(f\"Epoch {epoch+1} avg losses - \"\n",
                "              f\"Total: {loss_history['total'][-1]:.4f}, \"\n",
                "              f\"Task: {loss_history['task'][-1]:.4f}, \"\n",
                "              f\"Logits: {loss_history['logits'][-1]:.4f} \"\n",
                "              f\"[{epoch_time:.1f}s]\")\n",
                "\n",
                "    total_time = time.time() - total_start_time\n",
                "\n",
                "    loss_history['epoch_times_seconds'] = epoch_times\n",
                "    loss_history['total_time_seconds'] = total_time\n",
                "\n",
                "    print(f\"\\nâœ“ Training completed: {experiment_name}\")\n",
                "    print(f\"  Total time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
                "    print(f\"  Avg time per epoch: {np.mean(epoch_times):.1f}s\")\n",
                "\n",
                "    return student_model, loss_history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section6_header"
            },
            "source": [
                "## Section 6: Train with Logits Only"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_logits_only"
            },
            "outputs": [],
            "source": [
                "# Create fresh student model\n",
                "student_logits_only = deepcopy(student_model)\n",
                "\n",
                "# Train with logits-only\n",
                "student_trained, history = train_student_logits_only(\n",
                "    student_model=student_logits_only,\n",
                "    teacher_model=teacher_model,\n",
                "    dataloader=train_dataloader,\n",
                "    alpha=0.5,\n",
                "    beta=0.5,\n",
                "    temperature=2.0,\n",
                "    epochs=EPOCHS,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    experiment_name=\"Logits-Only KLD Training\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "eval_trained"
            },
            "outputs": [],
            "source": [
                "print(\"Evaluating Trained Student (Logits Only)...\")\n",
                "trained_metrics = evaluate_metrics(student_trained, eval_dataloader, device=device)\n",
                "trained_loss = trained_metrics['loss']\n",
                "trained_ppl = trained_metrics['perplexity']\n",
                "\n",
                "if RUN_FULL_BENCHMARKS:\n",
                "    results = model_evaluation(\n",
                "        model_obj=student_trained,\n",
                "        tokenizer=tokenizer,\n",
                "        tasks=BENCHMARK_TASKS,\n",
                "        device=device,\n",
                "        limit=BENCHMARK_LIMIT,\n",
                "        batch_size=BATCH_EVAL\n",
                "    )\n",
                "\n",
                "    benchmark_results['logits_only_trained'] = results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section7_header"
            },
            "source": [
                "## Section 7: Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "results_summary"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RESULTS SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nPerplexity Comparison:\")\n",
                "print(f\"  Teacher (baseline):     {teacher_ppl:.2f}\")\n",
                "print(f\"  Student (pruned, no KD): {student_ppl:.2f}\")\n",
                "print(f\"  Student (Logits-Only KD): {trained_ppl:.2f}\")\n",
                "\n",
                "# Calculate recovery\n",
                "degradation = student_ppl - teacher_ppl\n",
                "recovered = student_ppl - trained_ppl\n",
                "recovery_pct = (recovered / degradation) * 100 if degradation > 0 else 0\n",
                "\n",
                "print(f\"\\nRecovery Analysis:\")\n",
                "print(f\"  Degradation from pruning: +{degradation:.2f} PPL\")\n",
                "print(f\"  Recovered via Logits KD:  -{recovered:.2f} PPL\")\n",
                "print(f\"  Recovery percentage:       {recovery_pct:.1f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "plot_training"
            },
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "for idx, (key, title) in enumerate([('total', 'Total Loss'), ('task', 'Task Loss (CE)'), ('logits', 'Logits Loss (KLD)')]):\n",
                "    axes[idx].plot(history[key], marker='o')\n",
                "    axes[idx].set_title(title)\n",
                "    axes[idx].set_xlabel('Epoch')\n",
                "    axes[idx].set_ylabel('Loss')\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('Training Progress: Logits-Only Knowledge Distillation', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section8_header"
            },
            "source": [
                "## Section 8: Save Model to Hugging Face"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hf_login"
            },
            "outputs": [],
            "source": [
                "from huggingface_hub import login, HfApi\n",
                "\n",
                "# Login to Hugging Face\n",
                "# You'll need to enter your HF token when prompted\n",
                "login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_model"
            },
            "outputs": [],
            "source": [
                "# Save the trained model to Hugging Face\n",
                "print(f\"\\nSaving model to Hugging Face as '{HF_MODEL_NAME}'...\")\n",
                "\n",
                "# Push model to hub\n",
                "student_trained.push_to_hub(\n",
                "    HF_MODEL_NAME,\n",
                "    commit_message=\"Depth-pruned Gemma-3-270m trained with Logits-Only KD\"\n",
                ")\n",
                "\n",
                "# Push tokenizer to hub\n",
                "tokenizer.push_to_hub(\n",
                "    HF_MODEL_NAME,\n",
                "    commit_message=\"Tokenizer for gem-3-small\"\n",
                ")\n",
                "\n",
                "print(f\"\\nâœ“ Model saved successfully!\")\n",
                "print(f\"  Model URL: https://huggingface.co/{HF_MODEL_NAME}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_model_card"
            },
            "outputs": [],
            "source": [
                "# Create model card\n",
                "model_card_content = f\"\"\"\n",
                "---\n",
                "license: apache-2.0\n",
                "language:\n",
                "- en\n",
                "base_model: google/gemma-3-270m\n",
                "tags:\n",
                "- knowledge-distillation\n",
                "- depth-pruning\n",
                "- gemma\n",
                "---\n",
                "\n",
                "# {HF_MODEL_NAME}\n",
                "\n",
                "## Model Description\n",
                "\n",
                "This is a depth-pruned version of google/gemma-3-270m, trained using Knowledge Distillation with Logits-Only KL Divergence.\n",
                "\n",
                "### Architecture\n",
                "- **Base Model**: google/gemma-3-270m (18 transformer blocks)\n",
                "- **Pruned Model**: 14 transformer blocks (4 least important blocks removed)\n",
                "- **Parameters**: ~{student_trained.num_parameters():,}\n",
                "\n",
                "### Training Details\n",
                "- **Dataset**: Cosmopedia (40,000 samples)\n",
                "- **Method**: Knowledge Distillation with KL Divergence on logits\n",
                "- **Epochs**: {EPOCHS}\n",
                "- **Learning Rate**: {LEARNING_RATE}\n",
                "- **Loss Weights**: Î±={0.5} (task), Î²={0.5} (logits)\n",
                "- **Temperature**: 2.0\n",
                "\n",
                "### Performance\n",
                "- **Teacher Perplexity**: {teacher_ppl:.2f}\n",
                "- **Pruned (no KD) Perplexity**: {student_ppl:.2f}\n",
                "- **Trained Perplexity**: {trained_ppl:.2f}\n",
                "- **Recovery**: {recovery_pct:.1f}%\n",
                "\n",
                "## Usage\n",
                "\n",
                "```python\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\"{HF_MODEL_NAME}\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"{HF_MODEL_NAME}\")\n",
                "\n",
                "# Generate text\n",
                "input_text = \"Once upon a time\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
                "outputs = model.generate(**inputs, max_length=100)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "```\n",
                "\n",
                "## Citation\n",
                "\n",
                "If you use this model, please cite:\n",
                "\n",
                "```bibtex\n",
                "@misc{{\n",
                "  author = {{Pere Martra}},\n",
                "  title = {{gem-3-small: Depth-pruned Gemma with Knowledge Distillation}},\n",
                "  year = {{2026}},\n",
                "  publisher = {{Hugging Face}},\n",
                "  url = {{https://huggingface.co/{HF_MODEL_NAME}}}\n",
                "}}\n",
                "```\n",
                "\"\"\"\n",
                "\n",
                "# Save model card\n",
                "api = HfApi()\n",
                "api.upload_file(\n",
                "    path_or_fileobj=model_card_content.encode(),\n",
                "    path_in_repo=\"README.md\",\n",
                "    repo_id=HF_MODEL_NAME,\n",
                "    repo_type=\"model\"\n",
                ")\n",
                "\n",
                "print(\"âœ“ Model card created and uploaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cleanup_header"
            },
            "source": [
                "## Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cleanup"
            },
            "outputs": [],
            "source": [
                "# Cleanup to free GPU memory\n",
                "del student_logits_only\n",
                "clear_gpu_cache()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TRAINING COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nThe trained model has been saved to Hugging Face as '{HF_MODEL_NAME}'\")\n",
                "print(f\"URL: https://huggingface.co/{HF_MODEL_NAME}\")\n",
                "print(\"\\nYou can load it using:\")\n",
                "print(f\"  model = AutoModelForCausalLM.from_pretrained('{HF_MODEL_NAME}')\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "machine_shape": "hm",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
