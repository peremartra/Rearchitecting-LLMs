{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Tailoring-LLM-Architectures/blob/main/CH06/CH06_NB02_Logits_KLD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_header"
      },
      "source": [
        "# **Tailoring LLM Architectures**\n",
        "## **Chapter 6: Knowledge Distillation with Logits Only**\n",
        "### Training a pruned model using only Logits and Skew KL Divergence\n",
        "\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "---\n",
        "\n",
        "**Hardware Environment:** NVIDIA A100 GPU\n",
        "- **Model:** google/gemma-3-270m (Teacher: 18 transformer blocks) → Pruned Student (14 transformer blocks)\n",
        "- **Dataset:** Cosmopedia (40,000 samples, 3 epochs)\n",
        "\n",
        "---\n",
        "\n",
        "**What we'll accomplish:**\n",
        "- Train a depth-pruned student model using **only logits and Skew KL Divergence**\n",
        "- No hidden state alignment (simpler, faster training)\n",
        "- Uses the same training framework as advanced KD but configured for logits-only\n",
        "- Save the trained model to Hugging Face as **gem-3-small**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section0_header"
      },
      "source": [
        "## Section 0: Environment & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Drive mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_cell"
      },
      "outputs": [],
      "source": [
        "RECOVERY_SAMPLES = 40000\n",
        "EPOCHS=3\n",
        "LEARNING_RATE=4e-5\n",
        "BATCH_SIZE = 16\n",
        "BATCH_EVAL=\"auto\"\n",
        "RUN_FULL_BENCHMARKS = True\n",
        "BENCHMARK_LIMIT = None\n",
        "BENCHMARK_TASKS = [\"arc_easy\", \"winogrande\", \"hellaswag\", \"lambada_openai\", \"piqa\"]\n",
        "HF_MODEL_NAME = \"gem-3-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate datasets\n",
        "!pip install -q optipfair matplotlib seaborn tqdm\n",
        "!pip install -q lm_eval langdetect codecarbon huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_cell"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from copy import deepcopy\n",
        "import warnings, time, json, os\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seed_cell"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "set_seed(42)\n",
        "print(\"✓ Random seed set to 42\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utils_download"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
        "from utils import evaluate_metrics, clear_gpu_cache, model_evaluation\n",
        "print(\"✓ utils.py loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1_header"
      },
      "source": [
        "## Section 1: Load Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_teacher"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"google/gemma-3-270m\"\n",
        "print(f\"Loading Teacher model: {MODEL_NAME}\")\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "teacher_model.eval()\n",
        "for param in teacher_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "n_teacher_layers = len(teacher_model.model.layers)\n",
        "hidden_dim = teacher_model.config.hidden_size\n",
        "print(f\"Teacher: {n_teacher_layers} layers, {teacher_model.num_parameters():,} params\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2_header"
      },
      "source": [
        "## Section 2: Prepare Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 512\n",
        "print(\"Loading Cosmopedia dataset...\")\n",
        "dataset_name = \"HuggingFaceTB/cosmopedia\"\n",
        "subsets = [\"stories\", \"wikihow\", \"openstax\", \"web_samples_v1\"]\n",
        "samples_per_subset = int(RECOVERY_SAMPLES / 4)\n",
        "\n",
        "all_samples = []\n",
        "for subset in subsets:\n",
        "    print(f\"  Loading {subset}...\")\n",
        "    subset_data = load_dataset(dataset_name, subset, split=\"train\", streaming=True)\n",
        "    subset_samples = list(subset_data.take(samples_per_subset))\n",
        "    all_samples.extend(subset_samples)\n",
        "    print(f\"    ✓ {len(subset_samples):,} samples\")\n",
        "\n",
        "distillation_dataset = Dataset.from_dict({'text': [s['text'] for s in all_samples]})\n",
        "print(f\"✓ Total samples: {len(distillation_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokenize_data"
      },
      "outputs": [],
      "source": [
        "print(\"Tokenizing...\")\n",
        "texts = [item['text'] for item in distillation_dataset]\n",
        "tokenized_data = []\n",
        "for i in tqdm(range(0, len(texts), 1000), desc=\"Tokenizing\"):\n",
        "    batch = tokenizer(texts[i:i+1000], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
        "    tokenized_data.append(batch)\n",
        "\n",
        "input_ids = torch.cat([b['input_ids'] for b in tokenized_data], dim=0)\n",
        "attention_mask = torch.cat([b['attention_mask'] for b in tokenized_data], dim=0)\n",
        "full_dataset = TensorDataset(input_ids, attention_mask)\n",
        "\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eval_dataloader_raw = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class DictDataLoader:\n",
        "    def __init__(self, dl): self.dataloader = dl\n",
        "    def __iter__(self):\n",
        "        for input_ids, attention_mask in self.dataloader:\n",
        "            yield {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "    def __len__(self): return len(self.dataloader)\n",
        "\n",
        "eval_dataloader = DictDataLoader(eval_dataloader_raw)\n",
        "print(f\"✓ Train: {len(train_dataset):,}, Val: {len(val_dataset):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3_header"
      },
      "source": [
        "## Section 3: Create Pruned Student Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_pruned"
      },
      "outputs": [],
      "source": [
        "import optipfair as opf\n",
        "student_model = deepcopy(teacher_model)\n",
        "importance_scores = opf.analyze_layer_importance(student_model, train_dataloader, show_progress=True)\n",
        "LAYERS_TO_REMOVE = sorted(importance_scores.keys(), key=lambda x: importance_scores[x])[:4]\n",
        "print(f\"Layers to remove: {LAYERS_TO_REMOVE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prune_student"
      },
      "outputs": [],
      "source": [
        "student_model = opf.prune_model_depth(model=student_model, layer_indices=LAYERS_TO_REMOVE, show_progress=True)\n",
        "for param in student_model.parameters():\n",
        "    param.requires_grad = True\n",
        "n_student_layers = len(student_model.model.layers)\n",
        "print(f\"✓ Student: {n_student_layers} layers, {student_model.num_parameters():,} params\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_baselines"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating Teacher...\")\n",
        "teacher_metrics = evaluate_metrics(teacher_model, eval_dataloader, device=device)\n",
        "teacher_ppl = teacher_metrics['perplexity']\n",
        "teacher_loss = teacher_metrics['loss']\n",
        "\n",
        "print(\"Evaluating Pruned Student...\")\n",
        "student_pruned_copy = deepcopy(student_model)\n",
        "student_metrics = evaluate_metrics(student_pruned_copy, eval_dataloader, device=device)\n",
        "student_ppl = student_metrics['perplexity']\n",
        "student_loss = student_metrics['loss']\n",
        "del student_pruned_copy\n",
        "clear_gpu_cache()\n",
        "\n",
        "print(f\"Teacher PPL: {teacher_ppl:.2f}, Student PPL: {student_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "benchmark_baselines"
      },
      "outputs": [],
      "source": [
        "benchmark_results = {}\n",
        "if RUN_FULL_BENCHMARKS:\n",
        "    print(\"Benchmarking Teacher...\")\n",
        "    benchmark_results['teacher'] = model_evaluation(model_obj=teacher_model, tokenizer=tokenizer, tasks=BENCHMARK_TASKS, device=device, limit=BENCHMARK_LIMIT, batch_size=BATCH_EVAL)\n",
        "    print(\"Benchmarking Pruned Student...\")\n",
        "    student_pruned_copy = deepcopy(student_model)\n",
        "    benchmark_results['student_pruned'] = model_evaluation(model_obj=student_pruned_copy, tokenizer=tokenizer, tasks=BENCHMARK_TASKS, device=device, limit=BENCHMARK_LIMIT, batch_size=BATCH_EVAL)\n",
        "    del student_pruned_copy\n",
        "    clear_gpu_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4_header"
      },
      "source": [
        "## Section 4: Shared Training Functions (from NB01)\n",
        "\n",
        "These functions support both logits-only and advanced KD with hidden states. For logits-only training, we set `gamma=0.0` and `delta=0.0` to disable trajectory and FDD losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loss_function"
      },
      "outputs": [],
      "source": [
        "def train_student_advanced(\n",
        "    student_model,\n",
        "    teacher_model,\n",
        "    dataloader,\n",
        "    layer_map=None,      # Not used for logits-only, but kept for compatibility\n",
        "    # Loss weights\n",
        "    alpha=0.1,           # Task loss\n",
        "    beta=0.8,            # Skew KLD (logits)\n",
        "    gamma=0.05,          # Trajectory loss (hidden states)\n",
        "    delta=0.05,          # FDD derivative loss\n",
        "    temperature=2.0,\n",
        "    skew_alpha=0.5,      # Skew interpolation factor\n",
        "    # Training params\n",
        "    epochs=3,\n",
        "    learning_rate=4e-5,\n",
        "    experiment_name=\"experiment\",\n",
        "    accumulation_steps=4\n",
        "):\n",
        "    \"\"\"\n",
        "    Train student model with Advanced Compound Loss (Skew KLD + FDD).\n",
        "    Can be configured for logits-only by setting gamma=0.0, delta=0.0.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    # Decide if we need hidden states (required if using trajectory OR derivative loss)\n",
        "    request_hidden_states = (gamma > 0 or delta > 0)\n",
        "\n",
        "    loss_history = {\n",
        "        'total': [], 'task': [], 'logits': [],\n",
        "        'trajectory': [], 'derivative': []\n",
        "    }\n",
        "    epoch_times = []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting Training: {experiment_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Epochs: {epochs}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "    print(f\"Loss weights: α={alpha}, β={beta}, γ={gamma}, δ={delta}\")\n",
        "    print(f\"Temperature: {temperature}, Skew α: {skew_alpha}\")\n",
        "    print(f\"Hidden states computation: {'ENABLED' if request_hidden_states else 'DISABLED'}\")\n",
        "    print(f\"Gradient Accumulation Steps: {accumulation_steps}\")\n",
        "    print(f\"Effective Batch Size: {dataloader.batch_size * accumulation_steps}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        epoch_losses = {k: [] for k in loss_history.keys()}\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        accumulated_losses = {k: 0.0 for k in loss_history.keys()}\n",
        "        accumulation_counter = 0\n",
        "\n",
        "        for batch_idx, (input_ids, attention_mask) in enumerate(progress_bar):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            # Student forward pass\n",
        "            student_outputs = student_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=request_hidden_states\n",
        "            )\n",
        "\n",
        "            # Teacher forward pass (no gradients)\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    output_hidden_states=request_hidden_states\n",
        "                )\n",
        "\n",
        "            # Prepare hidden states (None if not needed)\n",
        "            student_hiddens = (student_outputs.hidden_states[1:]\n",
        "                             if request_hidden_states else None)\n",
        "            teacher_hiddens = (teacher_outputs.hidden_states[1:]\n",
        "                             if request_hidden_states else None)\n",
        "\n",
        "            # Compute advanced compound loss\n",
        "            loss, loss_dict = compute_compound_loss_advanced(\n",
        "                student_logits=student_outputs.logits,\n",
        "                teacher_logits=teacher_outputs.logits,\n",
        "                student_hiddens=student_hiddens,\n",
        "                teacher_hiddens=teacher_hiddens,\n",
        "                labels=labels,\n",
        "                layer_map=layer_map,\n",
        "                alpha=alpha,\n",
        "                beta=beta,\n",
        "                gamma=gamma,\n",
        "                delta=delta,\n",
        "                temperature=temperature,\n",
        "                skew_alpha=skew_alpha\n",
        "            )\n",
        "\n",
        "            # Gradient accumulation\n",
        "            scaled_loss = loss / accumulation_steps\n",
        "            scaled_loss.backward()\n",
        "\n",
        "            for key in accumulated_losses:\n",
        "                accumulated_losses[key] += loss_dict[key]\n",
        "            accumulation_counter += 1\n",
        "\n",
        "            # Optimizer step\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                avg_losses = {k: v / accumulation_counter for k, v in accumulated_losses.items()}\n",
        "                for key in avg_losses:\n",
        "                    epoch_losses[key].append(avg_losses[key])\n",
        "\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f\"{avg_losses['total']:.4f}\",\n",
        "                    'task': f\"{avg_losses['task']:.4f}\",\n",
        "                    'logits': f\"{avg_losses['logits']:.4f}\",\n",
        "                    'traj': f\"{avg_losses['trajectory']:.4f}\",\n",
        "                    'deriv': f\"{avg_losses['derivative']:.4f}\"\n",
        "                })\n",
        "\n",
        "                accumulated_losses = {k: 0.0 for k in loss_history.keys()}\n",
        "                accumulation_counter = 0\n",
        "\n",
        "        # Handle remaining batches\n",
        "        if accumulation_counter > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            avg_losses = {k: v / accumulation_counter for k, v in accumulated_losses.items()}\n",
        "            for key in avg_losses:\n",
        "                epoch_losses[key].append(avg_losses[key])\n",
        "\n",
        "        # Record epoch averages\n",
        "        for key in epoch_losses:\n",
        "            if epoch_losses[key]:\n",
        "                loss_history[key].append(np.mean(epoch_losses[key]))\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} avg losses - \"\n",
        "              f\"Total: {loss_history['total'][-1]:.4f}, \"\n",
        "              f\"Task: {loss_history['task'][-1]:.4f}, \"\n",
        "              f\"Logits: {loss_history['logits'][-1]:.4f}, \"\n",
        "              f\"Traj: {loss_history['trajectory'][-1]:.4f}, \"\n",
        "              f\"Deriv: {loss_history['derivative'][-1]:.4f} \"\n",
        "              f\"[{epoch_time:.1f}s]\")\n",
        "\n",
        "    total_time = time.time() - total_start_time\n",
        "    loss_history['epoch_times_seconds'] = epoch_times\n",
        "    loss_history['total_time_seconds'] = total_time\n",
        "\n",
        "    print(f\"\\n✓ Training completed: {experiment_name}\")\n",
        "    print(f\"  Total time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
        "    print(f\"  Avg time per epoch: {np.mean(epoch_times):.1f}s\")\n",
        "\n",
        "    return student_model, loss_history\n",
        "\n",
        "\n",
        "def compute_compound_loss_advanced(\n",
        "    student_logits,      # [batch, seq_len, vocab_size]\n",
        "    teacher_logits,      # [batch, seq_len, vocab_size]\n",
        "    student_hiddens,     # List of [batch, seq_len, hidden_dim] or None\n",
        "    teacher_hiddens,     # List of [batch, seq_len, hidden_dim] or None\n",
        "    labels,              # [batch, seq_len]\n",
        "    layer_map,           # List of teacher indices for each student Transformer Block\n",
        "    alpha=0.1,           # weight for task loss\n",
        "    beta=0.8,            # weight for logits loss (Skew KLD)\n",
        "    gamma=0.1,           # weight for hidden trajectory loss\n",
        "    delta=0.1,           # weight for FDD derivative loss\n",
        "    temperature=2.0,     # temperature for soft labels\n",
        "    skew_alpha=0.1       # interpolation factor for Skew KLD (0=Forward, 1=Reverse)\n",
        "):\n",
        "    \"\"\"\n",
        "    Advanced compound loss combining state-of-the-art techniques:\n",
        "\n",
        "    1. Task Loss: Standard cross-entropy with hard labels\n",
        "    2. Skew KLD: Interpolates between Forward and Reverse KLD (DistiLLM-2)\n",
        "       - Forward KLD (α=0): Student covers all teacher modes (mean-seeking)\n",
        "       - Reverse KLD (α=1): Student focuses on high-confidence modes (mode-seeking)\n",
        "       - Skew (α=0.1): Best of both worlds, numerically stable\n",
        "    3. Trajectory Loss: Cosine similarity between hidden states (standard feature KD)\n",
        "    4. FDD Derivative Loss: Aligns the \"rate of change\" between consecutive Transformer Blocks\n",
        "       - Forces student to learn HOW to transform representations, not just WHAT to produce\n",
        "       - Critical for depth-pruned models that must take \"bigger steps\" with fewer Transformer Blocks\n",
        "\n",
        "    Reference:\n",
        "    - Skew KLD: DistiLLM-2 (2024)\n",
        "    - FDD: Feature Dynamics Distillation, ACL 2025\n",
        "    \"\"\"\n",
        "    device = student_logits.device\n",
        "\n",
        "    # =========================================================================\n",
        "    # 1. TASK LOSS (Cross-Entropy with hard labels)\n",
        "    # =========================================================================\n",
        "    shift_logits = student_logits[..., :-1, :].contiguous()\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "    loss_task = F.cross_entropy(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1),\n",
        "        ignore_index=-100\n",
        "    )\n",
        "\n",
        "    # =========================================================================\n",
        "    # 2. SKEW KLD (Interpolated Divergence for Logits)\n",
        "    # =========================================================================\n",
        "    # Standard softmax with temperature\n",
        "    with torch.no_grad():\n",
        "        student_probs = F.softmax(student_logits[..., :-1, :] / temperature, dim=-1)\n",
        "        teacher_probs = F.softmax(teacher_logits[..., :-1, :] / temperature, dim=-1)\n",
        "\n",
        "        # Skew: interpolate between teacher and student distributions\n",
        "        # When skew_alpha=0.0: 100% teacher (Forward KLD, equivalent to standard)\n",
        "        # When skew_alpha=0.5: 50% teacher + 50% student (balanced)\n",
        "        mixed_probs = skew_alpha * student_probs + (1 - skew_alpha) * teacher_probs\n",
        "\n",
        "    # KL divergence against the mixed target\n",
        "    student_log_probs = F.log_softmax(student_logits[..., :-1, :] / temperature, dim=-1)\n",
        "    kl_elementwise = student_probs * (student_log_probs - torch.log(mixed_probs + 1e-9))\n",
        "    loss_logits = kl_elementwise.sum(dim=-1).mean() * (temperature ** 2)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 3. TRAJECTORY LOSS (Cosine Similarity of Hidden States)\n",
        "    # =========================================================================\n",
        "    # Standard feature alignment - matches \"where\" the student is in representation space\n",
        "    # Only compute if gamma > 0 and we have hidden states\n",
        "    if gamma > 0 and student_hiddens is not None and teacher_hiddens is not None:\n",
        "        loss_trajectory = 0.0\n",
        "        for student_idx, teacher_idx in enumerate(layer_map):\n",
        "            student_h = student_hiddens[student_idx]\n",
        "            teacher_h = teacher_hiddens[teacher_idx]\n",
        "\n",
        "            # Flatten and normalize\n",
        "            student_flat = student_h.reshape(-1, student_h.size(-1))\n",
        "            teacher_flat = teacher_h.reshape(-1, teacher_h.size(-1))\n",
        "            student_norm = F.normalize(student_flat, p=2, dim=1)\n",
        "            teacher_norm = F.normalize(teacher_flat, p=2, dim=1)\n",
        "\n",
        "            # Cosine similarity loss: 1 - cos_sim (0 = perfect alignment)\n",
        "            cos_sim = (student_norm * teacher_norm).sum(dim=1).mean()\n",
        "            loss_trajectory += (1 - cos_sim)\n",
        "\n",
        "        loss_trajectory = loss_trajectory / len(layer_map)\n",
        "    else:\n",
        "        loss_trajectory = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # =========================================================================\n",
        "    # 4. FDD DERIVATIVE LOSS (Feature Dynamics Distillation)\n",
        "    # =========================================================================\n",
        "    # Matches the \"velocity\" of representation change between consecutive Transformer Blocks\n",
        "    # Key insight: In residual networks, x_{l+1} = x_l + F(x_l)\n",
        "    # The delta F(x_l) represents how much the Transformer Block transforms the representation\n",
        "    # A depth-pruned student must learn to make larger, more efficient transformations\n",
        "    # Only compute if delta > 0 and we have hidden states\n",
        "    loss_derivative = torch.tensor(0.0, device=device)\n",
        "\n",
        "    if delta > 0 and student_hiddens is not None and teacher_hiddens is not None:\n",
        "        num_derivatives = 0\n",
        "\n",
        "        for student_idx in range(len(layer_map) - 1):\n",
        "            teacher_idx = layer_map[student_idx]\n",
        "            teacher_idx_next = layer_map[student_idx + 1]\n",
        "\n",
        "            # Student delta: change between consecutive student Transformer Blocks\n",
        "            student_delta = student_hiddens[student_idx + 1] - student_hiddens[student_idx]\n",
        "\n",
        "            # Teacher delta: change between corresponding teacher Transformer Blocks\n",
        "            teacher_delta = teacher_hiddens[teacher_idx_next] - teacher_hiddens[teacher_idx]\n",
        "\n",
        "            # Flatten and normalize the deltas\n",
        "            student_delta_flat = student_delta.reshape(-1, student_delta.size(-1))\n",
        "            teacher_delta_flat = teacher_delta.reshape(-1, teacher_delta.size(-1))\n",
        "\n",
        "            student_delta_norm = F.normalize(student_delta_flat, p=2, dim=1)\n",
        "            teacher_delta_norm = F.normalize(teacher_delta_flat, p=2, dim=1)\n",
        "\n",
        "            # Cosine similarity of derivatives\n",
        "            cos_sim_deriv = (student_delta_norm * teacher_delta_norm).sum(dim=1).mean()\n",
        "            loss_derivative += (1 - cos_sim_deriv)\n",
        "            num_derivatives += 1\n",
        "\n",
        "        if num_derivatives > 0:\n",
        "            loss_derivative = loss_derivative / num_derivatives\n",
        "\n",
        "    # =========================================================================\n",
        "    # COMBINE ALL LOSSES\n",
        "    # =========================================================================\n",
        "    total_loss = (\n",
        "        alpha * loss_task +\n",
        "        beta * loss_logits +\n",
        "        gamma * loss_trajectory +\n",
        "        delta * loss_derivative\n",
        "    )\n",
        "\n",
        "    loss_dict = {\n",
        "        'total': total_loss.item(),\n",
        "        'task': loss_task.item(),\n",
        "        'logits': loss_logits.item(),\n",
        "        'trajectory': loss_trajectory.item(),\n",
        "        'derivative': loss_derivative.item()\n",
        "    }\n",
        "\n",
        "    return total_loss, loss_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_header"
      },
      "source": [
        "## Section 5: Train with Logits Only\n",
        "\n",
        "Using the shared training function configured for logits-only mode (gamma=0, delta=0, skew_alpha=0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model"
      },
      "outputs": [],
      "source": [
        "student_logits_only = deepcopy(student_model)\n",
        "student_trained, history = train_student_advanced(\n",
        "    student_model=student_logits_only, \n",
        "    teacher_model=teacher_model, \n",
        "    dataloader=train_dataloader,\n",
        "    layer_map=None,          # Not used for logits-only\n",
        "    # Loss weights - Logits-only configuration\n",
        "    alpha=0.5,               # Task loss weight\n",
        "    beta=0.5,                # KLD weight\n",
        "    gamma=0.0,               # NO trajectory loss (disabled)\n",
        "    delta=0.0,               # NO FDD loss (disabled)\n",
        "    temperature=2.0,\n",
        "    skew_alpha=0.0,          # Forward KLD (0.0 = standard KLD, equivalent to original)\n",
        "    # Training params\n",
        "    epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    experiment_name=\"Logits-Only with Skew KLD Training\",\n",
        "    accumulation_steps=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_trained"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating Trained Student...\")\n",
        "trained_metrics = evaluate_metrics(student_trained, eval_dataloader, device=device)\n",
        "trained_ppl = trained_metrics['perplexity']\n",
        "trained_loss = trained_metrics['loss']\n",
        "print(f\"Trained PPL: {trained_ppl:.2f}\")\n",
        "\n",
        "degradation = student_ppl - teacher_ppl\n",
        "recovered = student_ppl - trained_ppl\n",
        "recovery_pct = (recovered / degradation) * 100 if degradation > 0 else 0\n",
        "print(f\"Recovery: {recovery_pct:.1f}%\")\n",
        "\n",
        "if RUN_FULL_BENCHMARKS:\n",
        "    print(\"Benchmarking Trained Student...\")\n",
        "    benchmark_results['logits_only_trained'] = model_evaluation(model_obj=student_trained, tokenizer=tokenizer, tasks=BENCHMARK_TASKS, device=device, limit=BENCHMARK_LIMIT, batch_size=BATCH_EVAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section7_header"
      },
      "source": [
        "## Section 6: Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_ppl"
      },
      "outputs": [],
      "source": [
        "# Perplexity Bar Chart\n",
        "models = ['Teacher\\n(Baseline)', 'Pruned\\n(No KD)', 'Trained\\n(Logits KD)']\n",
        "ppls = [teacher_ppl, student_ppl, trained_ppl]\n",
        "colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(models, ppls, color=colors, edgecolor='black', linewidth=1.5)\n",
        "ax.set_ylabel('Perplexity (↓ lower is better)', fontsize=12)\n",
        "ax.set_title('Perplexity Comparison: Teacher vs Pruned vs Trained', fontsize=14, fontweight='bold')\n",
        "for bar, ppl in zip(bars, ppls):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{ppl:.2f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "ax.axhline(y=teacher_ppl, color='#2ecc71', linestyle='--', alpha=0.7, label=f'Teacher baseline: {teacher_ppl:.2f}')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('ppl_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"\\n✓ Recovery: {recovery_pct:.1f}% of degradation recovered\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_benchmarks"
      },
      "outputs": [],
      "source": [
        "# Benchmark Bar Chart\n",
        "if RUN_FULL_BENCHMARKS and benchmark_results:\n",
        "    tasks = BENCHMARK_TASKS\n",
        "    x = np.arange(len(tasks))\n",
        "    width = 0.25\n",
        "    \n",
        "    teacher_scores = [benchmark_results['teacher'].get(t, {}).get('acc', 0) * 100 for t in tasks]\n",
        "    pruned_scores = [benchmark_results['student_pruned'].get(t, {}).get('acc', 0) * 100 for t in tasks]\n",
        "    trained_scores = [benchmark_results['logits_only_trained'].get(t, {}).get('acc', 0) * 100 for t in tasks]\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    bars1 = ax.bar(x - width, teacher_scores, width, label='Teacher (Baseline)', color='#2ecc71', edgecolor='black')\n",
        "    bars2 = ax.bar(x, pruned_scores, width, label='Pruned (No KD)', color='#e74c3c', edgecolor='black')\n",
        "    bars3 = ax.bar(x + width, trained_scores, width, label='Trained (Logits KD)', color='#3498db', edgecolor='black')\n",
        "    \n",
        "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax.set_title('Benchmark Comparison: Teacher vs Pruned vs Trained', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([t.replace('_', '\\n') for t in tasks], fontsize=10)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    def add_labels(bars):\n",
        "        for bar in bars:\n",
        "            h = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, h + 1, f'{h:.1f}', ha='center', va='bottom', fontsize=8)\n",
        "    add_labels(bars1)\n",
        "    add_labels(bars2)\n",
        "    add_labels(bars3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('benchmark_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print average recovery\n",
        "    avg_teacher = np.mean(teacher_scores)\n",
        "    avg_pruned = np.mean(pruned_scores)\n",
        "    avg_trained = np.mean(trained_scores)\n",
        "    bench_degradation = avg_teacher - avg_pruned\n",
        "    bench_recovered = avg_trained - avg_pruned\n",
        "    bench_recovery_pct = (bench_recovered / bench_degradation) * 100 if bench_degradation > 0 else 0\n",
        "    print(f\"\\nBenchmark Average: Teacher={avg_teacher:.1f}%, Pruned={avg_pruned:.1f}%, Trained={avg_trained:.1f}%\")\n",
        "    print(f\"Benchmark Recovery: {bench_recovery_pct:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_training"
      },
      "outputs": [],
      "source": [
        "# Training Loss Curves - Extract only relevant losses (trajectory & derivative are 0)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "loss_keys = [\n",
        "    ('total', 'Total Loss'), \n",
        "    ('task', 'Task Loss (CE)'), \n",
        "    ('logits', 'Logits Loss (Skew KLD)')\n",
        "]\n",
        "for idx, (key, title) in enumerate(loss_keys):\n",
        "    axes[idx].plot(history[key], marker='o', linewidth=2, markersize=8)\n",
        "    axes[idx].set_title(title, fontsize=12)\n",
        "    axes[idx].set_xlabel('Epoch')\n",
        "    axes[idx].set_ylabel('Loss')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "plt.suptitle('Training Progress: Logits-Only Knowledge Distillation', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section75_header"
      },
      "source": [
        "## Section 6.5: Save Experiment Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer): return int(obj)\n",
        "        if isinstance(obj, np.floating): return float(obj)\n",
        "        if isinstance(obj, np.ndarray): return obj.tolist()\n",
        "        return super().default(obj)\n",
        "\n",
        "results_data = {\n",
        "    \"metadata\": {\"experiment_name\": \"Logits-Only KD with Skew KLD\", \"timestamp\": datetime.now().isoformat(), \"torch_version\": torch.__version__},\n",
        "    \"models\": {\"teacher\": MODEL_NAME, \"student\": f\"Gemma-270m-Pruned ({n_student_layers} layers)\"},\n",
        "    \"training_config\": {\"alpha\": 0.5, \"beta\": 0.5, \"gamma\": 0.0, \"delta\": 0.0, \"skew_alpha\": 0.0, \"temperature\": 2.0, \"epochs\": EPOCHS, \"learning_rate\": LEARNING_RATE, \"batch_size\": BATCH_SIZE, \"samples\": RECOVERY_SAMPLES},\n",
        "    \"results\": {\n",
        "        \"teacher\": {\"perplexity\": teacher_ppl, \"loss\": teacher_loss},\n",
        "        \"student_pruned\": {\"perplexity\": student_ppl, \"loss\": student_loss},\n",
        "        \"logits_only_trained\": {\"perplexity\": trained_ppl, \"loss\": trained_loss, \"recovery_pct\": recovery_pct, \"training_time\": history.get('total_time_seconds', 0)}\n",
        "    }\n",
        "}\n",
        "if RUN_FULL_BENCHMARKS:\n",
        "    for k, v in benchmark_results.items():\n",
        "        if k in results_data['results']: results_data['results'][k]['benchmarks'] = v\n",
        "\n",
        "json_path = \"/content/drive/MyDrive/ch06nb01/CH06_NB02_Logits_KLD_results.json\"\n",
        "os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
        "with open(json_path, 'w') as f: json.dump(results_data, f, indent=2, cls=NumpyEncoder)\n",
        "print(f\"✓ Results saved to: {json_path}\")\n",
        "with open(\"CH06_NB02_Logits_KLD_results.json\", 'w') as f: json.dump(results_data, f, indent=2, cls=NumpyEncoder)\n",
        "print(\"✓ Local backup saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section8_header"
      },
      "source": [
        "## Section 7: Save Model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf_login"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login, HfApi\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "push_model"
      },
      "outputs": [],
      "source": [
        "print(f\"Saving model to HuggingFace as '{HF_MODEL_NAME}'...\")\n",
        "student_trained.push_to_hub(HF_MODEL_NAME, commit_message=\"Depth-pruned Gemma-3-270m with Logits-Only KD\")\n",
        "tokenizer.push_to_hub(HF_MODEL_NAME, commit_message=\"Tokenizer for gem-3-small\")\n",
        "print(f\"✓ Model saved: https://huggingface.co/{HF_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "del student_logits_only\n",
        "clear_gpu_cache()\n",
        "print(f\"\\n{'='*60}\\nTRAINING COMPLETE\\n{'='*60}\")\n",
        "print(f\"Model: https://huggingface.co/{HF_MODEL_NAME}\")\n",
        "print(f\"Results: {json_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
