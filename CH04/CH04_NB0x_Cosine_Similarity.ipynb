{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6iKp/Mokh3y4FrO/YZCUb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH04/CH04_NB0x_Cosine_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 4: Depth Pruning: Building Smaller and Faster Models\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* Qwen3-0.6B\n",
        "_____\n",
        "\n",
        "In this notebook we explore how to evaluate the contribution of different transformer blocks to the LLM’s objective using a dataset.\n",
        "\n",
        "To do this, we use cosine similarity between the input and the output of the transformer block. The lower the similarity, the greater the modification that block has introduced to the data.\n",
        "\n",
        "Blocks with higher similarity between input and output will be the candidates to be removed from the model.\n"
      ],
      "metadata": {
        "id": "vf04d7qXqwUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up notebook"
      ],
      "metadata": {
        "id": "3XUED5yYwoJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "      \"torch==2.8.0+cu126\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "      \"lm_eval==0.4.9.1\" \\\n",
        "      \"sentencepiece==0.2.1\" \\\n",
        "      \"sentence-transformers==5.1.0\" \\\n",
        "      \"optipfair==0.1.4\""
      ],
      "metadata": {
        "id": "4AlU0F9Nu6xT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CCBC8p1FvBsV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R5xvVourquO6",
        "outputId": "746ef66f-08e4-45b4-ac6c-e14bc3d9a1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "ghFZPniOw0fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MpLvzaifv_B6",
        "outputId": "9ec0937f-25bf-476c-cf4e-0e7843d15a25"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen3ForCausalLM(\n",
              "  (model): Qwen3Model(\n",
              "    (embed_tokens): Embedding(151936, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen3DecoderLayer(\n",
              "        (self_attn): Qwen3Attention(\n",
              "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Qwen3MLP(\n",
              "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "    (rotary_emb): Qwen3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Datasets"
      ],
      "metadata": {
        "id": "gP2pYvmfw3hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RECOVERY_SAMPLES = 100\n",
        "BATCH_SIZE = 8\n",
        "MAX_LENGTH = 512"
      ],
      "metadata": {
        "id": "B0INZII5BIMM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re going to use two different datasets to visualize how some layers are more important than others depending on the data being used.\n",
        "\n",
        "* **Wikitext**: Contains highly complex text. To process this kind of text, the model needs to rely on its deeper layers to understand context, semantic relations, and complex grammatical structures.\n",
        "* **SMS Spam**: A completely different dataset, made up of short sentences with simple and direct language. It doesn’t require deep semantic understanding.\n"
      ],
      "metadata": {
        "id": "2epEOHrEbT36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1 = load_dataset('wikitext', 'wikitext-2-raw-v1', split=f'train[:100]')\n",
        "\n",
        "dataset2 = load_dataset('sms_spam', split=f'train[:100]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AxpLIebQoJe",
        "outputId": "5008f39c-2f81-4941-e997-1227205caa84"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando WikiText-2 (frases largas, enciclopédico)...\n",
            "Cargando SMS Spam (frases muy cortas, mensajes)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset, text_field='text'):\n",
        "  def tokenize_function(examples):\n",
        "      if text_field in examples:\n",
        "          texts = examples[text_field]\n",
        "      elif 'sms' in examples:  # SMS dataset\n",
        "          texts = examples['sms']\n",
        "      elif 'text' in examples:\n",
        "          texts = examples['text']\n",
        "      else:\n",
        "          texts = examples[list(examples.keys())[0]]  # First available field\n",
        "\n",
        "      return tokenizer(\n",
        "          texts,\n",
        "          truncation=True,\n",
        "          padding='max_length',\n",
        "          max_length=MAX_LENGTH,\n",
        "          return_tensors='pt'\n",
        "      )\n",
        "\n",
        "  tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "  tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "  return DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "8YAKyNaL6bqr"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear dataloaders\n",
        "dataloader1 = prepare_dataset(dataset1)  # WikiText (largo)\n",
        "dataloader2 = prepare_dataset(dataset2)  # SMS (corto)"
      ],
      "metadata": {
        "id": "VDYfIVBFXj_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Layer Importande using Cosine Similarity\n",
        "\n",
        "To decide which layers to remove, we measure their contribution using cosine similarity. We chose this metric because it’s perfect for this task: it measures the change in semantic direction between the input and output vectors of a layer, ignoring their magnitude.\n",
        "\n",
        "This gives us a normalized score that we convert into an importance score (1 - similarity).\n",
        "\n",
        "A score close to zero identifies a “passive” layer that barely alters the information, making it an ideal candidate for removal.\n"
      ],
      "metadata": {
        "id": "nGHINZat6ldT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Model Hooks.\n"
      ],
      "metadata": {
        "id": "Af4D7jK87Kud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture the input and output of the layers we use PyTorch hooks, which let us study/spy on the model’s behavior.\n"
      ],
      "metadata": {
        "id": "IC3EkRvAeZWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_layer_hooks(model):\n",
        "    \"\"\"\n",
        "    Register hooks to capture input/output of each transformer layer\n",
        "    Returns: hooks list and storage dictionaries\n",
        "    \"\"\"\n",
        "    num_layers = len(model.model.layers)\n",
        "    layer_inputs = {}\n",
        "    layer_outputs = {}\n",
        "    hooks = []\n",
        "\n",
        "    def create_input_hook(layer_idx):\n",
        "        def hook(module, input):\n",
        "            if isinstance(input, tuple) and len(input) > 0:\n",
        "                layer_inputs[layer_idx] = input[0].detach()\n",
        "        return hook\n",
        "\n",
        "    def create_output_hook(layer_idx):\n",
        "        def hook(module, input, output):\n",
        "            if isinstance(output, tuple) and len(output) > 0:\n",
        "                layer_outputs[layer_idx] = output[0].detach()\n",
        "            else:\n",
        "                layer_outputs[layer_idx] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    # Register hooks for each layer\n",
        "    for i, layer in enumerate(model.model.layers):\n",
        "        hooks.append(layer.register_forward_pre_hook(create_input_hook(i)))\n",
        "        hooks.append(layer.register_forward_hook(create_output_hook(i)))\n",
        "\n",
        "    return hooks, layer_inputs, layer_outputs, num_layers"
      ],
      "metadata": {
        "id": "vT40G6hq6itp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Cosine Similarity"
      ],
      "metadata": {
        "id": "qBTC2Dh97jx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_importance(input_tensor, output_tensor, layer_idx, is_first_batch=False):\n",
        "    \"\"\"\n",
        "    Calculate importance score using cosine similarity between input and output tensors\n",
        "    Returns: importance score (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    # Validate tensor dimensions\n",
        "    if input_tensor.numel() == 0 or output_tensor.numel() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        # Flatten tensors: [batch_size, features]\n",
        "        input_flat = input_tensor.view(input_tensor.size(0), -1)\n",
        "        output_flat = output_tensor.view(output_tensor.size(0), -1)\n",
        "\n",
        "        # Filter out non-finite values\n",
        "        input_valid_mask = torch.all(torch.isfinite(input_flat), dim=1)\n",
        "        output_valid_mask = torch.all(torch.isfinite(output_flat), dim=1)\n",
        "        valid_mask = input_valid_mask & output_valid_mask\n",
        "\n",
        "        if not valid_mask.any():\n",
        "            if is_first_batch:\n",
        "                print(f\"Warning: Layer {layer_idx} has all inf/nan samples\")\n",
        "            return 0.0\n",
        "\n",
        "        # Use only valid samples\n",
        "        input_valid = input_flat[valid_mask]\n",
        "        output_valid = output_flat[valid_mask]\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = F.cosine_similarity(input_valid, output_valid, dim=1)\n",
        "\n",
        "        # Filter finite similarities and calculate importance\n",
        "        finite_similarities = similarity[torch.isfinite(similarity)]\n",
        "        if len(finite_similarities) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        importance = 1 - finite_similarities.mean().item()\n",
        "\n",
        "        # Debug info for first batch only\n",
        "        if is_first_batch:\n",
        "            valid_samples = valid_mask.sum().item()\n",
        "            avg_similarity = finite_similarities.mean().item()\n",
        "\n",
        "        return importance\n",
        "\n",
        "    except Exception as e:\n",
        "        if is_first_batch:\n",
        "            print(f\"Error in layer {layer_idx}: {e}\")\n",
        "        return 0.0\n"
      ],
      "metadata": {
        "id": "TxH8OR6U6jWE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We aggregate the results"
      ],
      "metadata": {
        "id": "VkbDmAVNewak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_importance_scores(layer_scores):\n",
        "    \"\"\"\n",
        "    Aggregate importance scores across all batches\n",
        "    Returns: dictionary with final averaged scores per layer\n",
        "    \"\"\"\n",
        "    final_scores = {}\n",
        "    for layer_idx, scores in layer_scores.items():\n",
        "        if scores:\n",
        "            # Filter out invalid scores\n",
        "            valid_scores = [s for s in scores if not (np.isnan(s) or np.isinf(s))]\n",
        "            final_scores[layer_idx] = np.mean(valid_scores) if valid_scores else 0.0\n",
        "        else:\n",
        "            final_scores[layer_idx] = 0.0\n",
        "\n",
        "    return final_scores\n"
      ],
      "metadata": {
        "id": "dI5dJlfK7qMa"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes the importance scores collected from all data batches for each layer. Then, it computes the average of these scores to get a single final consolidated importance score for each layer of the model."
      ],
      "metadata": {
        "id": "AL33kTqtfbz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_layer_importance_cosine(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Calculate layer importance using cosine similarity between input/output representations\n",
        "\n",
        "    Args:\n",
        "        model: Transformer model\n",
        "        dataloader: DataLoader with tokenized text data\n",
        "        device: torch device (cuda/cpu)\n",
        "\n",
        "    Returns:\n",
        "        dict: Layer importance scores {layer_idx: importance_score}\n",
        "    \"\"\"\n",
        "    # Setup hooks and storage\n",
        "    hooks, layer_inputs, layer_outputs, num_layers = setup_layer_hooks(model)\n",
        "    layer_importance_scores = {i: [] for i in range(num_layers)}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass to trigger hooks\n",
        "            model(**inputs)\n",
        "\n",
        "            # Calculate importance for each layer\n",
        "            for layer_idx in range(num_layers):\n",
        "                if layer_idx not in layer_inputs or layer_idx not in layer_outputs:\n",
        "                    layer_importance_scores[layer_idx].append(0.0)\n",
        "                    continue\n",
        "\n",
        "                input_tensor = layer_inputs[layer_idx]\n",
        "                output_tensor = layer_outputs[layer_idx]\n",
        "\n",
        "                importance = calculate_cosine_importance(\n",
        "                    input_tensor, output_tensor, layer_idx,\n",
        "                    is_first_batch=(batch_idx == 0)\n",
        "                )\n",
        "                layer_importance_scores[layer_idx].append(importance)\n",
        "\n",
        "            # Clear storage for next batch\n",
        "            layer_inputs.clear()\n",
        "            layer_outputs.clear()\n",
        "\n",
        "    # Cleanup hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Aggregate final scores\n",
        "    final_scores = aggregate_importance_scores(layer_importance_scores)\n",
        "\n",
        "    return final_scores"
      ],
      "metadata": {
        "id": "iRjXO8fd7uWz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtaining & Studying results"
      ],
      "metadata": {
        "id": "LXZvQP6Ufge-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sorted_importance(scores):\n",
        "    for i, (layer, score) in enumerate(sorted(scores.items(), key=lambda x: float(x[1]), reverse=True), 1):\n",
        "        print(f\"{i:2d}. Layer {layer:2d}: {float(score):.6f}\")"
      ],
      "metadata": {
        "id": "KxECy2RMVKaN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_importance= calculate_layer_importance_cosine(model, dataloader1, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw68nEqr76P-",
        "outputId": "8fff865f-162a-45da-a1d0-48d2a5fa5e27"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating cosine-based importance for 28 layers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches:   8%|▊         | 1/13 [00:00<00:06,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0: 8/8 valid samples, avg_similarity=0.1332, importance=0.866821\n",
            "Layer 1: 8/8 valid samples, avg_similarity=0.6338, importance=0.366211\n",
            "Layer 2: 8/8 valid samples, avg_similarity=0.3281, importance=0.671875\n",
            "Layer 3: 8/8 valid samples, avg_similarity=0.9316, importance=0.068359\n",
            "Layer 4: 8/8 valid samples, avg_similarity=0.9204, importance=0.079590\n",
            "Layer 5: 8/8 valid samples, avg_similarity=0.9268, importance=0.073242\n",
            "Layer 6: 8/8 valid samples, avg_similarity=0.9277, importance=0.072266\n",
            "Layer 7: 8/8 valid samples, avg_similarity=0.9492, importance=0.050781\n",
            "Layer 8: 8/8 valid samples, avg_similarity=0.9390, importance=0.061035\n",
            "Layer 9: 8/8 valid samples, avg_similarity=0.9243, importance=0.075684\n",
            "Layer 10: 8/8 valid samples, avg_similarity=0.9336, importance=0.066406\n",
            "Layer 11: 8/8 valid samples, avg_similarity=0.9224, importance=0.077637\n",
            "Layer 12: 8/8 valid samples, avg_similarity=0.9331, importance=0.066895\n",
            "Layer 13: 8/8 valid samples, avg_similarity=0.9263, importance=0.073730\n",
            "Layer 14: 8/8 valid samples, avg_similarity=0.9316, importance=0.068359\n",
            "Layer 15: 8/8 valid samples, avg_similarity=0.9321, importance=0.067871\n",
            "Layer 16: 8/8 valid samples, avg_similarity=0.9229, importance=0.077148\n",
            "Layer 17: 8/8 valid samples, avg_similarity=0.9082, importance=0.091797\n",
            "Layer 18: 8/8 valid samples, avg_similarity=0.9453, importance=0.054688\n",
            "Layer 19: 8/8 valid samples, avg_similarity=0.9224, importance=0.077637\n",
            "Layer 20: 8/8 valid samples, avg_similarity=0.9429, importance=0.057129\n",
            "Layer 21: 8/8 valid samples, avg_similarity=0.9160, importance=0.083984\n",
            "Layer 22: 8/8 valid samples, avg_similarity=0.9033, importance=0.096680\n",
            "Layer 23: 8/8 valid samples, avg_similarity=0.8799, importance=0.120117\n",
            "Layer 24: 8/8 valid samples, avg_similarity=0.8999, importance=0.100098\n",
            "Layer 25: 8/8 valid samples, avg_similarity=0.8931, importance=0.106934\n",
            "Layer 26: 8/8 valid samples, avg_similarity=0.9121, importance=0.087891\n",
            "Layer 27: 8/8 valid samples, avg_similarity=0.8135, importance=0.186523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|██████████| 13/13 [00:05<00:00,  2.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_sorted_importance(wiki_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uebaKFEe8GpS",
        "outputId": "719f9f5f-3b98-4290-918c-5e54ec1a9d1d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. Layer  0: 0.890395\n",
            " 2. Layer  2: 0.771541\n",
            " 3. Layer  1: 0.307580\n",
            " 4. Layer 27: 0.173190\n",
            " 5. Layer 23: 0.082933\n",
            " 6. Layer 25: 0.074669\n",
            " 7. Layer 24: 0.072416\n",
            " 8. Layer 22: 0.069261\n",
            " 9. Layer 26: 0.063664\n",
            "10. Layer 21: 0.062763\n",
            "11. Layer 17: 0.060885\n",
            "12. Layer 19: 0.054763\n",
            "13. Layer 16: 0.051645\n",
            "14. Layer  4: 0.051382\n",
            "15. Layer 11: 0.051194\n",
            "16. Layer  9: 0.049692\n",
            "17. Layer 13: 0.048528\n",
            "18. Layer  5: 0.047476\n",
            "19. Layer  6: 0.047138\n",
            "20. Layer 14: 0.045335\n",
            "21. Layer 15: 0.045335\n",
            "22. Layer  3: 0.044283\n",
            "23. Layer 12: 0.044246\n",
            "24. Layer 10: 0.044171\n",
            "25. Layer 20: 0.042405\n",
            "26. Layer  8: 0.040152\n",
            "27. Layer 18: 0.037861\n",
            "28. Layer  7: 0.033391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sms_importance = calculate_layer_importance_cosine(model, dataloader2, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTywurOkR7Cm",
        "outputId": "ffe287e6-9298-43a4-af76-467542ac7323"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating cosine-based importance for 28 layers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches:   8%|▊         | 1/13 [00:00<00:06,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0: 8/8 valid samples, avg_similarity=0.0981, importance=0.901855\n",
            "Layer 1: 8/8 valid samples, avg_similarity=0.7793, importance=0.220703\n",
            "Layer 2: 8/8 valid samples, avg_similarity=0.0504, importance=0.949585\n",
            "Layer 3: 8/8 valid samples, avg_similarity=0.9985, importance=0.001465\n",
            "Layer 4: 8/8 valid samples, avg_similarity=0.9980, importance=0.001953\n",
            "Layer 5: 8/8 valid samples, avg_similarity=0.9980, importance=0.001953\n",
            "Layer 6: 8/8 valid samples, avg_similarity=0.9976, importance=0.002441\n",
            "Layer 7: 8/8 valid samples, avg_similarity=0.9971, importance=0.002930\n",
            "Layer 8: 8/8 valid samples, avg_similarity=0.9966, importance=0.003418\n",
            "Layer 9: 8/8 valid samples, avg_similarity=0.9961, importance=0.003906\n",
            "Layer 10: 8/8 valid samples, avg_similarity=0.9956, importance=0.004395\n",
            "Layer 11: 8/8 valid samples, avg_similarity=0.9951, importance=0.004883\n",
            "Layer 12: 8/8 valid samples, avg_similarity=0.9961, importance=0.003906\n",
            "Layer 13: 8/8 valid samples, avg_similarity=0.9961, importance=0.003906\n",
            "Layer 14: 8/8 valid samples, avg_similarity=0.9951, importance=0.004883\n",
            "Layer 15: 8/8 valid samples, avg_similarity=0.9946, importance=0.005371\n",
            "Layer 16: 8/8 valid samples, avg_similarity=0.9937, importance=0.006348\n",
            "Layer 17: 8/8 valid samples, avg_similarity=0.9937, importance=0.006348\n",
            "Layer 18: 8/8 valid samples, avg_similarity=0.9912, importance=0.008789\n",
            "Layer 19: 8/8 valid samples, avg_similarity=0.9844, importance=0.015625\n",
            "Layer 20: 8/8 valid samples, avg_similarity=0.9829, importance=0.017090\n",
            "Layer 21: 8/8 valid samples, avg_similarity=0.9736, importance=0.026367\n",
            "Layer 22: 8/8 valid samples, avg_similarity=0.9824, importance=0.017578\n",
            "Layer 23: 8/8 valid samples, avg_similarity=0.9844, importance=0.015625\n",
            "Layer 24: 8/8 valid samples, avg_similarity=0.9761, importance=0.023926\n",
            "Layer 25: 8/8 valid samples, avg_similarity=0.9810, importance=0.019043\n",
            "Layer 26: 8/8 valid samples, avg_similarity=0.9814, importance=0.018555\n",
            "Layer 27: 8/8 valid samples, avg_similarity=0.8525, importance=0.147461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|██████████| 13/13 [00:05<00:00,  2.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_sorted_importance(sms_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPKWuhurSUpe",
        "outputId": "c616af92-02ea-4ae7-da5b-9e1603d05b8b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. Layer  2: 0.948648\n",
            " 2. Layer  0: 0.896963\n",
            " 3. Layer  1: 0.277306\n",
            " 4. Layer 27: 0.147085\n",
            " 5. Layer 21: 0.025203\n",
            " 6. Layer 24: 0.023287\n",
            " 7. Layer 25: 0.020959\n",
            " 8. Layer 26: 0.017353\n",
            " 9. Layer 22: 0.017315\n",
            "10. Layer 20: 0.016752\n",
            "11. Layer 19: 0.015925\n",
            "12. Layer 23: 0.014836\n",
            "13. Layer 18: 0.008977\n",
            "14. Layer 17: 0.006686\n",
            "15. Layer 16: 0.006197\n",
            "16. Layer 15: 0.005334\n",
            "17. Layer 14: 0.005258\n",
            "18. Layer 11: 0.004845\n",
            "19. Layer 10: 0.004770\n",
            "20. Layer 12: 0.004094\n",
            "21. Layer  9: 0.003906\n",
            "22. Layer 13: 0.003906\n",
            "23. Layer  8: 0.003418\n",
            "24. Layer  7: 0.002967\n",
            "25. Layer  6: 0.002817\n",
            "26. Layer  5: 0.002329\n",
            "27. Layer  4: 0.001953\n",
            "28. Layer  3: 0.001465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_importance(scores1, scores2, name1=\"Dataset1\", name2=\"Dataset2\"):\n",
        "    print(f\"{'Layer':<5} {name1:<10} {name2:<10} {'Diff':<8}\")\n",
        "    print(\"-\" * 35)\n",
        "    for layer in sorted(scores1.keys()):\n",
        "        s1, s2 = float(scores1[layer]), float(scores2[layer])\n",
        "        diff = abs(s1 - s2)\n",
        "        print(f\"{layer:<5} {s1:<10.4f} {s2:<10.4f} {diff:<8.4f}\")"
      ],
      "metadata": {
        "id": "7a63I8HvUnIm"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_importance(wiki_importance, sms_importance, \"wiki\", \"SMS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IBQcFl8gyI_",
        "outputId": "bc7be7c0-ddd2-40f6-bf38-26991d7c1453"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer wiki       SMS        Diff    \n",
            "-----------------------------------\n",
            "0     0.8904     0.8970     0.0066  \n",
            "1     0.3076     0.2773     0.0303  \n",
            "2     0.7715     0.9486     0.1771  \n",
            "3     0.0443     0.0015     0.0428  \n",
            "4     0.0514     0.0020     0.0494  \n",
            "5     0.0475     0.0023     0.0451  \n",
            "6     0.0471     0.0028     0.0443  \n",
            "7     0.0334     0.0030     0.0304  \n",
            "8     0.0402     0.0034     0.0367  \n",
            "9     0.0497     0.0039     0.0458  \n",
            "10    0.0442     0.0048     0.0394  \n",
            "11    0.0512     0.0048     0.0463  \n",
            "12    0.0442     0.0041     0.0402  \n",
            "13    0.0485     0.0039     0.0446  \n",
            "14    0.0453     0.0053     0.0401  \n",
            "15    0.0453     0.0053     0.0400  \n",
            "16    0.0516     0.0062     0.0454  \n",
            "17    0.0609     0.0067     0.0542  \n",
            "18    0.0379     0.0090     0.0289  \n",
            "19    0.0548     0.0159     0.0388  \n",
            "20    0.0424     0.0168     0.0257  \n",
            "21    0.0628     0.0252     0.0376  \n",
            "22    0.0693     0.0173     0.0519  \n",
            "23    0.0829     0.0148     0.0681  \n",
            "24    0.0724     0.0233     0.0491  \n",
            "25    0.0747     0.0210     0.0537  \n",
            "26    0.0637     0.0174     0.0463  \n",
            "27    0.1732     0.1471     0.0261  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of our comparison reveal a clear pattern. The first layers (0–2) and the last one (27) prove to be important in both datasets, suggesting that they perform fundamental functions, such as the initial processing of the input and the consolidation of the output.\n",
        "\n",
        "The key difference lies in the behavior of the intermediate layers (roughly 3–26). While in the complex text of Wikitext these layers carry out a measurable job of semantic refinement, in the simple SMS text their contribution is practically null, becoming “passive.” This shows that the importance of a layer varies depending on the complexity of the task, thus validating “depth pruning” as an effective strategy to create more efficient models for specialized tasks.\n"
      ],
      "metadata": {
        "id": "yw3B3bVZia7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the pruned model for the SMS dataset by removing the 4 least relevant layers."
      ],
      "metadata": {
        "id": "9LN1lqRsnh9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos usar la información obtenida para eliminar los bloques Transfomer menos importantes del modelo para ser usado con el Dataset SMS."
      ],
      "metadata": {
        "id": "gGA5DjnSocOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optipfair import prune_model\n",
        "\n",
        "sms_model, stats = prune_model(\n",
        "    model=model,\n",
        "    pruning_type=\"DEPTH\",\n",
        "    layer_indices=[3, 4, 5, 6],\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE0DLlJong2Z",
        "outputId": "48e675c8-91a3-45be-e948-f1045613b860"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing layers: 100%|██████████| 20/20 [00:00<00:00, 193732.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new model has only 24 transformer blocks."
      ],
      "metadata": {
        "id": "TQkngm7uoWbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (stats['percentage_reduction'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoiVSUm8o5XU",
        "outputId": "34e03adc-71f1-4d0c-fbdb-7be010c760dd"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.38227543762604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sms_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvHU8hC3hSzm",
        "outputId": "52be9b43-4b5f-4737-d9a7-e7b88d68b244"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen3ForCausalLM(\n",
            "  (model): Qwen3Model(\n",
            "    (embed_tokens): Embedding(151936, 1024)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): Qwen3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j4eU-ZsjoTok"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}