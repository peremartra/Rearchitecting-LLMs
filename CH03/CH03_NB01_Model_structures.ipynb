{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH03/CH03_NB01_Model_structures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttp4xZYBCZGt"
      },
      "source": [
        "#Rearchitecting LLMs\n",
        "## Structural techniques for efficient models\n",
        "\n",
        "\n",
        "###Â Chapter 3: Transformer anatomy: knowing what you'll optimize\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* distilgpt2\n",
        "* meta-llama/Llama-3.2-1B\n",
        "* google/gemma-3-270m\n",
        "* microsoft/Phi-4-mini-instruct\n",
        "* Llama-3.1-8B-Instruct\n",
        "\n",
        "_____\n",
        "\n",
        "In this notebook you'll find the structure of different transformer models.\n",
        "\n",
        "We'll take a journey from the classic DistilGPT to more current models like Llama-3.2 or Gemma-3, studying optimizations like Grouped Query Attention (GQA) or Gated Linear Units (GLU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgj6Ve8pYR12",
        "outputId": "e85d4012-9557-4dbb-dc44-b97c09784bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/10.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/10.4 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m164.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m164.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "    \"transformers==4.51.3\" \\\n",
        "    \"accelerate==1.3.0\" \\\n",
        "    \"bitsandbytes\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "#import psutil\n",
        "#import os\n",
        "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "#from transformers.models.auto.modeling_auto import MODEL_MAPPING\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3VvovHna5YOH"
      },
      "outputs": [],
      "source": [
        "def clean_memory(model):\n",
        "    \"\"\"Limpia memoria\"\"\"\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L1Ibd3dzYVtE"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=\"auto\", #torch.float16,\n",
        "        device_map=\"cpu\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD5IM9nokFV8"
      },
      "source": [
        "# Chapter architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5efsCb9WQL"
      },
      "source": [
        "## 3.1 Classical architecture: DistilGPT2\n",
        "\n",
        "We've chosen the DistilGPT2 model for being a base model that represents the essence of a transformer model.\n",
        "\n",
        "With it we'll explain how the attention layer works and what major blocks form the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esbMB40EYcUQ"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"distilbert/distilgpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiAjLsOoYv-L",
        "outputId": "c184a5c5-e2d6-4e1a-ee5a-a38ec9ad5ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVb_oF8Z35eU",
        "outputId": "03df2b7e-2b46-446b-ac22-239f61618574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention heads: 12\n",
            "Head dimensions: 64\n"
          ]
        }
      ],
      "source": [
        "config = model.config\n",
        "clean_memory(model)\n",
        "print(f\"Attention heads: {config.n_head}\")\n",
        "print(f\"Head dimensions: {config.n_embd // config.n_head}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU6gOLcvUJOG"
      },
      "source": [
        "Apart from the structure of the model layers we can also check its code. It's worth mentioning that the order of the layers we see when executing `print(model)` doesn't have to be the execution order.\n",
        "\n",
        "The layers appear in the order they have been created in the `__init__()` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alm7JvFnxhnp",
        "outputId": "f4d44995-f3a0-44fd-e4e3-d2c2719e9499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
            "        hidden_states = self.c_fc(hidden_states)\n",
            "        hidden_states = self.act(hidden_states)\n",
            "        hidden_states = self.c_proj(hidden_states)\n",
            "        hidden_states = self.dropout(hidden_states)\n",
            "        return hidden_states\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "mlp_layer = model.transformer.h[0].mlp\n",
        "print(inspect.getsource(mlp_layer.forward))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-ASlxRr-qkT",
        "outputId": "e58dba8e-627d-4646-c62d-76ddaa78c8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def __init__(self, config):\n",
            "        super().__init__(config)\n",
            "\n",
            "        self.embed_dim = config.hidden_size\n",
            "\n",
            "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
            "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
            "\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
            "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
            "\n",
            "        # Model parallel\n",
            "        self.model_parallel = False\n",
            "        self.device_map = None\n",
            "        self.gradient_checkpointing = False\n",
            "        self._attn_implementation = config._attn_implementation\n",
            "\n",
            "        # Initialize weights and apply final processing\n",
            "        self.post_init()\n",
            "\n"
          ]
        }
      ],
      "source": [
        "init_source_code = inspect.getsource(model.transformer.__init__)\n",
        "print(init_source_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjWxPtUp7H_K"
      },
      "source": [
        " But they execute in the order they're called from the `forward()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq1wGfCfU4k5"
      },
      "source": [
        "The execution of the layers is controlled from the different `forward()` methods of the model.\n",
        "\n",
        "Apart from the general `forward`, a model has different `forward` methods for the most important modules, like the `forward` of Attention and the `forward` of MLP.\n",
        "\n",
        "In the cell below we see the general `forward` of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvsLU6qh-50j",
        "outputId": "ab1fe225-993e-422c-ba33-85be0c7b44eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
            "    @add_code_sample_docstrings(\n",
            "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
            "        output_type=BaseModelOutputWithPastAndCrossAttentions,\n",
            "        config_class=_CONFIG_FOR_DOC,\n",
            "    )\n",
            "    def forward(\n",
            "        self,\n",
            "        input_ids: Optional[torch.LongTensor] = None,\n",
            "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
            "        attention_mask: Optional[torch.FloatTensor] = None,\n",
            "        token_type_ids: Optional[torch.LongTensor] = None,\n",
            "        position_ids: Optional[torch.LongTensor] = None,\n",
            "        head_mask: Optional[torch.FloatTensor] = None,\n",
            "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
            "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
            "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
            "        use_cache: Optional[bool] = None,\n",
            "        output_attentions: Optional[bool] = None,\n",
            "        output_hidden_states: Optional[bool] = None,\n",
            "        return_dict: Optional[bool] = None,\n",
            "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
            "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
            "        output_hidden_states = (\n",
            "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
            "        )\n",
            "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
            "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
            "\n",
            "        if input_ids is not None and inputs_embeds is not None:\n",
            "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
            "        elif input_ids is not None:\n",
            "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
            "            input_shape = input_ids.size()\n",
            "            input_ids = input_ids.view(-1, input_shape[-1])\n",
            "            batch_size = input_ids.shape[0]\n",
            "        elif inputs_embeds is not None:\n",
            "            input_shape = inputs_embeds.size()[:-1]\n",
            "            batch_size = inputs_embeds.shape[0]\n",
            "        else:\n",
            "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
            "\n",
            "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
            "\n",
            "        if token_type_ids is not None:\n",
            "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
            "\n",
            "        if past_key_values is None:\n",
            "            past_length = 0\n",
            "            past_key_values = tuple([None] * len(self.h))\n",
            "        else:\n",
            "            past_length = past_key_values[0][0].size(-2)\n",
            "        if position_ids is None:\n",
            "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
            "            position_ids = position_ids.unsqueeze(0)\n",
            "\n",
            "        if inputs_embeds is None:\n",
            "            inputs_embeds = self.wte(input_ids)\n",
            "        position_embeds = self.wpe(position_ids)\n",
            "        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n",
            "\n",
            "        # Attention mask.\n",
            "        _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False and head_mask is None\n",
            "        attention_mask = attention_mask.view(batch_size, -1) if attention_mask is not None else None\n",
            "        if self._attn_implementation == \"flash_attention_2\":\n",
            "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
            "        elif _use_sdpa:\n",
            "            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n",
            "                attention_mask=attention_mask,\n",
            "                input_shape=(batch_size, input_shape[-1]),\n",
            "                inputs_embeds=inputs_embeds,\n",
            "                past_key_values_length=past_length,\n",
            "            )\n",
            "        else:\n",
            "            if attention_mask is not None:\n",
            "                # We create a 3D attention mask from a 2D tensor mask.\n",
            "                # Sizes are [batch_size, 1, 1, to_seq_length]\n",
            "                # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
            "                # this attention mask is more simple than the triangular masking of causal attention\n",
            "                # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
            "                attention_mask = attention_mask[:, None, None, :]\n",
            "\n",
            "                # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
            "                # masked positions, this operation will create a tensor which is 0.0 for\n",
            "                # positions we want to attend and the dtype's smallest value for masked positions.\n",
            "                # Since we are adding it to the raw scores before the softmax, this is\n",
            "                # effectively the same as removing these entirely.\n",
            "                attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
            "                attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
            "\n",
            "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
            "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
            "        if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
            "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
            "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
            "            if encoder_attention_mask is None:\n",
            "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
            "            if _use_sdpa:\n",
            "                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
            "                    mask=encoder_attention_mask, dtype=inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
            "                )\n",
            "            elif not self._attn_implementation == \"flash_attention_2\":\n",
            "                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
            "        else:\n",
            "            encoder_attention_mask = None\n",
            "\n",
            "        # Prepare head mask if needed\n",
            "        # 1.0 in head_mask indicate we keep the head\n",
            "        # attention_probs has shape bsz x n_heads x N x N\n",
            "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
            "        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
            "\n",
            "        if token_type_ids is not None:\n",
            "            token_type_embeds = self.wte(token_type_ids)\n",
            "            hidden_states = hidden_states + token_type_embeds\n",
            "\n",
            "        hidden_states = self.drop(hidden_states)\n",
            "\n",
            "        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)\n",
            "\n",
            "        if self.gradient_checkpointing and self.training:\n",
            "            if use_cache:\n",
            "                logger.warning_once(\n",
            "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
            "                )\n",
            "                use_cache = False\n",
            "\n",
            "        presents = () if use_cache else None\n",
            "        all_self_attentions = () if output_attentions else None\n",
            "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
            "        all_hidden_states = () if output_hidden_states else None\n",
            "        for i in range(len(self.h)):\n",
            "            block, layer_past = self.h[i], past_key_values[i]\n",
            "            # Model parallel\n",
            "            if self.model_parallel:\n",
            "                torch.cuda.set_device(hidden_states.device)\n",
            "                # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
            "                if layer_past is not None:\n",
            "                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
            "                # Ensure that attention_mask is always on the same device as hidden_states\n",
            "                if attention_mask is not None:\n",
            "                    attention_mask = attention_mask.to(hidden_states.device)\n",
            "                if isinstance(head_mask, torch.Tensor):\n",
            "                    head_mask = head_mask.to(hidden_states.device)\n",
            "            if output_hidden_states:\n",
            "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
            "\n",
            "            if self.gradient_checkpointing and self.training:\n",
            "                outputs = self._gradient_checkpointing_func(\n",
            "                    block.__call__,\n",
            "                    hidden_states,\n",
            "                    None,\n",
            "                    attention_mask,\n",
            "                    head_mask[i],\n",
            "                    encoder_hidden_states,\n",
            "                    encoder_attention_mask,\n",
            "                    use_cache,\n",
            "                    output_attentions,\n",
            "                )\n",
            "            else:\n",
            "                outputs = block(\n",
            "                    hidden_states,\n",
            "                    layer_past=layer_past,\n",
            "                    attention_mask=attention_mask,\n",
            "                    head_mask=head_mask[i],\n",
            "                    encoder_hidden_states=encoder_hidden_states,\n",
            "                    encoder_attention_mask=encoder_attention_mask,\n",
            "                    use_cache=use_cache,\n",
            "                    output_attentions=output_attentions,\n",
            "                )\n",
            "\n",
            "            hidden_states = outputs[0]\n",
            "            if use_cache is True:\n",
            "                presents = presents + (outputs[1],)\n",
            "\n",
            "            if output_attentions:\n",
            "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
            "                if self.config.add_cross_attention:\n",
            "                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
            "\n",
            "            # Model Parallel: If it's the last layer for that device, put things on the next device\n",
            "            if self.model_parallel:\n",
            "                for k, v in self.device_map.items():\n",
            "                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
            "                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
            "\n",
            "        hidden_states = self.ln_f(hidden_states)\n",
            "\n",
            "        hidden_states = hidden_states.view(output_shape)\n",
            "        # Add last hidden state\n",
            "        if output_hidden_states:\n",
            "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
            "\n",
            "        if not return_dict:\n",
            "            return tuple(\n",
            "                v\n",
            "                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
            "                if v is not None\n",
            "            )\n",
            "\n",
            "        return BaseModelOutputWithPastAndCrossAttentions(\n",
            "            last_hidden_state=hidden_states,\n",
            "            past_key_values=presents,\n",
            "            hidden_states=all_hidden_states,\n",
            "            attentions=all_self_attentions,\n",
            "            cross_attentions=all_cross_attentions,\n",
            "        )\n",
            "\n"
          ]
        }
      ],
      "source": [
        "forward_source_code = inspect.getsource(model.transformer.forward)\n",
        "print(forward_source_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T6E8SP0ngwb"
      },
      "source": [
        "When showing the forward method of the MLP module we can see how the execution order of the layers doesn't match the one that appears when doing `print(model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLVX-h9Vm3ki",
        "outputId": "54007071-87c7-42ac-bc67-f05d9127b7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
            "        hidden_states = self.c_fc(hidden_states)\n",
            "        hidden_states = self.act(hidden_states)\n",
            "        hidden_states = self.c_proj(hidden_states)\n",
            "        hidden_states = self.dropout(hidden_states)\n",
            "        return hidden_states\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mlp_layer = model.transformer.h[0].mlp\n",
        "print(inspect.getsource(mlp_layer.forward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHlcPB3LkmE4"
      },
      "source": [
        "### DistilGPT2 - Key Insights Mapped to Model Structure.\n",
        "\n",
        "â€¢ **Classical Stacked Architecture**: 6 identical GPT2Block layers\n",
        "  â†’ **Line**: `(0-5): 6 x GPT2Block(`\n",
        "\n",
        "â€¢ **Traditional MLP Design**: Simple linear transformation (768â†’3072â†’768) with 4x expansion ratio\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (c_fc): Conv1D(nf=3072, nx=768)\n",
        "  (c_proj): Conv1D(nf=768, nx=3072)\n",
        "  ```\n",
        "\n",
        "â€¢ **Multi-Head Attention**: Classic attention implementation using Conv1D layers (c_attn projects to 2304 dimensions for Q, K, V)\n",
        "  â†’ **Line**: `(c_attn): Conv1D(nf=2304, nx=768)`\n",
        "\n",
        "â€¢ **Compact Embedding Space**: 768-dimensional hidden states with 50,257 vocabulary tokens\n",
        "  â†’ **Line**: `(wte): Embedding(50257, 768)`\n",
        "\n",
        "â€¢ **Moderate Parameter Count**: ~82M parameters\n",
        "  â†’ **Pattern**: Calculated from embedding dimensions (50257Ã—768) + layer parameters (visible through 768 dimensions throughout)\n",
        "\n",
        "â€¢ **Classical Width Pruning Target**: The MLP's c_fc layer (768â†’3072) represents the traditional approach to width pruning\n",
        "  â†’ **Line**: `(c_fc): Conv1D(nf=3072, nx=768)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywmvDq6S_cWf"
      },
      "source": [
        "## 3.2 Llama-3.2-1B The modern architecture\n",
        "\n",
        "We select Llama-3.2-1B for belonging to the family of models that has popularized the modern structure of Transformers. When analyzing its structure, we'll focus on the two major evolutions compared to the classic architecture: the introduction of MLP blocks with GLU and the optimization of attention with Grouped-Query Attention (GQA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGK6OhUQ_VPX"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"meta-llama/Llama-3.2-1B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdM97jxu_m2d",
        "outputId": "6b0e7f9b-825a-42a1-d503-effdfbced3fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVuvfLRI4NIR",
        "outputId": "a51c822d-8a55-4fdf-b1a5-889248fb7de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention heads: 32\n",
            "Num key values: 8\n",
            "Head dimensions: 64\n"
          ]
        }
      ],
      "source": [
        "config = model.config\n",
        "clean_memory(model)\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Num key values: {config.num_key_value_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQxymwpEoF67",
        "outputId": "3087fa5c-d25b-4196-dfee-c5016faabe02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def forward(self, x):\n",
            "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "        return down_proj\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Inspecting GLU Architecture\n",
        "mlp_module = model.model.layers[0].mlp\n",
        "print(inspect.getsource(mlp_module.forward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGuOhAXRoc1i"
      },
      "source": [
        "In the code above we can see how the Llama-3.2 GLU works.\n",
        "\n",
        "The gate_proj(x) branch activates with SiLU, while up_proj(x) stays linear. Then both are multiplied element-wise and the result passes through down_proj to return to the model width.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0DxePWiuuxf"
      },
      "source": [
        "### Llama-3.2-1B - Key Insights Mapped to Model Structure.\n",
        "\n",
        "â€¢ **Modern GLU Architecture**: MLP uses **gating mechanism** (gate_proj + up_proj â†’ down_proj)\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "  ```\n",
        "\n",
        "â€¢ **Grouped Query Attention (GQA)**: Q projections use full 2048 dimensions while K/V use only 512\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "  (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "  ```\n",
        "\n",
        "â€¢ **Higher Expansion Ratio**: MLP expands from 2048â†’8192 (4x ratio) but with GLU structure\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  ```\n",
        "\n",
        "â€¢ **RMSNorm Normalization**: Uses RMS normalization instead of LayerNorm\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  ```\n",
        "\n",
        "â€¢ **Deeper Architecture**: 16 layers vs DistilGPT2's 6 - more **Depth Pruning opportunities**\n",
        "  â†’ **Line**: `(0-15): 16 x LlamaDecoderLayer(`\n",
        "\n",
        "â€¢ **Rotary Position Embeddings**: Advanced positional encoding that works better for longer sequences\n",
        "  â†’ **Line**: `(rotary_emb): LlamaRotaryEmbedding()`\n",
        "\n",
        "â€¢ **SiLU Activation**: Smooth activation function in MLP that influences how gating works in the GLU structure\n",
        "  â†’ **Line**: `(act_fn): SiLU()`\n",
        "\n",
        "â€¢ **Perfect Width Pruning Candidate**: The dual-path MLP structure (gate_proj + up_proj) makes this architecture ideal for demonstrating advanced neuron selection strategies\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZWsqfLhjHRK"
      },
      "source": [
        "# Exploring Other Notable Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFTnPt59_zIj"
      },
      "source": [
        "## google/gemma-3-270m\n",
        "Gemma-3 is an evolution of the pattern introduced with Llama, confirming that Llama's pattern is a de facto standard in the industry, adopted by other big players like Google.\n",
        "\n",
        "Additionally, it allows us to connect with the practical work from Chapter 2. When observing its structure, we'll see that it's almost identical to Llama's, but we'll focus on the subtle implementation variations, like the use of different activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBXbUiTz_wU7"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"google/gemma-3-270m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUD9qT4D_7zL",
        "outputId": "36f56e59-c693-45d9-98ad-3298d462350e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EuCOkBj5Juv",
        "outputId": "2c70bba3-94a5-4e87-bc6b-ccdfe515b02b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention heads: 4\n",
            "Num key values: 1\n",
            "Head dimensions: 160\n"
          ]
        }
      ],
      "source": [
        "config = model.config\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Num key values: {config.num_key_value_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")\n",
        "clean_memory(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvtCQcYjodiP"
      },
      "source": [
        "**Gemma-3-270M - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "â€¢ **Compact GLU Architecture**: Modern GLU structure (gate_proj + up_proj â†’ down_proj) like Llama but in a much smaller footprint\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
        "  ```\n",
        "\n",
        "â€¢ **Grouped Query Attention with Normalization**: GQA structure (Q: 640â†’1024, K/V: 640â†’256) plus unique **q_norm and k_norm** layers\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
        "  (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
        "  (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
        "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
        "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
        "  ```\n",
        "\n",
        "â€¢ **Ultra-Lightweight Design**: Only 640 hidden dimensions and 270M parameters\n",
        "  â†’ **Pattern**: `in_features=640` appears throughout + `Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)`\n",
        "\n",
        "â€¢ **Conservative Expansion Ratio**: 3.2x MLP expansion (640â†’2048) - more modest than Llama's 4x\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  ```\n",
        "\n",
        "â€¢ **Excessive Normalization**: Four RMSNorm layers per decoder block - demonstrates modern architecture's emphasis on training stability\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  ```\n",
        "\n",
        "â€¢ **Dual Rotary Embeddings**: Both standard and \"local\" rotary embeddings - advanced positional encoding\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (rotary_emb): Gemma3RotaryEmbedding()\n",
        "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
        "  ```\n",
        "\n",
        "â€¢ **GELU-Tanh Activation**: Uses PytorchGELUTanh instead of SiLU - shows activation function diversity\n",
        "  â†’ **Line**: `(act_fn): PytorchGELUTanh()`\n",
        "\n",
        "â€¢ **Scaled Word Embeddings**: `Gemma3TextScaledWordEmbedding` indicates specialized embedding handling\n",
        "  â†’ **Line**: `(embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)`\n",
        "\n",
        "â€¢ **Perfect Educational Model**: Small enough for laptop experimentation but sophisticated enough to demonstrate all modern techniques - ideal for **Depth Pruning** (18 layers) and **Width Pruning** examples\n",
        "  â†’ **Line**: `(0-17): 18 x Gemma3DecoderLayer(` + compact dimensions throughout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nEWChaLBKbE"
      },
      "source": [
        "## microsoft/Phi-4-mini-instruct\n",
        "\n",
        "We include microsoft/Phi-4-mini-instruct as a case study on diversity in architecture design. With this model, we'll observe two fundamental differences compared to the Llama/Gemma pattern: first, its attention mechanism, which returns to the classic Multi-Head Attention (MHA) implementation; and second, its MLP block, which uses an ingenious fused layer that differs from the three-layer design we've seen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwVsSKqRAFdI"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"microsoft/Phi-4-mini-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UipxacyiBNp8",
        "outputId": "55fc19d1-da96-4f0b-9d9a-8d991ee3b1ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(200064, 3072, padding_idx=199999)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): Phi3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfeGU6sH5XA3",
        "outputId": "22a8b101-d28c-435d-80a9-748e7386b47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention heads: 24\n",
            "Num key values: 8\n",
            "Head dimensions: 128\n"
          ]
        }
      ],
      "source": [
        "config = model.config\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Num key values: {config.num_key_value_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")\n",
        "clean_memory(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgSxtb3AkmNZ"
      },
      "source": [
        "**Phi-4-mini-instruct - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "â€¢ **Deep Architecture**: 32 layers, demonstrates **extreme Depth Pruning potential** but requires sophisticated layer selection strategies\n",
        "  â†’ **Line**: `(0-31): 32 x Phi3DecoderLayer(`\n",
        "\n",
        "â€¢ **Fused GLU Implementation**: `gate_up_proj` combines gating and up projection in a single matrix (3072â†’16384), then splits internally\n",
        "  â†’ **Line**: `(gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)`\n",
        "\n",
        "â€¢ **Compact Multi-Head Attention**: Single `qkv_proj` (3072â†’5120) handles all Q, K, V projections\n",
        "  â†’ **Line**: `(qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)`\n",
        "\n",
        "â€¢ **Asymmetric Down Projection**: `down_proj` expects 8192 inputs but `gate_up_proj` outputs 16384 - confirms the internal splitting of the fused projection\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
        "  (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
        "  ```\n",
        "\n",
        "â€¢ **Residual Dropout Layers**: Explicit dropout for attention and MLP residual connections - additional regularization not seen in Llama\n",
        "  â†’ **Lines**:\n",
        "  ```\n",
        "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
        "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
        "  ```\n",
        "\n",
        "â€¢ **Large Vocabulary**: 200K+ tokens vs Llama's 128K - shows how modern models handle larger tokenization schemes\n",
        "  â†’ **Line**: `(embed_tokens): Embedding(200064, 3072, padding_idx=199999)`\n",
        "\n",
        "â€¢ **Zero Dropout Training**: All dropout probabilities set to 0.0 - indicates this is an inference-optimized checkpoint\n",
        "  â†’ **Pattern**: `p=0.0` in both `(resid_attn_dropout)` and `(resid_mlp_dropout)` lines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtrT6XqWyWVc"
      },
      "source": [
        "##Llama-3.1-8B-Instruct quantized.\n",
        "\n",
        "This model has been chosen for two strategic reasons. First, its large size (8B parameters) makes quantization a practical necessity to run it in most environments, allowing us to see how the quantization library replaces its layers. Second, being an -Instruct model, we observe that the architecture is the same as the foundational model and that the ability to follow instructions is added to the model just with fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7A5mKnCB8hy"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q8NIn87zHpC"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW8xlEa4z1Lu"
      },
      "outputs": [],
      "source": [
        "model_instruct_quantized = AutoModelForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ostMLAVu4Nc_",
        "outputId": "c78d0d03-8c82-478a-eea8-c98869dbca8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model_instruct_quantized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnHLAqEB7p13",
        "outputId": "cab08be6-f400-4c64-f192-a2a4e63e3acf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention heads: 24\n",
            "Num key values: 8\n",
            "Head dimensions: 128\n"
          ]
        }
      ],
      "source": [
        "config = model.config\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Num key values: {config.num_key_value_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")\n",
        "clean_memory(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RQnzIE9hNcJ"
      },
      "source": [
        "* The `Linear` layers of the Llama-3.2-1B model have been replaced by `Linear4bit` layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl2cERJGlE-T"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we've inspected five different models to understand their internal components.\n",
        "\n",
        "DistilGPT2 is the one that has the classic structure: simple MLP with 4x expansion and traditional Multi-Head Attention.\n",
        "\n",
        "Llama-3.2-1B introduced the two key innovations of modern models: GLU architecture in the MLP (gate_proj + up_proj â†’ down_proj) and Grouped Query Attention to optimize memory.\n",
        "\n",
        "Gemma-3 confirmed that the Llama pattern has become standard, maintaining the same GLU structure but adding additional normalizations and using GELU-Tanh instead of SiLU as activation in the GLU structure.\n",
        "\n",
        "With Phi-4 we've seen that there's room for variations: its fused GLU implementation (gate_up_proj) is more memory efficient, and its return to classic MHA shows that not all optimizations are universal.\n",
        "\n",
        "The key lesson is that, regardless of variations, all these models share similar components we can modify: stacked layers for depth pruning, MLP projections for width pruning, and attention mechanisms we can bypass dynamically.\n",
        "\n",
        "Understanding these structures is fundamental for the optimization techniques we'll apply in the upcoming chapters."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPPmZZQCNcYe3C0egMW2MB5",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
