{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOazPKXJ0UflQPkEvv46Pp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH03/CH03_NB01_Model_structures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 3: The Internal Structure of Modern Transformers\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* distilgpt2\n",
        "* meta-llama/Llama-3.2-1B\n",
        "* google/gemma-3-270m\n",
        "* microsoft/Phi-4-mini-instruct\n",
        "* Qwen/Qwen3-0.6B\"\n",
        "\n",
        "_____\n",
        "\n",
        "In this notebook you'll find the structure of different transformer models.Retry"
      ],
      "metadata": {
        "id": "ttp4xZYBCZGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vgj6Ve8pYR12"
      },
      "outputs": [],
      "source": [
        "# Análisis de Estructuras de Modelos de Lenguaje\n",
        "# Notebook optimizado para Google Colab con gestión eficiente de memoria\n",
        "\n",
        "# Instalar dependencias necesarias\n",
        "#!pip install transformers torch accelerate sentencepiece\n",
        "\n",
        "!pip install -q \\\n",
        "      \"torch==2.8.0+cu126\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "#import psutil\n",
        "#import os\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "#from transformers.models.auto.modeling_auto import MODEL_MAPPING\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_memory(model):\n",
        "    \"\"\"Limpia memoria\"\"\"\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3VvovHna5YOH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    # Cargar modelo\n",
        "    model = AutoModel.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cpu\",  # Cargar en CPU para ahorrar memoria GPU\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "L1Ibd3dzYVtE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DistilGPT2"
      ],
      "metadata": {
        "id": "Iv5efsCb9WQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesar cada modelo\n",
        "model = load_model(\"DistilGPT2\")"
      ],
      "metadata": {
        "id": "esbMB40EYcUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiAjLsOoYv-L",
        "outputId": "1f578a64-6802-40a2-f75b-ff9036bc6416"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-5): 6 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2Attention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# meta-llama/Llama-3.2-1B"
      ],
      "metadata": {
        "id": "ywmvDq6S_cWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"meta-llama/Llama-3.2-1B\")"
      ],
      "metadata": {
        "id": "kGK6OhUQ_VPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdM97jxu_m2d",
        "outputId": "0d90a00c-a0af-443c-b6a1-9c4cb9375e9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaModel(\n",
            "  (embed_tokens): Embedding(128256, 2048)\n",
            "  (layers): ModuleList(\n",
            "    (0-15): 16 x LlamaDecoderLayer(\n",
            "      (self_attn): LlamaAttention(\n",
            "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "      )\n",
            "      (mlp): LlamaMLP(\n",
            "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        (act_fn): SiLU()\n",
            "      )\n",
            "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    )\n",
            "  )\n",
            "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "  (rotary_emb): LlamaRotaryEmbedding()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# google/gemma-3-270m"
      ],
      "metadata": {
        "id": "lFTnPt59_zIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"google/gemma-3-270m\")"
      ],
      "metadata": {
        "id": "iBXbUiTz_wU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUD9qT4D_7zL",
        "outputId": "c0fda0eb-c239-40de-e930-d19874872977"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma3TextModel(\n",
            "  (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "  (layers): ModuleList(\n",
            "    (0-17): 18 x Gemma3DecoderLayer(\n",
            "      (self_attn): Gemma3Attention(\n",
            "        (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "        (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "      )\n",
            "      (mlp): Gemma3MLP(\n",
            "        (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "        (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "        (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "        (act_fn): PytorchGELUTanh()\n",
            "      )\n",
            "      (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    )\n",
            "  )\n",
            "  (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "  (rotary_emb): Gemma3RotaryEmbedding()\n",
            "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# microsoft/Phi-4-mini-instruct"
      ],
      "metadata": {
        "id": "1nEWChaLBKbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"microsoft/Phi-4-mini-instruct\")"
      ],
      "metadata": {
        "id": "XwVsSKqRAFdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UipxacyiBNp8",
        "outputId": "9638acfd-4f7f-465b-90dc-35cd41adced7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3Model(\n",
            "  (embed_tokens): Embedding(200064, 3072, padding_idx=199999)\n",
            "  (layers): ModuleList(\n",
            "    (0-31): 32 x Phi3DecoderLayer(\n",
            "      (self_attn): Phi3Attention(\n",
            "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)\n",
            "      )\n",
            "      (mlp): Phi3MLP(\n",
            "        (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "        (activation_fn): SiLU()\n",
            "      )\n",
            "      (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "  (rotary_emb): Phi3RotaryEmbedding()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7A5mKnCB8hy"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}