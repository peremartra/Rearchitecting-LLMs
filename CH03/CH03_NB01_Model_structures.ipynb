{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7E02bl6dzd/SvZT4Pgg5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH03/CH03_NB01_Model_structures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 3: The Internal Structure of Modern Transformers\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* distilgpt2\n",
        "* meta-llama/Llama-3.2-1B\n",
        "* google/gemma-3-270m\n",
        "* microsoft/Phi-4-mini-instruct\n",
        "* Qwen/Qwen3-0.6B\"\n",
        "\n",
        "_____\n",
        "\n",
        "In this notebook you'll find the structure of different transformer models.Retry"
      ],
      "metadata": {
        "id": "ttp4xZYBCZGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgj6Ve8pYR12"
      },
      "outputs": [],
      "source": [
        "# Análisis de Estructuras de Modelos de Lenguaje\n",
        "# Notebook optimizado para Google Colab con gestión eficiente de memoria\n",
        "\n",
        "# Instalar dependencias necesarias\n",
        "#!pip install transformers torch accelerate sentencepiece\n",
        "\n",
        "!pip install -q \\\n",
        "      \"torch==2.8.0+cu126\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "#import psutil\n",
        "#import os\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "#from transformers.models.auto.modeling_auto import MODEL_MAPPING\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_memory(model):\n",
        "    \"\"\"Limpia memoria\"\"\"\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3VvovHna5YOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    # Cargar modelo\n",
        "    model = AutoModel.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cpu\",  # Cargar en CPU para ahorrar memoria GPU\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "L1Ibd3dzYVtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DistilGPT2"
      ],
      "metadata": {
        "id": "Iv5efsCb9WQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesar cada modelo\n",
        "model = load_model(\"DistilGPT2\")"
      ],
      "metadata": {
        "id": "esbMB40EYcUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiAjLsOoYv-L",
        "outputId": "1f578a64-6802-40a2-f75b-ff9036bc6416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-5): 6 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2Attention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DistilGPT2 - Key Characteristics:\n",
        "\n",
        "• **Classical Stacked Architecture**: 6 identical GPT2Block layers - perfect example for **Depth Pruning** opportunities (removing entire blocks)\n",
        "\n",
        "• **Traditional MLP Design**: Simple linear transformation (768→3072→768) with 4x expansion ratio - no gating mechanism like modern GLU architectures\n",
        "\n",
        "• **Multi-Head Attention**: Classic attention implementation using Conv1D layers (c_attn projects to 2304 dimensions for Q, K, V)\n",
        "\n",
        "• **Pre-Norm Architecture**: LayerNorm applied before attention (ln_1) and MLP (ln_2), not after - affects how information flows\n",
        "\n",
        "• **Compact Embedding Space**: 768-dimensional hidden states with 50,257 vocabulary tokens\n",
        "\n",
        "• **Moderate Parameter Count**: ~82M parameters - small enough for experimentation but complex enough to demonstrate techniques\n",
        "\n",
        "• **Classical Width Pruning Target**: The MLP's c_fc layer (768→3072) represents the traditional approach to width pruning, though less effective than modern GLU variants"
      ],
      "metadata": {
        "id": "cHlcPB3LkmE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# meta-llama/Llama-3.2-1B"
      ],
      "metadata": {
        "id": "ywmvDq6S_cWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"meta-llama/Llama-3.2-1B\")"
      ],
      "metadata": {
        "id": "kGK6OhUQ_VPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdM97jxu_m2d",
        "outputId": "0d90a00c-a0af-443c-b6a1-9c4cb9375e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaModel(\n",
            "  (embed_tokens): Embedding(128256, 2048)\n",
            "  (layers): ModuleList(\n",
            "    (0-15): 16 x LlamaDecoderLayer(\n",
            "      (self_attn): LlamaAttention(\n",
            "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "      )\n",
            "      (mlp): LlamaMLP(\n",
            "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        (act_fn): SiLU()\n",
            "      )\n",
            "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    )\n",
            "  )\n",
            "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "  (rotary_emb): LlamaRotaryEmbedding()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama-3.2-1B - Key Characteristics:\n",
        "\n",
        "• **Modern GLU Architecture**: MLP uses **gating mechanism** (gate_proj + up_proj → down_proj) - this is where **Width Pruning becomes extremely powerful** and will be our focus in Chapter 5\n",
        "\n",
        "• **Grouped Query Attention (GQA)**: Q projections use full 2048 dimensions while K/V use only 512 - more efficient than Multi-Head Attention but adds implementation complexity\n",
        "\n",
        "• **Higher Expansion Ratio**: MLP expands from 2048→8192 (4x ratio) but with GLU structure, creating massive pruning opportunities in the gate and up projections\n",
        "\n",
        "• **Bias-Free Design**: No bias parameters throughout - cleaner architecture and fewer parameters to manage during pruning\n",
        "\n",
        "• **RMSNorm Normalization**: Uses RMS normalization instead of LayerNorm - more efficient computation and better training stability\n",
        "\n",
        "• **Deeper Architecture**: 16 layers vs DistilGPT2's 6 - more **Depth Pruning opportunities** but requires careful layer selection\n",
        "\n",
        "• **Rotary Position Embeddings**: Advanced positional encoding that works better for longer sequences - affects how attention patterns form\n",
        "\n",
        "• **SiLU Activation**: Smooth activation function in MLP that influences how gating works in the GLU structure\n",
        "\n",
        "• **Perfect Width Pruning Candidate**: The dual-path MLP structure (gate_proj + up_proj) makes this architecture ideal for demonstrating advanced neuron selection strategies"
      ],
      "metadata": {
        "id": "N0DxePWiuuxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# google/gemma-3-270m"
      ],
      "metadata": {
        "id": "lFTnPt59_zIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"google/gemma-3-270m\")"
      ],
      "metadata": {
        "id": "iBXbUiTz_wU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUD9qT4D_7zL",
        "outputId": "c0fda0eb-c239-40de-e930-d19874872977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma3TextModel(\n",
            "  (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "  (layers): ModuleList(\n",
            "    (0-17): 18 x Gemma3DecoderLayer(\n",
            "      (self_attn): Gemma3Attention(\n",
            "        (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "        (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "      )\n",
            "      (mlp): Gemma3MLP(\n",
            "        (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "        (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "        (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "        (act_fn): PytorchGELUTanh()\n",
            "      )\n",
            "      (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    )\n",
            "  )\n",
            "  (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "  (rotary_emb): Gemma3RotaryEmbedding()\n",
            "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma-3-270m - Key Characteristics:\n",
        "\n",
        "• **Compact GLU Architecture**: Modern GLU structure (gate_proj + up_proj → down_proj) like Llama but in a much smaller footprint - ideal for **Width Pruning experimentation** without heavy computational costs\n",
        "\n",
        "• **Grouped Query Attention with Normalization**: GQA structure (Q: 640→1024, K/V: 640→256) plus unique **q_norm and k_norm** layers - shows attention evolution beyond basic GQA\n",
        "\n",
        "• **Ultra-Lightweight Design**: Only 640 hidden dimensions and 270M parameters - perfect for rapid prototyping and educational demonstrations\n",
        "\n",
        "• **Conservative Expansion Ratio**: 3.2x MLP expansion (640→2048) - more modest than Llama's 4x, showing how different architectures balance efficiency vs capacity\n",
        "\n",
        "• **Excessive Normalization**: Four RMSNorm layers per decoder block (input, post-attention, pre-feedforward, post-feedforward) - demonstrates modern architecture's emphasis on training stability\n",
        "\n",
        "• **Dual Rotary Embeddings**: Both standard and \"local\" rotary embeddings - advanced positional encoding for better sequence understanding\n",
        "\n",
        "• **GELU-Tanh Activation**: Uses PytorchGELUTanh instead of SiLU - shows activation function diversity in modern architectures\n",
        "\n",
        "• **Scaled Word Embeddings**: `Gemma3TextScaledWordEmbedding` indicates specialized embedding handling - affects how token representations are initialized\n",
        "\n",
        "• **Perfect Educational Model**: Small enough for laptop experimentation but sophisticated enough to demonstrate all modern techniques - ideal for **Depth Pruning** (18 layers) and **Width Pruning** examples\n"
      ],
      "metadata": {
        "id": "jvtCQcYjodiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# microsoft/Phi-4-mini-instruct"
      ],
      "metadata": {
        "id": "1nEWChaLBKbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"microsoft/Phi-4-mini-instruct\")"
      ],
      "metadata": {
        "id": "XwVsSKqRAFdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UipxacyiBNp8",
        "outputId": "9638acfd-4f7f-465b-90dc-35cd41adced7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3Model(\n",
            "  (embed_tokens): Embedding(200064, 3072, padding_idx=199999)\n",
            "  (layers): ModuleList(\n",
            "    (0-31): 32 x Phi3DecoderLayer(\n",
            "      (self_attn): Phi3Attention(\n",
            "        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "        (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)\n",
            "      )\n",
            "      (mlp): Phi3MLP(\n",
            "        (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "        (activation_fn): SiLU()\n",
            "      )\n",
            "      (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "  (rotary_emb): Phi3RotaryEmbedding()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7A5mKnCB8hy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}