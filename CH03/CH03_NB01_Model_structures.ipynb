{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4j0UkeSJ6ydgbGQC3i/ca",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH03/CH03_NB01_Model_structures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 3: The Internal Structure of Modern Transformers\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* distilgpt2\n",
        "* meta-llama/Llama-3.2-1B\n",
        "* google/gemma-3-270m\n",
        "* microsoft/Phi-4-mini-instruct\n",
        "* Llama-3.1-8B-Instruct\n",
        "\n",
        "_____\n",
        "\n",
        "In this notebook you'll find the structure of different transformer models."
      ],
      "metadata": {
        "id": "ttp4xZYBCZGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vgj6Ve8pYR12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d6d69d53-9d48-4016-a712-1595f076c049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "    \"transformers==4.51.3\" \\\n",
        "    \"accelerate==1.3.0\" \\\n",
        "    \"bitsandbytes\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "#import psutil\n",
        "#import os\n",
        "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "#from transformers.models.auto.modeling_auto import MODEL_MAPPING\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_memory(model):\n",
        "    \"\"\"Limpia memoria\"\"\"\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3VvovHna5YOH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=\"auto\", #torch.float16,\n",
        "        device_map=\"cpu\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "L1Ibd3dzYVtE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter architectures"
      ],
      "metadata": {
        "id": "WD5IM9nokFV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DistilGPT2\n",
        "\n",
        "We've chosen the DistilGPT2 model for being a base model that represents the essence of a transformer model.\n",
        "\n",
        "With it we'll explain how the attention layer works and what major blocks form the models."
      ],
      "metadata": {
        "id": "Iv5efsCb9WQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"DistilGPT2\")"
      ],
      "metadata": {
        "id": "esbMB40EYcUQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yiAjLsOoYv-L",
        "outputId": "8c3538f6-c292-4a65-8fb7-28fa161c23c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DistilGPT2 - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Classical Stacked Architecture**: 6 identical GPT2Block layers\n",
        "  → **Line**: `(0-5): 6 x GPT2Block(`\n",
        "\n",
        "• **Traditional MLP Design**: Simple linear transformation (768→3072→768) with 4x expansion ratio\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (c_fc): Conv1D(nf=3072, nx=768)\n",
        "  (c_proj): Conv1D(nf=768, nx=3072)\n",
        "  ```\n",
        "\n",
        "• **Multi-Head Attention**: Classic attention implementation using Conv1D layers (c_attn projects to 2304 dimensions for Q, K, V)\n",
        "  → **Line**: `(c_attn): Conv1D(nf=2304, nx=768)`\n",
        "\n",
        "• **Pre-Norm Architecture**: LayerNorm applied before attention (ln_1) and MLP (ln_2), not after\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "  ```\n",
        "\n",
        "• **Compact Embedding Space**: 768-dimensional hidden states with 50,257 vocabulary tokens\n",
        "  → **Line**: `(wte): Embedding(50257, 768)`\n",
        "\n",
        "• **Moderate Parameter Count**: ~82M parameters\n",
        "  → **Pattern**: Calculated from embedding dimensions (50257×768) + layer parameters (visible through 768 dimensions throughout)\n",
        "\n",
        "• **Classical Width Pruning Target**: The MLP's c_fc layer (768→3072) represents the traditional approach to width pruning\n",
        "  → **Line**: `(c_fc): Conv1D(nf=3072, nx=768)`"
      ],
      "metadata": {
        "id": "cHlcPB3LkmE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## meta-llama/Llama-3.2-1B\n",
        "\n",
        "We select Llama-3.2-1B for belonging to the family of models that has popularized the modern structure of Transformers. When analyzing its structure, we'll focus on the two major evolutions compared to the classic architecture: the introduction of MLP blocks with GLU and the optimization of attention with Grouped-Query Attention (GQA)."
      ],
      "metadata": {
        "id": "ywmvDq6S_cWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"meta-llama/Llama-3.2-1B\")"
      ],
      "metadata": {
        "id": "kGK6OhUQ_VPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MdM97jxu_m2d",
        "outputId": "9e46bf88-5b6c-4ec6-8564-8f392fcb56e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Llama-3.2-1B - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Modern GLU Architecture**: MLP uses **gating mechanism** (gate_proj + up_proj → down_proj)\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Grouped Query Attention (GQA)**: Q projections use full 2048 dimensions while K/V use only 512\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "  (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Higher Expansion Ratio**: MLP expands from 2048→8192 (4x ratio) but with GLU structure\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Bias-Free Design**: No bias parameters throughout - cleaner architecture\n",
        "  → **Pattern**: `bias=False` appears in all Linear layers throughout the model\n",
        "\n",
        "• **RMSNorm Normalization**: Uses RMS normalization instead of LayerNorm\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  ```\n",
        "\n",
        "• **Deeper Architecture**: 16 layers vs DistilGPT2's 6 - more **Depth Pruning opportunities**\n",
        "  → **Line**: `(0-15): 16 x LlamaDecoderLayer(`\n",
        "\n",
        "• **Rotary Position Embeddings**: Advanced positional encoding that works better for longer sequences\n",
        "  → **Line**: `(rotary_emb): LlamaRotaryEmbedding()`\n",
        "\n",
        "• **SiLU Activation**: Smooth activation function in MLP that influences how gating works in the GLU structure\n",
        "  → **Line**: `(act_fn): SiLU()`\n",
        "\n",
        "• **Perfect Width Pruning Candidate**: The dual-path MLP structure (gate_proj + up_proj) makes this architecture ideal for demonstrating advanced neuron selection strategies\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  ```"
      ],
      "metadata": {
        "id": "N0DxePWiuuxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/gemma-3-270m\n",
        "Gemma-3 is an evolution of the pattern introduced with Llama, confirming that Llama's pattern is a de facto standard in the industry, adopted by other big players like Google.\n",
        "\n",
        "Additionally, it allows us to connect with the practical work from Chapter 2. When observing its structure, we'll see that it's almost identical to Llama's, but we'll focus on the subtle implementation variations, like the use of different activation functions."
      ],
      "metadata": {
        "id": "lFTnPt59_zIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"google/gemma-3-270m\")"
      ],
      "metadata": {
        "id": "iBXbUiTz_wU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sUD9qT4D_7zL",
        "outputId": "4a76773e-a2ab-4394-bd1b-89caff215d83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma-3-270M - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Compact GLU Architecture**: Modern GLU structure (gate_proj + up_proj → down_proj) like Llama but in a much smaller footprint\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Grouped Query Attention with Normalization**: GQA structure (Q: 640→1024, K/V: 640→256) plus unique **q_norm and k_norm** layers\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
        "  (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
        "  (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
        "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
        "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
        "  ```\n",
        "\n",
        "• **Ultra-Lightweight Design**: Only 640 hidden dimensions and 270M parameters\n",
        "  → **Pattern**: `in_features=640` appears throughout + `Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)`\n",
        "\n",
        "• **Conservative Expansion Ratio**: 3.2x MLP expansion (640→2048) - more modest than Llama's 4x\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Excessive Normalization**: Four RMSNorm layers per decoder block - demonstrates modern architecture's emphasis on training stability\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  ```\n",
        "\n",
        "• **Dual Rotary Embeddings**: Both standard and \"local\" rotary embeddings - advanced positional encoding\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (rotary_emb): Gemma3RotaryEmbedding()\n",
        "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
        "  ```\n",
        "\n",
        "• **GELU-Tanh Activation**: Uses PytorchGELUTanh instead of SiLU - shows activation function diversity\n",
        "  → **Line**: `(act_fn): PytorchGELUTanh()`\n",
        "\n",
        "• **Scaled Word Embeddings**: `Gemma3TextScaledWordEmbedding` indicates specialized embedding handling\n",
        "  → **Line**: `(embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)`\n",
        "\n",
        "• **Perfect Educational Model**: Small enough for laptop experimentation but sophisticated enough to demonstrate all modern techniques - ideal for **Depth Pruning** (18 layers) and **Width Pruning** examples\n",
        "  → **Line**: `(0-17): 18 x Gemma3DecoderLayer(` + compact dimensions throughout\n"
      ],
      "metadata": {
        "id": "jvtCQcYjodiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Other Notable Architectures"
      ],
      "metadata": {
        "id": "cZWsqfLhjHRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## microsoft/Phi-4-mini-instruct\n",
        "\n",
        "We include microsoft/Phi-4-mini-instruct as a case study on diversity in architecture design. With this model, we'll observe two fundamental differences compared to the Llama/Gemma pattern: first, its attention mechanism, which returns to the classic Multi-Head Attention (MHA) implementation; and second, its MLP block, which uses an ingenious fused layer that differs from the three-layer design we've seen."
      ],
      "metadata": {
        "id": "1nEWChaLBKbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"microsoft/Phi-4-mini-instruct\")"
      ],
      "metadata": {
        "id": "XwVsSKqRAFdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "clean_memory(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UipxacyiBNp8",
        "outputId": "b9158cbc-23dc-46a7-b39c-075cfb851de3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(200064, 3072, padding_idx=199999)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): Phi3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phi-4-mini-instruct - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Deep Architecture**: 32 layers, demonstrates **extreme Depth Pruning potential** but requires sophisticated layer selection strategies\n",
        "  → **Line**: `(0-31): 32 x Phi3DecoderLayer(`\n",
        "\n",
        "• **Fused GLU Implementation**: `gate_up_proj` combines gating and up projection in a single matrix (3072→16384), then splits internally\n",
        "  → **Line**: `(gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)`\n",
        "\n",
        "• **Compact Multi-Head Attention**: Single `qkv_proj` (3072→5120) handles all Q, K, V projections\n",
        "  → **Line**: `(qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)`\n",
        "\n",
        "• **Asymmetric Down Projection**: `down_proj` expects 8192 inputs but `gate_up_proj` outputs 16384 - confirms the internal splitting of the fused projection\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
        "  (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Residual Dropout Layers**: Explicit dropout for attention and MLP residual connections - additional regularization not seen in Llama\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
        "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
        "  ```\n",
        "\n",
        "• **Large Vocabulary**: 200K+ tokens vs Llama's 128K - shows how modern models handle larger tokenization schemes\n",
        "  → **Line**: `(embed_tokens): Embedding(200064, 3072, padding_idx=199999)`\n",
        "\n",
        "• **Zero Dropout Training**: All dropout probabilities set to 0.0 - indicates this is an inference-optimized checkpoint\n",
        "  → **Pattern**: `p=0.0` in both `(resid_attn_dropout)` and `(resid_mlp_dropout)` lines\n"
      ],
      "metadata": {
        "id": "zgSxtb3AkmNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Llama-3.1-8B-Instruct quantized.\n",
        "\n",
        "This model has been chosen for two strategic reasons. First, its large size (8B parameters) makes quantization a practical necessity to run it in most environments, allowing us to see how the quantization library replaces its layers. Second, being an -Instruct model, we observe that the architecture is the same as the foundational model and that the ability to follow instructions is added to the model just with fine-tuning."
      ],
      "metadata": {
        "id": "xtrT6XqWyWVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "R7A5mKnCB8hy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ],
      "metadata": {
        "id": "7Q8NIn87zHpC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_instruct_quantized = AutoModelForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )"
      ],
      "metadata": {
        "id": "JW8xlEa4z1Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_instruct_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ostMLAVu4Nc_",
        "outputId": "cfedc769-ebd6-43c1-bbba-47fe4a2f63b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `Linear` layers of the Llama-3.2-1B model have been replaced by `Linear4bit` layers."
      ],
      "metadata": {
        "id": "-RQnzIE9hNcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFB_jo2Nzczo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we've inspected five different models to understand their internal components.\n",
        "\n",
        "DistilGPT2 is the one that has the classic structure: simple MLP with 4x expansion and traditional Multi-Head Attention.\n",
        "\n",
        "Llama-3.2-1B introduced the two key innovations of modern models: GLU architecture in the MLP (gate_proj + up_proj → down_proj) and Grouped Query Attention to optimize memory.\n",
        "\n",
        "Gemma-3 confirmed that the Llama pattern has become standard, maintaining the same GLU structure but adding additional normalizations and using GELU-Tanh instead of SiLU as activation in the GLU structure.\n",
        "\n",
        "With Phi-4 we've seen that there's room for variations: its fused GLU implementation (gate_up_proj) is more memory efficient, and its return to classic MHA shows that not all optimizations are universal.\n",
        "\n",
        "The key lesson is that, regardless of variations, all these models share similar components we can modify: stacked layers for depth pruning, MLP projections for width pruning, and attention mechanisms we can bypass dynamically.\n",
        "\n",
        "Understanding these structures is fundamental for the optimization techniques we'll apply in the upcoming chapters."
      ],
      "metadata": {
        "id": "yl2cERJGlE-T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAbNcDMZlIBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}