{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOp0J9CLhdBJ+lyp18MCbmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d92d82301db64c6f8b56bf2eff5e5ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3b88c85179b401b89da4a710ea92b0d",
              "IPY_MODEL_85b750975f24427db5404327482af8db",
              "IPY_MODEL_9cd375b369cf4ef3bd641eba9dc27566"
            ],
            "layout": "IPY_MODEL_9635d0fa01c345f99c9b5013049438bc"
          }
        },
        "b3b88c85179b401b89da4a710ea92b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b795a336ef147559a1592508b618d16",
            "placeholder": "​",
            "style": "IPY_MODEL_8691fb91e8cf4e9a9f6bd33c56507f5e",
            "value": "config.json: 100%"
          }
        },
        "85b750975f24427db5404327482af8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_575ab8e62a2841eab7fc065d7345b634",
            "max": 762,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0aa308391fc45bbacd7d2ed29f94795",
            "value": 762
          }
        },
        "9cd375b369cf4ef3bd641eba9dc27566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7083b58bd914a338d3311f1b8519b1d",
            "placeholder": "​",
            "style": "IPY_MODEL_963d17aa7a754122a976a0c63f0f7a5b",
            "value": " 762/762 [00:00&lt;00:00, 82.2kB/s]"
          }
        },
        "9635d0fa01c345f99c9b5013049438bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b795a336ef147559a1592508b618d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8691fb91e8cf4e9a9f6bd33c56507f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575ab8e62a2841eab7fc065d7345b634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0aa308391fc45bbacd7d2ed29f94795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7083b58bd914a338d3311f1b8519b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963d17aa7a754122a976a0c63f0f7a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "754505a8bbaa4ac7860a2e8ed62b1c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0aeff77e8fd14148a9f745983685af60",
              "IPY_MODEL_7c904097fe6e44628cebac253c8c4034",
              "IPY_MODEL_c5b7a28e1a844e3ab54a1abd10e8767a"
            ],
            "layout": "IPY_MODEL_c02073333bcf4996ad3c9a85bbff6372"
          }
        },
        "0aeff77e8fd14148a9f745983685af60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b7d7e16e441425cbc48be1d8fc244e1",
            "placeholder": "​",
            "style": "IPY_MODEL_0cf8b6c8baaf418e8382ab40e2499a83",
            "value": "model.safetensors: 100%"
          }
        },
        "7c904097fe6e44628cebac253c8c4034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d23ab7ac3d481c8a72a6024cfd16c4",
            "max": 352824413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cd5be25b58f47d49af977e3a93ee6f4",
            "value": 352824413
          }
        },
        "c5b7a28e1a844e3ab54a1abd10e8767a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64aa56c8b9874acab8d75784f28c561b",
            "placeholder": "​",
            "style": "IPY_MODEL_547d0fdef8cf4aee9a2ddfd550c05458",
            "value": " 353M/353M [00:03&lt;00:00, 134MB/s]"
          }
        },
        "c02073333bcf4996ad3c9a85bbff6372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7d7e16e441425cbc48be1d8fc244e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cf8b6c8baaf418e8382ab40e2499a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63d23ab7ac3d481c8a72a6024cfd16c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd5be25b58f47d49af977e3a93ee6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64aa56c8b9874acab8d75784f28c561b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547d0fdef8cf4aee9a2ddfd550c05458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "622e1de9b95d45e5b6d32577e2f37892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e3dce102f9241b89b8dbe3e606fd6ae",
              "IPY_MODEL_8975fdfb2cec4046855ce0f474c7d4e6",
              "IPY_MODEL_5fedcae4856f49f2bd1329a267b23c16"
            ],
            "layout": "IPY_MODEL_e822d3a7abbf4166b4af8f9e261ddf9d"
          }
        },
        "7e3dce102f9241b89b8dbe3e606fd6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a2042745bcf4b6e942201c1457974e4",
            "placeholder": "​",
            "style": "IPY_MODEL_d80d0494f72e44a2bdab6daa50f2b2b5",
            "value": "generation_config.json: 100%"
          }
        },
        "8975fdfb2cec4046855ce0f474c7d4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16fd371d7886445895b6960d7408e8aa",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4397568a7db8414ebdebc2d80a008e47",
            "value": 124
          }
        },
        "5fedcae4856f49f2bd1329a267b23c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a114268b4e84e5e8ca90c6d74bed5b9",
            "placeholder": "​",
            "style": "IPY_MODEL_10992c7af978445099fe83e58ce62465",
            "value": " 124/124 [00:00&lt;00:00, 14.2kB/s]"
          }
        },
        "e822d3a7abbf4166b4af8f9e261ddf9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2042745bcf4b6e942201c1457974e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80d0494f72e44a2bdab6daa50f2b2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16fd371d7886445895b6960d7408e8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4397568a7db8414ebdebc2d80a008e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a114268b4e84e5e8ca90c6d74bed5b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10992c7af978445099fe83e58ce62465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH03/CH03_NB01_Model_structures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 3: The Internal Structure of Modern Transformers\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* distilgpt2\n",
        "* meta-llama/Llama-3.2-1B\n",
        "* google/gemma-3-270m\n",
        "* microsoft/Phi-4-mini-instruct\n",
        "* Llama-3.1-8B-Instruct\n",
        "\n",
        "_____\n",
        "\n",
        "In this notebook you'll find the structure of different transformer models."
      ],
      "metadata": {
        "id": "ttp4xZYBCZGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vgj6Ve8pYR12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663a53c8-6ec0-487d-ecc6-a93330046531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "    \"transformers==4.51.3\" \\\n",
        "    \"accelerate==1.3.0\" \\\n",
        "    \"bitsandbytes\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "#import psutil\n",
        "#import os\n",
        "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "#from transformers.models.auto.modeling_auto import MODEL_MAPPING\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_memory(model):\n",
        "    \"\"\"Limpia memoria\"\"\"\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3VvovHna5YOH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=\"auto\", #torch.float16,\n",
        "        device_map=\"cpu\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "L1Ibd3dzYVtE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter architectures"
      ],
      "metadata": {
        "id": "WD5IM9nokFV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DistilGPT2\n",
        "\n",
        "We've chosen the DistilGPT2 model for being a base model that represents the essence of a transformer model.\n",
        "\n",
        "With it we'll explain how the attention layer works and what major blocks form the models."
      ],
      "metadata": {
        "id": "Iv5efsCb9WQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"DistilGPT2\")"
      ],
      "metadata": {
        "id": "esbMB40EYcUQ",
        "outputId": "64b8c118-0904-407e-bd61-93c70b7a1fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "d92d82301db64c6f8b56bf2eff5e5ef5",
            "b3b88c85179b401b89da4a710ea92b0d",
            "85b750975f24427db5404327482af8db",
            "9cd375b369cf4ef3bd641eba9dc27566",
            "9635d0fa01c345f99c9b5013049438bc",
            "0b795a336ef147559a1592508b618d16",
            "8691fb91e8cf4e9a9f6bd33c56507f5e",
            "575ab8e62a2841eab7fc065d7345b634",
            "b0aa308391fc45bbacd7d2ed29f94795",
            "e7083b58bd914a338d3311f1b8519b1d",
            "963d17aa7a754122a976a0c63f0f7a5b",
            "754505a8bbaa4ac7860a2e8ed62b1c07",
            "0aeff77e8fd14148a9f745983685af60",
            "7c904097fe6e44628cebac253c8c4034",
            "c5b7a28e1a844e3ab54a1abd10e8767a",
            "c02073333bcf4996ad3c9a85bbff6372",
            "5b7d7e16e441425cbc48be1d8fc244e1",
            "0cf8b6c8baaf418e8382ab40e2499a83",
            "63d23ab7ac3d481c8a72a6024cfd16c4",
            "8cd5be25b58f47d49af977e3a93ee6f4",
            "64aa56c8b9874acab8d75784f28c561b",
            "547d0fdef8cf4aee9a2ddfd550c05458",
            "622e1de9b95d45e5b6d32577e2f37892",
            "7e3dce102f9241b89b8dbe3e606fd6ae",
            "8975fdfb2cec4046855ce0f474c7d4e6",
            "5fedcae4856f49f2bd1329a267b23c16",
            "e822d3a7abbf4166b4af8f9e261ddf9d",
            "2a2042745bcf4b6e942201c1457974e4",
            "d80d0494f72e44a2bdab6daa50f2b2b5",
            "16fd371d7886445895b6960d7408e8aa",
            "4397568a7db8414ebdebc2d80a008e47",
            "0a114268b4e84e5e8ca90c6d74bed5b9",
            "10992c7af978445099fe83e58ce62465"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d92d82301db64c6f8b56bf2eff5e5ef5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "754505a8bbaa4ac7860a2e8ed62b1c07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "622e1de9b95d45e5b6d32577e2f37892"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiAjLsOoYv-L",
        "outputId": "d797ad86-9cb4-4bb8-94cc-2448fa0ff827"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "clean_memory(model)\n",
        "print(f\"Attention heads: {config.n_head}\")\n",
        "print(f\"Head dimensions: {config.n_embd // config.n_head}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVb_oF8Z35eU",
        "outputId": "65d1c954-cd71-4e02-95ef-26f284259d0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention heads: 12\n",
            "Head dimensions: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "mlp_layer = model.transformer.h[0].mlp\n",
        "print(inspect.getsource(mlp_layer.forward))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alm7JvFnxhnp",
        "outputId": "866bf382-f915-4352-c37a-ca30dda922de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
            "        hidden_states = self.c_fc(hidden_states)\n",
            "        hidden_states = self.act(hidden_states)\n",
            "        hidden_states = self.c_proj(hidden_states)\n",
            "        hidden_states = self.dropout(hidden_states)\n",
            "        return hidden_states\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_source_code = inspect.getsource(model.transformer.__init__)\n",
        "print(init_source_code)"
      ],
      "metadata": {
        "id": "c-ASlxRr-qkT",
        "outputId": "91dcdac4-5f37-4c48-ea98-b0cd3d4ef7d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def __init__(self, config):\n",
            "        super().__init__(config)\n",
            "\n",
            "        self.embed_dim = config.hidden_size\n",
            "\n",
            "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
            "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
            "\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
            "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
            "\n",
            "        # Model parallel\n",
            "        self.model_parallel = False\n",
            "        self.device_map = None\n",
            "        self.gradient_checkpointing = False\n",
            "        self._attn_implementation = config._attn_implementation\n",
            "\n",
            "        # Initialize weights and apply final processing\n",
            "        self.post_init()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forward_source_code = inspect.getsource(model.transformer.forward)\n",
        "print(forward_source_code)"
      ],
      "metadata": {
        "id": "RvsLU6qh-50j",
        "outputId": "691ed6a5-6858-430e-c30b-03467b33c484",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
            "    @add_code_sample_docstrings(\n",
            "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
            "        output_type=BaseModelOutputWithPastAndCrossAttentions,\n",
            "        config_class=_CONFIG_FOR_DOC,\n",
            "    )\n",
            "    def forward(\n",
            "        self,\n",
            "        input_ids: Optional[torch.LongTensor] = None,\n",
            "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
            "        attention_mask: Optional[torch.FloatTensor] = None,\n",
            "        token_type_ids: Optional[torch.LongTensor] = None,\n",
            "        position_ids: Optional[torch.LongTensor] = None,\n",
            "        head_mask: Optional[torch.FloatTensor] = None,\n",
            "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
            "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
            "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
            "        use_cache: Optional[bool] = None,\n",
            "        output_attentions: Optional[bool] = None,\n",
            "        output_hidden_states: Optional[bool] = None,\n",
            "        return_dict: Optional[bool] = None,\n",
            "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
            "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
            "        output_hidden_states = (\n",
            "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
            "        )\n",
            "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
            "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
            "\n",
            "        if input_ids is not None and inputs_embeds is not None:\n",
            "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
            "        elif input_ids is not None:\n",
            "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
            "            input_shape = input_ids.size()\n",
            "            input_ids = input_ids.view(-1, input_shape[-1])\n",
            "            batch_size = input_ids.shape[0]\n",
            "        elif inputs_embeds is not None:\n",
            "            input_shape = inputs_embeds.size()[:-1]\n",
            "            batch_size = inputs_embeds.shape[0]\n",
            "        else:\n",
            "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
            "\n",
            "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
            "\n",
            "        if token_type_ids is not None:\n",
            "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
            "\n",
            "        if past_key_values is None:\n",
            "            past_length = 0\n",
            "            past_key_values = tuple([None] * len(self.h))\n",
            "        else:\n",
            "            past_length = past_key_values[0][0].size(-2)\n",
            "        if position_ids is None:\n",
            "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
            "            position_ids = position_ids.unsqueeze(0)\n",
            "\n",
            "        if inputs_embeds is None:\n",
            "            inputs_embeds = self.wte(input_ids)\n",
            "        position_embeds = self.wpe(position_ids)\n",
            "        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n",
            "\n",
            "        # Attention mask.\n",
            "        _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False and head_mask is None\n",
            "        attention_mask = attention_mask.view(batch_size, -1) if attention_mask is not None else None\n",
            "        if self._attn_implementation == \"flash_attention_2\":\n",
            "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
            "        elif _use_sdpa:\n",
            "            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n",
            "                attention_mask=attention_mask,\n",
            "                input_shape=(batch_size, input_shape[-1]),\n",
            "                inputs_embeds=inputs_embeds,\n",
            "                past_key_values_length=past_length,\n",
            "            )\n",
            "        else:\n",
            "            if attention_mask is not None:\n",
            "                # We create a 3D attention mask from a 2D tensor mask.\n",
            "                # Sizes are [batch_size, 1, 1, to_seq_length]\n",
            "                # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
            "                # this attention mask is more simple than the triangular masking of causal attention\n",
            "                # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
            "                attention_mask = attention_mask[:, None, None, :]\n",
            "\n",
            "                # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
            "                # masked positions, this operation will create a tensor which is 0.0 for\n",
            "                # positions we want to attend and the dtype's smallest value for masked positions.\n",
            "                # Since we are adding it to the raw scores before the softmax, this is\n",
            "                # effectively the same as removing these entirely.\n",
            "                attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
            "                attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
            "\n",
            "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
            "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
            "        if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
            "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
            "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
            "            if encoder_attention_mask is None:\n",
            "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
            "            if _use_sdpa:\n",
            "                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
            "                    mask=encoder_attention_mask, dtype=inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
            "                )\n",
            "            elif not self._attn_implementation == \"flash_attention_2\":\n",
            "                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
            "        else:\n",
            "            encoder_attention_mask = None\n",
            "\n",
            "        # Prepare head mask if needed\n",
            "        # 1.0 in head_mask indicate we keep the head\n",
            "        # attention_probs has shape bsz x n_heads x N x N\n",
            "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
            "        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
            "\n",
            "        if token_type_ids is not None:\n",
            "            token_type_embeds = self.wte(token_type_ids)\n",
            "            hidden_states = hidden_states + token_type_embeds\n",
            "\n",
            "        hidden_states = self.drop(hidden_states)\n",
            "\n",
            "        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)\n",
            "\n",
            "        if self.gradient_checkpointing and self.training:\n",
            "            if use_cache:\n",
            "                logger.warning_once(\n",
            "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
            "                )\n",
            "                use_cache = False\n",
            "\n",
            "        presents = () if use_cache else None\n",
            "        all_self_attentions = () if output_attentions else None\n",
            "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
            "        all_hidden_states = () if output_hidden_states else None\n",
            "        for i in range(len(self.h)):\n",
            "            block, layer_past = self.h[i], past_key_values[i]\n",
            "            # Model parallel\n",
            "            if self.model_parallel:\n",
            "                torch.cuda.set_device(hidden_states.device)\n",
            "                # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
            "                if layer_past is not None:\n",
            "                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
            "                # Ensure that attention_mask is always on the same device as hidden_states\n",
            "                if attention_mask is not None:\n",
            "                    attention_mask = attention_mask.to(hidden_states.device)\n",
            "                if isinstance(head_mask, torch.Tensor):\n",
            "                    head_mask = head_mask.to(hidden_states.device)\n",
            "            if output_hidden_states:\n",
            "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
            "\n",
            "            if self.gradient_checkpointing and self.training:\n",
            "                outputs = self._gradient_checkpointing_func(\n",
            "                    block.__call__,\n",
            "                    hidden_states,\n",
            "                    None,\n",
            "                    attention_mask,\n",
            "                    head_mask[i],\n",
            "                    encoder_hidden_states,\n",
            "                    encoder_attention_mask,\n",
            "                    use_cache,\n",
            "                    output_attentions,\n",
            "                )\n",
            "            else:\n",
            "                outputs = block(\n",
            "                    hidden_states,\n",
            "                    layer_past=layer_past,\n",
            "                    attention_mask=attention_mask,\n",
            "                    head_mask=head_mask[i],\n",
            "                    encoder_hidden_states=encoder_hidden_states,\n",
            "                    encoder_attention_mask=encoder_attention_mask,\n",
            "                    use_cache=use_cache,\n",
            "                    output_attentions=output_attentions,\n",
            "                )\n",
            "\n",
            "            hidden_states = outputs[0]\n",
            "            if use_cache is True:\n",
            "                presents = presents + (outputs[1],)\n",
            "\n",
            "            if output_attentions:\n",
            "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
            "                if self.config.add_cross_attention:\n",
            "                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
            "\n",
            "            # Model Parallel: If it's the last layer for that device, put things on the next device\n",
            "            if self.model_parallel:\n",
            "                for k, v in self.device_map.items():\n",
            "                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
            "                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
            "\n",
            "        hidden_states = self.ln_f(hidden_states)\n",
            "\n",
            "        hidden_states = hidden_states.view(output_shape)\n",
            "        # Add last hidden state\n",
            "        if output_hidden_states:\n",
            "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
            "\n",
            "        if not return_dict:\n",
            "            return tuple(\n",
            "                v\n",
            "                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
            "                if v is not None\n",
            "            )\n",
            "\n",
            "        return BaseModelOutputWithPastAndCrossAttentions(\n",
            "            last_hidden_state=hidden_states,\n",
            "            past_key_values=presents,\n",
            "            hidden_states=all_hidden_states,\n",
            "            attentions=all_self_attentions,\n",
            "            cross_attentions=all_cross_attentions,\n",
            "        )\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DistilGPT2 - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Classical Stacked Architecture**: 6 identical GPT2Block layers\n",
        "  → **Line**: `(0-5): 6 x GPT2Block(`\n",
        "\n",
        "• **Traditional MLP Design**: Simple linear transformation (768→3072→768) with 4x expansion ratio\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (c_fc): Conv1D(nf=3072, nx=768)\n",
        "  (c_proj): Conv1D(nf=768, nx=3072)\n",
        "  ```\n",
        "\n",
        "• **Multi-Head Attention**: Classic attention implementation using Conv1D layers (c_attn projects to 2304 dimensions for Q, K, V)\n",
        "  → **Line**: `(c_attn): Conv1D(nf=2304, nx=768)`\n",
        "\n",
        "• **Pre-Norm Architecture**: LayerNorm applied before attention (ln_1) and MLP (ln_2), not after\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "  ```\n",
        "\n",
        "• **Compact Embedding Space**: 768-dimensional hidden states with 50,257 vocabulary tokens\n",
        "  → **Line**: `(wte): Embedding(50257, 768)`\n",
        "\n",
        "• **Moderate Parameter Count**: ~82M parameters\n",
        "  → **Pattern**: Calculated from embedding dimensions (50257×768) + layer parameters (visible through 768 dimensions throughout)\n",
        "\n",
        "• **Classical Width Pruning Target**: The MLP's c_fc layer (768→3072) represents the traditional approach to width pruning\n",
        "  → **Line**: `(c_fc): Conv1D(nf=3072, nx=768)`"
      ],
      "metadata": {
        "id": "cHlcPB3LkmE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## meta-llama/Llama-3.2-1B\n",
        "\n",
        "We select Llama-3.2-1B for belonging to the family of models that has popularized the modern structure of Transformers. When analyzing its structure, we'll focus on the two major evolutions compared to the classic architecture: the introduction of MLP blocks with GLU and the optimization of attention with Grouped-Query Attention (GQA)."
      ],
      "metadata": {
        "id": "ywmvDq6S_cWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"meta-llama/Llama-3.2-1B\")"
      ],
      "metadata": {
        "id": "kGK6OhUQ_VPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdM97jxu_m2d",
        "outputId": "9c03ca60-0763-4735-983b-11757de2b219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "clean_memory(model)\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")"
      ],
      "metadata": {
        "outputId": "cf923922-89fd-4547-d363-1b5f50d8105e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVuvfLRI4NIR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention heads: 32\n",
            "Head dimensions: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Llama-3.2-1B - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Modern GLU Architecture**: MLP uses **gating mechanism** (gate_proj + up_proj → down_proj)\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Grouped Query Attention (GQA)**: Q projections use full 2048 dimensions while K/V use only 512\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "  (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Higher Expansion Ratio**: MLP expands from 2048→8192 (4x ratio) but with GLU structure\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Bias-Free Design**: No bias parameters throughout - cleaner architecture\n",
        "  → **Pattern**: `bias=False` appears in all Linear layers throughout the model\n",
        "\n",
        "• **RMSNorm Normalization**: Uses RMS normalization instead of LayerNorm\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "  ```\n",
        "\n",
        "• **Deeper Architecture**: 16 layers vs DistilGPT2's 6 - more **Depth Pruning opportunities**\n",
        "  → **Line**: `(0-15): 16 x LlamaDecoderLayer(`\n",
        "\n",
        "• **Rotary Position Embeddings**: Advanced positional encoding that works better for longer sequences\n",
        "  → **Line**: `(rotary_emb): LlamaRotaryEmbedding()`\n",
        "\n",
        "• **SiLU Activation**: Smooth activation function in MLP that influences how gating works in the GLU structure\n",
        "  → **Line**: `(act_fn): SiLU()`\n",
        "\n",
        "• **Perfect Width Pruning Candidate**: The dual-path MLP structure (gate_proj + up_proj) makes this architecture ideal for demonstrating advanced neuron selection strategies\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "  ```"
      ],
      "metadata": {
        "id": "N0DxePWiuuxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/gemma-3-270m\n",
        "Gemma-3 is an evolution of the pattern introduced with Llama, confirming that Llama's pattern is a de facto standard in the industry, adopted by other big players like Google.\n",
        "\n",
        "Additionally, it allows us to connect with the practical work from Chapter 2. When observing its structure, we'll see that it's almost identical to Llama's, but we'll focus on the subtle implementation variations, like the use of different activation functions."
      ],
      "metadata": {
        "id": "lFTnPt59_zIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"google/gemma-3-270m\")"
      ],
      "metadata": {
        "id": "iBXbUiTz_wU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUD9qT4D_7zL",
        "outputId": "9bb42615-ada6-4cd1-9ce9-64ae7ccfe126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
            "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
            "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "clean_memory(model)\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")"
      ],
      "metadata": {
        "outputId": "29c7ac88-c5b5-4c6a-cfab-88515e39af54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EuCOkBj5Juv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention heads: 4\n",
            "Head dimensions: 160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemma-3-270M - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Compact GLU Architecture**: Modern GLU structure (gate_proj + up_proj → down_proj) like Llama but in a much smaller footprint\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Grouped Query Attention with Normalization**: GQA structure (Q: 640→1024, K/V: 640→256) plus unique **q_norm and k_norm** layers\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
        "  (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
        "  (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
        "  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
        "  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
        "  ```\n",
        "\n",
        "• **Ultra-Lightweight Design**: Only 640 hidden dimensions and 270M parameters\n",
        "  → **Pattern**: `in_features=640` appears throughout + `Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)`\n",
        "\n",
        "• **Conservative Expansion Ratio**: 3.2x MLP expansion (640→2048) - more modest than Llama's 4x\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Excessive Normalization**: Four RMSNorm layers per decoder block - demonstrates modern architecture's emphasis on training stability\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
        "  ```\n",
        "\n",
        "• **Dual Rotary Embeddings**: Both standard and \"local\" rotary embeddings - advanced positional encoding\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (rotary_emb): Gemma3RotaryEmbedding()\n",
        "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
        "  ```\n",
        "\n",
        "• **GELU-Tanh Activation**: Uses PytorchGELUTanh instead of SiLU - shows activation function diversity\n",
        "  → **Line**: `(act_fn): PytorchGELUTanh()`\n",
        "\n",
        "• **Scaled Word Embeddings**: `Gemma3TextScaledWordEmbedding` indicates specialized embedding handling\n",
        "  → **Line**: `(embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)`\n",
        "\n",
        "• **Perfect Educational Model**: Small enough for laptop experimentation but sophisticated enough to demonstrate all modern techniques - ideal for **Depth Pruning** (18 layers) and **Width Pruning** examples\n",
        "  → **Line**: `(0-17): 18 x Gemma3DecoderLayer(` + compact dimensions throughout\n"
      ],
      "metadata": {
        "id": "jvtCQcYjodiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Other Notable Architectures"
      ],
      "metadata": {
        "id": "cZWsqfLhjHRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## microsoft/Phi-4-mini-instruct\n",
        "\n",
        "We include microsoft/Phi-4-mini-instruct as a case study on diversity in architecture design. With this model, we'll observe two fundamental differences compared to the Llama/Gemma pattern: first, its attention mechanism, which returns to the classic Multi-Head Attention (MHA) implementation; and second, its MLP block, which uses an ingenious fused layer that differs from the three-layer design we've seen."
      ],
      "metadata": {
        "id": "1nEWChaLBKbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"microsoft/Phi-4-mini-instruct\")"
      ],
      "metadata": {
        "id": "XwVsSKqRAFdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UipxacyiBNp8",
        "outputId": "0eaec1de-f2e8-40e8-ec58-625621e3b4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(200064, 3072, padding_idx=199999)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): Phi3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=200064, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "clean_memory(model)\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")"
      ],
      "metadata": {
        "outputId": "1e93c4b4-5579-4c46-e516-2ad8fbc60ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfeGU6sH5XA3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention heads: 24\n",
            "Head dimensions: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phi-4-mini-instruct - Key Insights Mapped to Model Structure:**\n",
        "\n",
        "• **Deep Architecture**: 32 layers, demonstrates **extreme Depth Pruning potential** but requires sophisticated layer selection strategies\n",
        "  → **Line**: `(0-31): 32 x Phi3DecoderLayer(`\n",
        "\n",
        "• **Fused GLU Implementation**: `gate_up_proj` combines gating and up projection in a single matrix (3072→16384), then splits internally\n",
        "  → **Line**: `(gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)`\n",
        "\n",
        "• **Compact Multi-Head Attention**: Single `qkv_proj` (3072→5120) handles all Q, K, V projections\n",
        "  → **Line**: `(qkv_proj): Linear(in_features=3072, out_features=5120, bias=False)`\n",
        "\n",
        "• **Asymmetric Down Projection**: `down_proj` expects 8192 inputs but `gate_up_proj` outputs 16384 - confirms the internal splitting of the fused projection\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
        "  (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
        "  ```\n",
        "\n",
        "• **Residual Dropout Layers**: Explicit dropout for attention and MLP residual connections - additional regularization not seen in Llama\n",
        "  → **Lines**:\n",
        "  ```\n",
        "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
        "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
        "  ```\n",
        "\n",
        "• **Large Vocabulary**: 200K+ tokens vs Llama's 128K - shows how modern models handle larger tokenization schemes\n",
        "  → **Line**: `(embed_tokens): Embedding(200064, 3072, padding_idx=199999)`\n",
        "\n",
        "• **Zero Dropout Training**: All dropout probabilities set to 0.0 - indicates this is an inference-optimized checkpoint\n",
        "  → **Pattern**: `p=0.0` in both `(resid_attn_dropout)` and `(resid_mlp_dropout)` lines\n"
      ],
      "metadata": {
        "id": "zgSxtb3AkmNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Llama-3.1-8B-Instruct quantized.\n",
        "\n",
        "This model has been chosen for two strategic reasons. First, its large size (8B parameters) makes quantization a practical necessity to run it in most environments, allowing us to see how the quantization library replaces its layers. Second, being an -Instruct model, we observe that the architecture is the same as the foundational model and that the ability to follow instructions is added to the model just with fine-tuning."
      ],
      "metadata": {
        "id": "xtrT6XqWyWVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "R7A5mKnCB8hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ],
      "metadata": {
        "id": "7Q8NIn87zHpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_instruct_quantized = AutoModelForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )"
      ],
      "metadata": {
        "id": "JW8xlEa4z1Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_instruct_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ostMLAVu4Nc_",
        "outputId": "e24a7118-8dc5-4db0-ef16-761cacf0d316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "clean_memory(model)\n",
        "print(f\"Attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Head dimensions: {config.hidden_size // config.num_attention_heads}\")"
      ],
      "metadata": {
        "outputId": "cf923922-89fd-4547-d363-1b5f50d8105e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnHLAqEB7p13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention heads: 32\n",
            "Head dimensions: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `Linear` layers of the Llama-3.2-1B model have been replaced by `Linear4bit` layers."
      ],
      "metadata": {
        "id": "-RQnzIE9hNcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFB_jo2Nzczo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we've inspected five different models to understand their internal components.\n",
        "\n",
        "DistilGPT2 is the one that has the classic structure: simple MLP with 4x expansion and traditional Multi-Head Attention.\n",
        "\n",
        "Llama-3.2-1B introduced the two key innovations of modern models: GLU architecture in the MLP (gate_proj + up_proj → down_proj) and Grouped Query Attention to optimize memory.\n",
        "\n",
        "Gemma-3 confirmed that the Llama pattern has become standard, maintaining the same GLU structure but adding additional normalizations and using GELU-Tanh instead of SiLU as activation in the GLU structure.\n",
        "\n",
        "With Phi-4 we've seen that there's room for variations: its fused GLU implementation (gate_up_proj) is more memory efficient, and its return to classic MHA shows that not all optimizations are universal.\n",
        "\n",
        "The key lesson is that, regardless of variations, all these models share similar components we can modify: stacked layers for depth pruning, MLP projections for width pruning, and attention mechanisms we can bypass dynamically.\n",
        "\n",
        "Understanding these structures is fundamental for the optimization techniques we'll apply in the upcoming chapters."
      ],
      "metadata": {
        "id": "yl2cERJGlE-T"
      }
    }
  ]
}