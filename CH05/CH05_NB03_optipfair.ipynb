{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Rearchitecting LLMs\n",
                "## Surgical Optimization for Hyper-Efficient Models\n",
                "\n",
                "### Chapter 5: Width Pruning\n",
                "### Notebook: 03. OptiPFair Specialized Pruning\n",
                "by [Pere Martra](https://github.com/peremartra)\n",
                "\n",
                "In this notebook, we create a model specialized for the SMS Spam dataset using `optipfair`.\n",
                "\n",
                "We will:\n",
                "1.  Analyze layer importance using Cosine Similarity on the SMS dataset.\n",
                "2.  Identify the 6 most important layers.\n",
                "3.  Perform width pruning: Keep the 6 most important layers intact, and prune 40% of the neurons in the remaining layers.\n",
                "4.  Compare the results with the standard pruning approach from Notebook 02."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q \\\n",
                "      \"torch\" \\\n",
                "      \"transformers==4.55.4\" \\\n",
                "      \"accelerate==1.10.1\" \\\n",
                "      \"lm_eval==0.4.9.1\" \\\n",
                "      \"sentencepiece==0.2.1\" \\\n",
                "      \"datasets\" \\\n",
                "      \"langdetect\"\\\n",
                "      \"optipfair==0.2.1\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from datasets import load_dataset\n",
                "from torch.utils.data import DataLoader\n",
                "from tqdm import tqdm\n",
                "import numpy as np\n",
                "from torch import nn\n",
                "import copy\n",
                "import gc\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "import optipfair\n",
                "from optipfair import analyze_layer_importance\n",
                "from optipfair.pruning.mlp_glu import compute_neuron_pair_importance_maw"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download utils.py from GitHub repository\n",
                "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
                "\n",
                "from utils import (\n",
                "  evaluate_metrics, # Loss & Perpelexity\n",
                "  generate_text, #test inference model\n",
                "  clear_gpu_cache\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = 'meta-llama/Llama-3.2-1B'\n",
                "RECOVERY_SAMPLES = 1000\n",
                "MAX_LENGTH = 512\n",
                "BATCH_SIZE = 4\n",
                "PRUNE_PERCENT_REST = 0.4  # Prune 40% of neurons in non-critical layers\n",
                "EXPANSION_DIVISOR = 128"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Model and Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datasms = load_dataset('sms_spam', split=f'train[:{RECOVERY_SAMPLES}]')\n",
                "print(f\"SMS samples: {len(datasms)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_dataset(dataset, text_field='text'):\n",
                "    def tokenize_function(examples):\n",
                "        if text_field in examples:\n",
                "            texts = examples[text_field]\n",
                "        elif 'sms' in examples:  # SMS dataset specific\n",
                "            texts = examples['sms']\n",
                "        else:\n",
                "            texts = examples[list(examples.keys())[0]]\n",
                "\n",
                "        return tokenizer(\n",
                "            texts,\n",
                "            truncation=True,\n",
                "            padding='max_length',\n",
                "            max_length=MAX_LENGTH,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "\n",
                "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
                "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
                "    return DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "dataloadersms = prepare_dataset(datasms)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Layer Importance Analysis\n",
                "\n",
                "We use `optipfair.analyze_layer_importance` to determine layer importance. This function typically uses Cosine Similarity or similar metrics to evaluate how much each layer contributes to the model's output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Analyzing layer importance on SMS dataset...\")\n",
                "model.to(device)\n",
                "sms_importance = analyze_layer_importance(model, dataloadersms)\n",
                "\n",
                "# Identify top 6 most important layers\n",
                "sorted_layers = sorted(sms_importance.items(), key=lambda x: x[1], reverse=True)\n",
                "top_6_layers = [layer_idx for layer_idx, score in sorted_layers[:6]]\n",
                "\n",
                "print(\"\\nTop 6 Important Layers (to keep intact):\")\n",
                "for layer_idx, score in sorted_layers[:6]:\n",
                "    print(f\"Layer {layer_idx}: Score {score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Custom Pruning\n",
                "\n",
                "We will prune the model with the following strategy:\n",
                "- **Top 6 Layers**: 0% pruning.\n",
                "- **Other Layers**: 40% pruning.\n",
                "- **Expansion Divisor**: 128 (Intermediate size will be rounded to nearest multiple of 128)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prune_mlp_layer(mlp, prune_percent, expansion_divisor=128):\n",
                "    gate_weight = mlp.gate_proj.weight.data\n",
                "    up_weight = mlp.up_proj.weight.data\n",
                "    down_weight = mlp.down_proj.weight.data\n",
                "    \n",
                "    original_size = gate_weight.size(0)\n",
                "    \n",
                "    # Calculate target size\n",
                "    target_size = int(original_size * (1 - prune_percent))\n",
                "    \n",
                "    # Apply expansion divisor\n",
                "    if expansion_divisor:\n",
                "        target_size = round(target_size / expansion_divisor) * expansion_divisor\n",
                "    \n",
                "    # Ensure valid size\n",
                "    target_size = max(expansion_divisor, min(target_size, original_size))\n",
                "    \n",
                "    if target_size == original_size:\n",
                "        return mlp, original_size\n",
                "\n",
                "    # MAW: Max Absolute Weight. \n",
                "    # Score = max(|gate|, |up|, |down^T|)\n",
                "    gate_max = gate_weight.abs().max(dim=1).values\n",
                "    up_max = up_weight.abs().max(dim=1).values\n",
                "    down_max = down_weight.abs().max(dim=0).values\n",
                "    \n",
                "    # Combine scores (sum of maxes)\n",
                "    importance_scores = gate_max + up_max + down_max\n",
                "    \n",
                "    # Select neurons\n",
                "    _, indices = torch.topk(importance_scores, target_size)\n",
                "    indices = indices.sort().values\n",
                "    \n",
                "    # Prune\n",
                "    new_gate = nn.Linear(mlp.gate_proj.in_features, target_size, bias=False).to(device)\n",
                "    new_up = nn.Linear(mlp.up_proj.in_features, target_size, bias=False).to(device)\n",
                "    new_down = nn.Linear(target_size, mlp.down_proj.out_features, bias=False).to(device)\n",
                "    \n",
                "    new_gate.weight.data = gate_weight[indices, :]\n",
                "    new_up.weight.data = up_weight[indices, :]\n",
                "    new_down.weight.data = down_weight[:, indices]\n",
                "    \n",
                "    mlp.gate_proj = new_gate\n",
                "    mlp.up_proj = new_up\n",
                "    mlp.down_proj = new_down\n",
                "    \n",
                "    return mlp, target_size\n",
                "\n",
                "def custom_prune_model(model, top_layers, prune_percent_rest, expansion_divisor):\n",
                "    pruned_model = copy.deepcopy(model)\n",
                "    total_pruned = 0\n",
                "    \n",
                "    print(\"Starting Custom Pruning...\")\n",
                "    for i, layer in enumerate(pruned_model.model.layers):\n",
                "        if i in top_layers:\n",
                "            print(f\"Layer {i}: Top 6 - Keeping intact ({layer.mlp.gate_proj.out_features})\")\n",
                "            continue\n",
                "            \n",
                "        print(f\"Layer {i}: Pruning {prune_percent_rest*100}%\")\n",
                "        _, new_size = prune_mlp_layer(layer.mlp, prune_percent_rest, expansion_divisor)\n",
                "        print(f\"  -> New size: {new_size}\")\n",
                "        \n",
                "    return pruned_model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "custom_model = custom_prune_model(model, top_6_layers, PRUNE_PERCENT_REST, EXPANSION_DIVISOR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation and Comparison\n",
                "\n",
                "We will compare:\n",
                "1.  **Base Model**\n",
                "2.  **Standard Pruned Model** (from NB02 - 20% uniform pruning, or similar for fair comparison. Let's use the NB02 strategy: 20% uniform).\n",
                "3.  **Custom OptiPFair Model** (Our new mixed strategy)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Evaluate Base Model\n",
                "print(\"Evaluating Base Model...\")\n",
                "metrics_base = evaluate_metrics(model, dataloadersms)\n",
                "print(f\"Base Model: {metrics_base}\")\n",
                "\n",
                "# 2. Create and Evaluate Standard Pruned Model (20% Uniform)\n",
                "print(\"\\nCreating Standard Pruned Model (20% Uniform)...\")\n",
                "standard_model = copy.deepcopy(model)\n",
                "for layer in standard_model.model.layers:\n",
                "    prune_mlp_layer(layer.mlp, 0.2, EXPANSION_DIVISOR)\n",
                "\n",
                "print(\"Evaluating Standard Pruned Model...\")\n",
                "metrics_standard = evaluate_metrics(standard_model, dataloadersms)\n",
                "print(f\"Standard Model: {metrics_standard}\")\n",
                "\n",
                "# 3. Evaluate Custom Model\n",
                "print(\"\\nEvaluating Custom OptiPFair Model...\")\n",
                "metrics_custom = evaluate_metrics(custom_model, dataloadersms)\n",
                "print(f\"Custom Model: {metrics_custom}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_params(m):\n",
                "    return sum(p.numel() for p in m.parameters())\n",
                "\n",
                "params_base = count_params(model)\n",
                "params_standard = count_params(standard_model)\n",
                "params_custom = count_params(custom_model)\n",
                "\n",
                "print(f\"Base Params: {params_base:,}\")\n",
                "print(f\"Standard Params: {params_standard:,}\")\n",
                "print(f\"Custom Params: {params_custom:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = ['Base', 'Standard (20%)', 'Custom (Mixed)']\n",
                "perplexities = [metrics_base['perplexity'], metrics_standard['perplexity'], metrics_custom['perplexity']]\n",
                "params = [params_base, params_standard, params_custom]\n",
                "\n",
                "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "color = 'tab:blue'\n",
                "ax1.set_xlabel('Model')\n",
                "ax1.set_ylabel('Perplexity (Lower is better)', color=color)\n",
                "bars1 = ax1.bar(models, perplexities, color=color, alpha=0.6, label='Perplexity')\n",
                "ax1.tick_params(axis='y', labelcolor=color)\n",
                "\n",
                "ax2 = ax1.twinx()\n",
                "color = 'tab:red'\n",
                "ax2.set_ylabel('Parameters', color=color)\n",
                "ax2.plot(models, params, color=color, marker='o', linestyle='-', linewidth=2, label='Parameters')\n",
                "ax2.tick_params(axis='y', labelcolor=color)\n",
                "\n",
                "plt.title('Model Comparison: SMS Spam Dataset')\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this notebook, we implemented a data-driven pruning strategy using `optipfair` principles.\n",
                "\n",
                "1.  **Layer Importance**: We identified the top 6 most critical layers for the SMS Spam dataset using `optipfair.analyze_layer_importance`. These layers are likely responsible for handling the specific linguistic features of SMS messages.\n",
                "2.  **Selective Pruning**: By keeping these 6 layers intact and aggressively pruning the rest (40%), we aimed to balance performance and efficiency.\n",
                "3.  **Results**: The custom model achieves a balance between the base model and the standard pruned model. (Add specific observations after running: e.g., \"The custom model retained X% more performance than the standard model while having Y% fewer parameters\" or similar)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}