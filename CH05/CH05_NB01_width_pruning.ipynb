{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/CH05/CH05/CH05_NB01_width_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "###Â Chapter 5: width pruning\n",
        "### Notebook: 01. Static Neuron Selection.\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white)](https://x.com/PereMartra) [![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* Llama-3.2-1B\n",
        "_____\n",
        "\n",
        "In this notebook, we explore width pruning applied to the  MLP modules with GLU structure, present in modern Transformers models like Llama, Gemma, Qwen, Mistral and more.  \n",
        "\n",
        "We'll implement a static neuron selection method that ranks neurons by their weight magnitude. The underlying hypothesis is simple: neurons whose weights have smaller absolute values contribute less to the model's transformations, making them candidates for removal with minimal impact on performance.\n",
        "\n",
        "By the end of this notebook, you'll understand how to surgically reduce the expansion ratio of an MLP moduleâ€”going from 4x to 2.4x in our Llama-3.2-1B exampleâ€”and measure the trade-offs this creates.\n",
        "\n",
        "```\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "```\n"
      ],
      "metadata": {
        "id": "vf04d7qXqwUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up notebook"
      ],
      "metadata": {
        "id": "3XUED5yYwoJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "      \"torch==2.8.0+cu126\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "      \"lm_eval==0.4.9.1\" \\\n",
        "      \"sentencepiece==0.2.1\" \\\n",
        "      \"sentence-transformers==5.1.0\" \\\n",
        "      \"langdetect\" \\\n",
        "      \"optipfair==0.1.5\""
      ],
      "metadata": {
        "id": "4AlU0F9Nu6xT",
        "outputId": "f52da50d-3d7f-46f3-bf1c-53ab2c1de4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m483.4/483.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from lm_eval import evaluator\n",
        "from torch import nn\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "from optipfair import prune_model"
      ],
      "metadata": {
        "id": "CCBC8p1FvBsV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R5xvVourquO6",
        "outputId": "dbc726ef-9d2b-4bbb-ccce-8507a16452f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "MAX_NEW_TOKENS = 50\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The helper functions used in previous notebooks have been grouped in the [`utils.py`](https://github.com/peremartra/llama-glu-expansion-pruning/blob/main/utils.py) file. To use them, we import the file from the repository."
      ],
      "metadata": {
        "id": "2WtyCZ5jOUZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")\n",
        "\n",
        "from utils import (\n",
        "  model_evaluation, # Evals with lm_eval\n",
        "  evaluate_metrics, # Loss & Perpelexity\n",
        "  generate_text, #test inference model\n",
        "  clear_gpu_cache\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4xdRPT0za11d",
        "outputId": "c9e72b33-05bc-43c0-add3-5a9ba031b7e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… utils.py downloaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Static neuron selection."
      ],
      "metadata": {
        "id": "-249Q9QoNESB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this experiment, we'll use Llama-3.2-1Bâ€”a compact but capable model that makes our pruning effects clearly visible. This model has a hidden size of 2048 and an intermediate size of 8192, giving it a 4x expansion ratio in its MLP modules. That 4x expansion is exactly what we're about to surgically reduce.\n",
        "\n",
        "Remember from Chapter 3: in GLU-based architectures like Llama, the MLP module expands the representation from hidden_size to intermediate_size, processes it through gating mechanisms, and contracts it back.\n",
        "\n",
        "By targeting this expansion, we can shrink the model without breaking the connections between modulesâ€”the attention layer will still receive tensors of size 2048, just as it expects."
      ],
      "metadata": {
        "id": "BTlN88hcOodZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model\n"
      ],
      "metadata": {
        "id": "P7Q7dolFPxhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'meta-llama/Llama-3.2-1B'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "model.generation_config.temperature = None\n",
        "model.generation_config.top_p = None\n",
        "model.generation_config.top_k = None"
      ],
      "metadata": {
        "id": "MpLvzaifv_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.1 Choosing neurons to discard"
      ],
      "metadata": {
        "id": "L7a4AcV1NRsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "MNO2GpudUC7n",
        "outputId": "a4efbdde-8408-4157-a54f-01fbc8d406f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we already saw in [chapter 3](https://github.com/peremartra/Rearchitecting-LLMs/tree/main/CH03), we find a set of three layers in the MLP module.\n",
        "* `gate_proj` and `up_proj` scale the information from 2048 to 8192 and `down_proj` returns it to 2048, its original size.\n",
        "\n",
        "Our width pruning method directly attacks this expansion.\n",
        "This allows us to vary the size of the different layers individually without breaking the model, the connection between the MLP layer and the Attention layer will always be with the original length of the data. In the case of Llama-3.2-1B 2048."
      ],
      "metadata": {
        "id": "ORQklNY8Jrl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated = generate_text(model, tokenizer, prompt, device)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "id": "ThxpKopfIjJz",
        "outputId": "01f90e4d-da13-48db-a6a8-6959650b52f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a rich history and culture, and it is home to many famous landmarks, including the E\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `compute_neuron_pair_importance` function calculates an importance score for each neuron by analyzing its weights across both `gate_proj` and `up_proj` layers. Why look at both? Because in GLU architectures, these two projections work as a pairâ€”one gates the otherâ€”so a neuron's true contribution depends on its combined weight magnitude across both transformations.\n",
        "\n",
        "The function computes the maximum absolute weight range for each neuron in both layers, then sums these values to produce a single importance score per neuron. Neurons with higher scores have larger weight magnitudes and are presumed to contribute more to the model's transformations, making them safer to keep.\n"
      ],
      "metadata": {
        "id": "VVf-JjX4QTuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "  \"\"\"\n",
        "  compute neuron pair importance scores (Maximum Absolute Weight)\n",
        "\n",
        "  Args:\n",
        "  - gate_weight: Weight matrix from the gate_proj layer.\n",
        "  - up_weight: Weight matrix from the up_weight layer.\n",
        "\n",
        "  Returns:\n",
        "  - importance_scores: Importance scores for each neuron pair.\n",
        "  \"\"\"\n",
        "\n",
        "  gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
        "  up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
        "  importance_scores = gate_max_abs + up_max_abs\n",
        "  return importance_scores"
      ],
      "metadata": {
        "id": "woDDHlqaqjYl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we can measure neuron importance, we need a function that actually performs the surgery. `prune_neuron_pairs` takes our importance scores, ranks all neurons, and keeps only the top performers based on our pruning percentage.\n",
        "\n",
        "The process is straightforward: calculate how many neurons to keep, identify their indices, create new smaller layers with the reduced dimensions, and copy over only the weights that correspond to our selected neurons. The result? Three new layers: `gate_proj`, `up_proj`, and `down_proj`, that are narrower but still perfectly compatible with the rest of the model."
      ],
      "metadata": {
        "id": "XO_I3-HJQeCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_neuron_pairs(mlp, prune_percent):\n",
        "    \"\"\"\n",
        "    Reduces the dimensions of the **gate_proj**,**up_proj**, **down_proj**\n",
        "    layers removing the least important neurons.\n",
        "\n",
        "    Args:\n",
        "    - mlp: Layers to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - new_gate_proj, new_up_proj, new_down_proj:  New pruned layers.\n",
        "    - k: New intermediate size.\n",
        "\n",
        "    \"\"\"\n",
        "    # Extract the weights from the MLP layers\n",
        "    #  these weights are used to calculate each neuron's\n",
        "    #  importance score in the next step.\n",
        "    gate_weight = mlp.gate_proj.weight.data.float()\n",
        "    up_weight = mlp.up_proj.weight.data.float()\n",
        "\n",
        "    #Compute importance stores. Neurons with higher importance scores\n",
        "    # are considered more important and less likely to be pruned.\n",
        "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
        "\n",
        "    #Store the original number of neurons in the intermediate layer.\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "    #Computes the number of neurons to prune.\n",
        "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
        "    #Calculate the number of neurons to keep. The new intermediate size.\n",
        "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
        "\n",
        "    #Just check that there is no big error calculating k. We can't prune all the neurons.\n",
        "    if k <= 0:\n",
        "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
        "\n",
        "    #Select the neuros to keep, by obtaining the indices to keep.\n",
        "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    #create the new layers\n",
        "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
        "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
        "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
        "\n",
        "    #copy weights to the new layers.\n",
        "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
        "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "    #return new layers and intermediate size.\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, k\n"
      ],
      "metadata": {
        "id": "fXY6U0T3shDh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.2 Creating the model."
      ],
      "metadata": {
        "id": "HFB3JUcHNm-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to scale this operation across the entire model. The `update_model` function orchestrates the pruning by walking through each of the model's transformer blocks, applying `prune_neuron_pairs` to every MLP module, and swapping in the newly pruned layers.\n",
        "\n",
        "Crucially, it also updates the model's configuration file to reflect the new `intermediate_size`. Without this step, any code that inspects the model's architecture would still see the old dimensionsâ€”a recipe for confusion later.\n",
        "\n",
        "When this function finishes, we'll have a fully functional model that's structurally identical to the original, just narrower in MLP Layers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mO7Qwv6FR708"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterates throught the model layers and applies pruning.\n",
        "def update_model(model, prune_percent):\n",
        "    \"\"\"\n",
        "    It modifies each mlp layer present in model, to retain only the most\n",
        "    important neurons. Creating new smaller versions of each layer pruned.\n",
        "\n",
        "    Args:\n",
        "    - model: Model to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - model: New pruned model.\n",
        "    \"\"\"\n",
        "    new_intermediate_size = None\n",
        "\n",
        "    #loop for each model layer.\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        #Since each layer is a LlamaDecoderLayer it contains multiple components\n",
        "        # Attention, MLP and Layer norms. We're targetting MLP component\n",
        "        # by accesing layer.mlp.\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        #Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
        "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
        "\n",
        "        #Replace the Origiginal Layers with Pruned Layers.\n",
        "        mlp.gate_proj = new_gate_proj\n",
        "        mlp.up_proj = new_up_proj\n",
        "        mlp.down_proj = new_down_proj\n",
        "\n",
        "        #new_intermediate_size only needs to be set once\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "    #Update the model config file.\n",
        "    model.config.intermediate_size = new_intermediate_size\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kGjnngrBsnuH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll prune 40% of the expansion neurons from every MLP module, shrinking the expansion ratio from 4x down to roughly 2.4x. This single change will reduce the model's total parameter count by over 26%.\n"
      ],
      "metadata": {
        "id": "4TImKF4DVQM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prune_percent = 0.4  # Prune 40% of neurons\n",
        "model_pruned = update_model(copy.deepcopy(model), prune_percent)"
      ],
      "metadata": {
        "id": "Ggjdm9z9szd_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "id": "Ehi2EjompBeR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculate the number of parameters\n",
        "original_param_count = count_parameters(model)\n",
        "pruned_param_count = count_parameters(model_pruned)\n",
        "reduction_in_params = original_param_count - pruned_param_count\n",
        "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
        "\n",
        "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
        "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
        "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sGcevz47SQ7c",
        "outputId": "cb8f99da-516c-4eac-8422-daa2ad104374"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model parameters: 913770496\n",
            "Reduction in parameters: 322043904\n",
            "Percentage of weight savings: 26.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the pruned model\n",
        "generated = generate_text(model_pruned, tokenizer, prompt, device)\n",
        "print(f\"Generated text after pruning: {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PAFwxlJNSTey",
        "outputId": "1737db37-5f39-4f51-e751-9c308e30eeb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text after pruning: Paris is the capital of France. It is also known as Paris, the city of the France, is located in the center of this country. Paris is a capital city, it is known also as the Paris city. This city is one of Europe's most famous cities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model still generates fluent text, but the output has clearly shifted. Compare the two responses: the original gives us a clean, factual statement (\"Paris is the capital of France and the largest city in the country\"), while the pruned version becomes repetitive and awkward (\"Paris, the city of the France\").\n",
        "\n",
        "The sentence structure degrades, coherence dropsâ€”but critically, the model hasn't collapsed into gibberish."
      ],
      "metadata": {
        "id": "3GTAfFcHVk2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "goaM8zC6XCra",
        "outputId": "f16037b3-b31b-46b0-c775-6a836da1cd58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=4916, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=4916, bias=False)\n",
            "          (down_proj): Linear(in_features=4916, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that now the intermediate_size is 4916, so the expansion has gone from 4x to 2.4x"
      ],
      "metadata": {
        "id": "jS2FG_msWG3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.3 Analyzing Benchmarks."
      ],
      "metadata": {
        "id": "iR4L61BmXHPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BENCHMARKS_BASE = [\n",
        "    {\"name\": \"truthfulqa_mc2\", \"num_fewshot\": 0},\n",
        "    #{\"name\": \"wikitext\", \"num_fewshot\": 0},\n",
        "    #{\"name\": \"mmlu\", \"num_fewshot\": 5},  # Standard is 5-shot for MMLU\n",
        "    #{\"name\": \"gsm8k\", \"num_fewshot\": 5},  # Chain-of-thought requires few-shot\n",
        "    #{\"name\": \"ifeval\", \"num_fewshot\": 0},\n",
        "    #{\"name\": \"leaderboard_musr\", \"num_fewshot\": 0}, # Removed this task as it seems to be causing issues\n",
        "]"
      ],
      "metadata": {
        "id": "4ack8yxoXFDJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_pruned = model_evaluation(model_pruned, tokenizer, BENCHMARKS_BASE, limit=10)"
      ],
      "metadata": {
        "id": "lqdzKzmNgjNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell above you can find the necessary line to run several of the benchmarks yourself, but keep in mind that if you expand the limit of values to use, you'll need several hours for some of them: ifeval, mmlu, gsm8k and they will need to be run on a GPU with more memory, like the L4.\n"
      ],
      "metadata": {
        "id": "mTMIMcUWW2wG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the table below, you can find the results for this same model obtained in different benchmarks. The data has been obtained directly from the experiments available in the repository: [**GLU Pruning - Width Pruning Evaluation**](https://github.com/peremartra/llama-glu-expansion-pruning/tree/main).\n",
        "\n",
        "You can consult them, along with the results of more models at: https://github.com/peremartra/llama-glu-expansion-pruning/tree/main/results"
      ],
      "metadata": {
        "id": "AeTSLbDeYk_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmarks\n",
        "| Expansion Ratio | MMLU            | GSM8K           | ARC-C           | HellaSwag       | BoolQ           | PIQA            | WikiText PPL      | Lambada PPL         |\n",
        "|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-------------------|---------------------|\n",
        "| **4.0x** (baseline) | 0.3111          | 0.0660          | 0.3626          | 0.6363          | 0.6343          | 0.7437          | 11.57             | 5.75                |\n",
        "| **2.4x** (40% pruning) | 0.2689 (-13.6%) | 0.0205 (-68.9%) | 0.2509 (-30.8%) | 0.3737 (-41.3%) | 0.622 (-1.9%)   | 0.6235 (-16.2%) | 56.33 (+386.9%)   | 90.38 (+1471.8%)    |\n",
        "\n",
        "### Benchmarks That Improve With Reduced Expansion\n",
        "\n",
        "| Expansion Ratio | IFEval | MUSR | TruthfulQA-MC1 | TruthfulQA-MC2 | WinoGrande |\n",
        "|-----------------|--------|------|----------------|----------------|------------|\n",
        "| **4.0x** (baseline) | 0.1035 | 0.3399 | 0.2338 | 0.3772 | 0.5991 |\n",
        "| **2.4x** (40% pruning) | 0.1516 (**+46.5%**) | 0.4286 (**+26.1%**) | 0.2485 (**+6.3%**) | 0.4298 (**+13.9%**) | 0.5706 (-4.8%) |"
      ],
      "metadata": {
        "id": "PMsnrVJ3XsPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#del(model)\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "Wg2-Uo2HhBZx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "GqBN2__uiSIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the generated model is a totally different model from the original, the effect that pruning has had on the different benchmarks has been uneven.\n",
        "\n",
        "The 'width-pruning' of 40% in the MLP expansion layers severely degrades the model's core capabilities. The reasoning metrics (GSM8K -68.9%), general knowledge (MMLU -13.6%), and common sense (HellaSwag -41.3%) suffer drastic drops. The most evident damage is seen in perplexity (PPL), which increases astronomically (+386.9% and +1471.8%), **indicating that the model's fundamental ability to model language and coherently predict text collapses**.\n",
        "\n",
        "However, this pruning reveals a surprising specialization. Although the model becomes less 'intelligent' in general terms, **it improves drastically in its ability to follow instructions (IFEval +46.5%), or MUSR + 45%, and in its truthfulness (TruthfulQA +13.9% on MC2)**.\n",
        "\n",
        "This suggests a clear 'trade-off': **the pruned model loses sophistication and world knowledge, but becomes more literal, obedient**, and less prone to hallucinating or deviating from the instruction, which improves its performance on tasks requiring strict fidelity."
      ],
      "metadata": {
        "id": "Wso-4J_-iVEn"
      }
    }
  ]
}