{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH05/CH05_NB02_data_sms_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f98e07",
      "metadata": {
        "id": "92f98e07"
      },
      "source": [
        "# Setting up notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ade7571c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ade7571c",
        "outputId": "d27a3cf6-e21f-4b10-b431-0b876272aa3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "      \"torch\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "      \"lm_eval==0.4.9.1\" \\\n",
        "      \"sentencepiece==0.2.1\" \\\n",
        "      \"datasets\" \\\n",
        "      \"langdetect\"\\\n",
        "      \"optipfair==0.2.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5050e0a5",
      "metadata": {
        "id": "5050e0a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from lm_eval import evaluator\n",
        "from torch import nn\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import gc\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6ec5a44a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ec5a44a",
        "outputId": "4e11e1f0-a5a8-45a9-bc2c-5478a21e540f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d307df",
      "metadata": {
        "id": "c7d307df"
      },
      "source": [
        "Download helper functions from the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "36300712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36300712",
        "outputId": "e7f2b098-f643-40e0-cd91-37cdcad19c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"✅ utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"❌ Failed to download utils.py\")\n",
        "\n",
        "from utils import (\n",
        "  model_evaluation, # Evals with lm_eval\n",
        "  evaluate_metrics, # Loss & Perpelexity\n",
        "  generate_text, #test inference model\n",
        "  clear_gpu_cache\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_detailed_performance(model, tokenizer, data_source, num_runs=3, max_new_tokens=50, max_samples=None):\n",
        "    \"\"\"\n",
        "    Measures inference performance metrics.\n",
        "\n",
        "    Args:\n",
        "        model: Model to evaluate\n",
        "        tokenizer: Tokenizer\n",
        "        data_source: DataLoader to sample from\n",
        "        num_runs: Number of runs per sample for averaging\n",
        "        max_new_tokens: Tokens to generate\n",
        "        max_samples: Limit number of samples (None = all)\n",
        "\n",
        "    Returns:\n",
        "        dict with timing statistics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    times = []\n",
        "    tokens_generated = []\n",
        "\n",
        "    samples = []\n",
        "    for batch in data_source:\n",
        "        for i in range(len(batch['input_ids'])):\n",
        "            samples.append(batch['input_ids'][i])\n",
        "            if max_samples and len(samples) >= max_samples:\n",
        "                break\n",
        "        if max_samples and len(samples) >= max_samples:\n",
        "            break\n",
        "\n",
        "    if max_samples:\n",
        "        samples = samples[:max_samples]\n",
        "\n",
        "    print(f\"Measuring performance on {len(samples)} samples...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in tqdm(samples, desc=\"Performance test\"):\n",
        "            input_ids = sample.unsqueeze(0).to(device)\n",
        "\n",
        "            for _ in range(num_runs):\n",
        "                start_time = time.time()\n",
        "                outputs = model.generate(\n",
        "                    input_ids,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "                end_time = time.time()\n",
        "\n",
        "                elapsed = end_time - start_time\n",
        "                times.append(elapsed)\n",
        "                tokens_generated.append(outputs.shape[1] - input_ids.shape[1])\n",
        "\n",
        "    avg_time = np.mean(times)\n",
        "    std_time = np.std(times)\n",
        "    avg_tokens = np.mean(tokens_generated)\n",
        "    tokens_per_sec = avg_tokens / avg_time if avg_time > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'avg_time_sec': avg_time,\n",
        "        'std_time_sec': std_time,\n",
        "        'avg_tokens': avg_tokens,\n",
        "        'tokens_per_sec': tokens_per_sec,\n",
        "        'num_samples': len(samples),\n",
        "        'num_runs': num_runs\n",
        "    }"
      ],
      "metadata": {
        "id": "lnRCe9yyUmbZ"
      },
      "id": "lnRCe9yyUmbZ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c52aeb41",
      "metadata": {
        "id": "c52aeb41"
      },
      "source": [
        "## Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9065c773",
      "metadata": {
        "id": "9065c773"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = 'meta-llama/Llama-3.2-1B'\n",
        "\n",
        "# Dataset configuration\n",
        "RECOVERY_SAMPLES = 100  # Calibration samples per dataset\n",
        "MAX_LENGTH = 1024\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# Pruning configuration\n",
        "PRUNE_PERCENT = 0.2  # 20% of neurons will be pruned\n",
        "\n",
        "# Generation configuration\n",
        "MAX_NEW_TOKENS = 50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a02967",
      "metadata": {
        "id": "48a02967"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "model.generation_config.temperature = None\n",
        "model.generation_config.top_p = None\n",
        "model.generation_config.top_k = None\n",
        "\n",
        "print(f\"✓ Loaded {MODEL_NAME}\")\n",
        "print(f\"  Layers: {len(model.model.layers)}\")\n",
        "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
        "print(f\"  Intermediate size: {model.config.intermediate_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef7cc965",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef7cc965",
        "outputId": "d634d1e3-7cb2-4d8c-d65a-61cfd622490e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model generation: Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a rich history and culture, and it is home to many famous landmarks, including the E\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated_base = generate_text(model, tokenizer, prompt, device)\n",
        "print(f\"Base model generation: {generated_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935a14d5",
      "metadata": {
        "id": "935a14d5"
      },
      "source": [
        "# Load and Prepare Calibration Datasets\n",
        "\n",
        "We'll load two contrasting datasets:\n",
        "\n",
        "1. **WikiText-2**: Long-form, encyclopedic text with complex language patterns\n",
        "2. **SMS Spam**: Short conversational messages with informal language\n",
        "\n",
        "These datasets will serve as calibration sources for our two pruned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d44889",
      "metadata": {
        "id": "10d44889"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "datawiki = load_dataset('wikitext', 'wikitext-2-raw-v1', split=f'train[:{RECOVERY_SAMPLES}]')\n",
        "datasms = load_dataset('sms_spam', split=f'train[:{RECOVERY_SAMPLES}]')\n",
        "\n",
        "print(f\"✓ WikiText samples: {len(datawiki)}\")\n",
        "print(f\"✓ SMS samples: {len(datasms)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5a909bea",
      "metadata": {
        "id": "5a909bea"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset, text_field='text'):\n",
        "    \"\"\"\n",
        "    Tokenizes and prepares a dataset for calibration.\n",
        "\n",
        "    Handles different dataset formats (WikiText uses 'text', SMS uses 'sms' field).\n",
        "    \"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        if text_field in examples:\n",
        "            texts = examples[text_field]\n",
        "        elif 'sms' in examples:  # SMS dataset specific\n",
        "            texts = examples['sms']\n",
        "        elif 'text' in examples:\n",
        "            texts = examples['text']\n",
        "        else:\n",
        "            texts = examples[list(examples.keys())[0]]\n",
        "\n",
        "        return tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "    return DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736f653f",
      "metadata": {
        "id": "736f653f"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "dataloaderwiki = prepare_dataset(datawiki)\n",
        "dataloadersms = prepare_dataset(datasms)\n",
        "\n",
        "print(f\"✓ Created dataloaders\")\n",
        "print(f\"  Wiki batches: {len(dataloaderwiki)}\")\n",
        "print(f\"  SMS batches: {len(dataloadersms)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base_wiki = evaluate_metrics(model, dataloaderwiki)\n"
      ],
      "metadata": {
        "id": "O48-PwbYrO4z",
        "outputId": "8bbf6d42-83cf-49fd-a284-decf8119eb41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "O48-PwbYrO4z",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 25/25 [00:14<00:00,  1.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base_sms = evaluate_metrics(model, dataloadersms)\n"
      ],
      "metadata": {
        "id": "3n_AMbL4sU0f",
        "outputId": "808138c2-bff5-4570-9547-a56c7eb8b0c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3n_AMbL4sU0f",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 25/25 [00:14<00:00,  1.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_wiki_timing = measure_detailed_performance(model, tokenizer, dataloaderwiki, max_samples=10)"
      ],
      "metadata": {
        "id": "akHFaKqOWRuj",
        "outputId": "2ee60603-73e0-452a-ddf6-c021aec808bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "akHFaKqOWRuj",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring performance on 10 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performance test:   0%|          | 0/10 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Performance test: 100%|██████████| 10/10 [00:20<00:00,  2.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_sms_timing = measure_detailed_performance(model, tokenizer, dataloadersms, max_samples=10)"
      ],
      "metadata": {
        "id": "WdMgItGZZQfJ",
        "outputId": "e6bb28a1-f1e9-4a58-d252-69ea24125904",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WdMgItGZZQfJ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring performance on 10 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performance test: 100%|██████████| 10/10 [00:32<00:00,  3.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n"
      ],
      "metadata": {
        "id": "yxtr-quOwQuS"
      },
      "id": "yxtr-quOwQuS",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_params = count_parameters(model)"
      ],
      "metadata": {
        "id": "_0RBrAtxvYVq"
      },
      "id": "_0RBrAtxvYVq",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_gpu_cache()\n"
      ],
      "metadata": {
        "id": "sxQHY7opwbi8"
      },
      "id": "sxQHY7opwbi8",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "031f18a6",
      "metadata": {
        "id": "031f18a6"
      },
      "source": [
        "# Data-Driven Pruning Functions\n",
        "\n",
        "These functions implement the CFSP-inspired methodology from CH05_NB02:\n",
        "- **Activation capture**: PyTorch hooks on down_proj to record runtime behavior\n",
        "- **Hybrid importance**: Combines weight magnitudes with activation norms\n",
        "- **Neuron pair pruning**: Removes least important neurons from gate_proj, up_proj, and down_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "161ac494",
      "metadata": {
        "id": "161ac494"
      },
      "source": [
        "## Activation Capture with PyTorch Hooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b2030c79",
      "metadata": {
        "id": "b2030c79"
      },
      "outputs": [],
      "source": [
        "# Global storage for accumulated activation norms\n",
        "_accumulated_act_norms = {}\n",
        "\n",
        "def setup_mlp_hooks_for_importance(model, device):\n",
        "    \"\"\"\n",
        "    Registers hooks on down_proj inputs (X_d) to calculate L2 norms\n",
        "    for each neuron, following CFSP Equation 8.\n",
        "\n",
        "    Accumulates norms across multiple calibration batches.\n",
        "\n",
        "    Returns:\n",
        "        handles: List of hook handles (for removal after calibration)\n",
        "    \"\"\"\n",
        "    global _accumulated_act_norms\n",
        "    _accumulated_act_norms.clear()\n",
        "\n",
        "    # Free memory before starting\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    handles = []\n",
        "\n",
        "    # Initialize storage on CPU to save VRAM\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        intermediate_size = layer.mlp.down_proj.in_features\n",
        "        _accumulated_act_norms[idx] = torch.zeros(\n",
        "            intermediate_size,\n",
        "            dtype=torch.float32,\n",
        "            device='cpu'\n",
        "        )\n",
        "\n",
        "    def make_hook(layer_idx):\n",
        "        def hook(module, input, output):\n",
        "            \"\"\"\n",
        "            Captures X_d (input to down_proj) and calculates its L2 norm.\n",
        "\n",
        "            X_d shape: [batch_size, seq_len, intermediate_size]\n",
        "            Output: [intermediate_size] with ||X_d^i|| for each neuron i\n",
        "            \"\"\"\n",
        "            X_d = input[0].detach()  # [B, S, I]\n",
        "\n",
        "            # Calculate L2 norm (Equation 8 from CFSP paper)\n",
        "            act_norms_L2 = torch.norm(\n",
        "                X_d.to(torch.float32),\n",
        "                p=2,\n",
        "                dim=(0, 1)  # Sum over batch and sequence\n",
        "            )\n",
        "\n",
        "            # Accumulate on CPU to save VRAM\n",
        "            _accumulated_act_norms[layer_idx] += act_norms_L2.cpu()\n",
        "\n",
        "        return hook\n",
        "\n",
        "    # Register hooks\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        handle = layer.mlp.down_proj.register_forward_hook(\n",
        "            make_hook(idx)\n",
        "        )\n",
        "        handles.append(handle)\n",
        "\n",
        "    print(f\"✓ Registered {len(handles)} hooks on down_proj\")\n",
        "\n",
        "    return handles\n",
        "\n",
        "def get_activation_norms():\n",
        "    \"\"\"\n",
        "    Returns the accumulated L2 norms in a format ready for pruning.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, torch.Tensor]: {layer_idx: norms_L2 [intermediate_size]}\n",
        "    \"\"\"\n",
        "    return {\n",
        "        layer_idx: norms.clone()\n",
        "        for layer_idx, norms in _accumulated_act_norms.items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b68596f",
      "metadata": {
        "id": "4b68596f"
      },
      "source": [
        "## Hybrid Importance Scoring"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_neuron_pair_importance(gate_weight, up_weight, down_weight, X_d_norm):\n",
        "    \"\"\"\n",
        "    Hybrid CFSP-inspired importance: Static magnitude + Dynamic activation\n",
        "    \"\"\"\n",
        "    gate_weight = gate_weight.float()\n",
        "    up_weight = up_weight.float()\n",
        "    down_weight = down_weight.float()\n",
        "    X_d_norm = X_d_norm.float().to(gate_weight.device)\n",
        "\n",
        "    # Static component (L2 norms)\n",
        "    gate_score = torch.norm(gate_weight, p=2, dim=1)\n",
        "    up_score = torch.norm(up_weight, p=2, dim=1)\n",
        "    down_score = torch.norm(down_weight, p=2, dim=0)\n",
        "\n",
        "    # Normalize to [0, 1] to equalize scales\n",
        "    gate_norm = gate_score / (gate_score.max() + 1e-8)\n",
        "    up_norm = up_score / (up_score.max() + 1e-8)\n",
        "    down_norm = down_score / (down_score.max() + 1e-8)\n",
        "\n",
        "    # Weighted combination (down_proj gets more weight)\n",
        "    #structural_score = 0.4 * down_norm + 0.4 * gate_norm + 0.2 * up_norm\n",
        "    structural_score = down_norm + gate_norm + up_norm\n",
        "\n",
        "    # Dynamic fusion (multiply by actual activations)\n",
        "    importance_scores = structural_score * X_d_norm\n",
        "\n",
        "    return importance_scores"
      ],
      "metadata": {
        "id": "DCB9XpBXmV-b"
      },
      "id": "DCB9XpBXmV-b",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4a89c3a0",
      "metadata": {
        "id": "4a89c3a0"
      },
      "source": [
        "## Neuron Pair Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "bdc89ded",
      "metadata": {
        "id": "bdc89ded"
      },
      "outputs": [],
      "source": [
        "def prune_neuron_pairs(mlp, prune_percent, X_d_norm, layer_idx):\n",
        "    \"\"\"\n",
        "    Prunes neuron pairs from MLP block using hybrid importance scores.\n",
        "\n",
        "    Reduces dimensions of gate_proj, up_proj, and down_proj layers.\n",
        "\n",
        "    Args:\n",
        "        mlp: LlamaMLP module to prune\n",
        "        prune_percent: Fraction of neurons to remove (e.g., 0.2 for 20%)\n",
        "        X_d_norm: Tensor [intermediate_size] with accumulated L2 norms\n",
        "        layer_idx: Layer index (for logging)\n",
        "\n",
        "    Returns:\n",
        "        new_gate_proj, new_up_proj, new_down_proj, k (new intermediate size)\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract weights\n",
        "    gate_weight = mlp.gate_proj.weight.data\n",
        "    up_weight = mlp.up_proj.weight.data\n",
        "    down_weight = mlp.down_proj.weight.data\n",
        "\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "\n",
        "    # Compute importance scores\n",
        "    importance_scores = compute_neuron_pair_importance(\n",
        "        gate_weight=gate_weight,\n",
        "        up_weight=up_weight,\n",
        "        down_weight=down_weight,\n",
        "        X_d_norm=X_d_norm\n",
        "    )\n",
        "\n",
        "    # Determine how many neurons to keep\n",
        "    num_to_prune = min(\n",
        "        int(prune_percent * original_intermediate_size),\n",
        "        original_intermediate_size - 1\n",
        "    )\n",
        "    k = original_intermediate_size - num_to_prune\n",
        "\n",
        "    if k <= 0:\n",
        "        raise ValueError(f\"Invalid k={k} for layer {layer_idx}\")\n",
        "\n",
        "    # Select top-k most important neurons\n",
        "    _, indices_to_keep = torch.topk(\n",
        "        importance_scores,\n",
        "        k,\n",
        "        largest=True,\n",
        "        sorted=True\n",
        "    )\n",
        "\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    # Create new pruned layers\n",
        "    new_gate_proj = nn.Linear(\n",
        "        mlp.gate_proj.in_features,\n",
        "        k,\n",
        "        bias=False\n",
        "    ).to(device)\n",
        "\n",
        "    new_up_proj = nn.Linear(\n",
        "        mlp.up_proj.in_features,\n",
        "        k,\n",
        "        bias=False\n",
        "    ).to(device)\n",
        "\n",
        "    new_down_proj = nn.Linear(\n",
        "        k,\n",
        "        mlp.down_proj.out_features,\n",
        "        bias=False\n",
        "    ).to(device)\n",
        "\n",
        "    # Copy weights for kept neurons\n",
        "    new_gate_proj.weight.data = gate_weight[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = up_weight[indices_to_keep, :]\n",
        "    new_down_proj.weight.data = down_weight[:, indices_to_keep]\n",
        "\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c50c139c",
      "metadata": {
        "id": "c50c139c"
      },
      "outputs": [],
      "source": [
        "def update_model(model, prune_percent, activation_norms):\n",
        "    \"\"\"\n",
        "    Applies pruning to all MLP layers in the model.\n",
        "\n",
        "    Args:\n",
        "        model: LlamaForCausalLM model to prune\n",
        "        prune_percent: Fraction of neurons to remove\n",
        "        activation_norms: Dict mapping layer_idx -> X_d_norm tensor\n",
        "\n",
        "    Returns:\n",
        "        model: Pruned model with updated configuration\n",
        "    \"\"\"\n",
        "\n",
        "    new_intermediate_size = None\n",
        "    pruning_stats = []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting pruning with {prune_percent*100:.1f}% width pruning\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        if idx not in activation_norms:\n",
        "            raise KeyError(f\"No activation norms for layer {idx}\")\n",
        "\n",
        "        X_d_norm = activation_norms[idx]\n",
        "        original_size = mlp.gate_proj.out_features\n",
        "\n",
        "        # Prune neuron pairs\n",
        "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(\n",
        "            mlp=mlp,\n",
        "            prune_percent=prune_percent,\n",
        "            X_d_norm=X_d_norm,\n",
        "            layer_idx=idx\n",
        "        )\n",
        "\n",
        "        # Replace layers\n",
        "        mlp.gate_proj = new_gate_proj\n",
        "        mlp.up_proj = new_up_proj\n",
        "        mlp.down_proj = new_down_proj\n",
        "\n",
        "        pruning_stats.append({\n",
        "            'layer': idx,\n",
        "            'original_size': original_size,\n",
        "            'new_size': new_size,\n",
        "            'pruned': original_size - new_size,\n",
        "            'kept_percent': (new_size / original_size) * 100\n",
        "        })\n",
        "\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "        if (idx + 1) % 4 == 0:\n",
        "            print(f\"  Pruned layers {idx-3:2d}-{idx:2d}: \"\n",
        "                  f\"{original_size} → {new_size} neurons \"\n",
        "                  f\"({(new_size/original_size)*100:.1f}% kept)\")\n",
        "\n",
        "    # Update model configuration\n",
        "    model.config.intermediate_size = new_intermediate_size\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Pruning completed!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Layers pruned: {len(pruning_stats)}\")\n",
        "    print(f\"  Original intermediate size: {original_size}\")\n",
        "    print(f\"  New intermediate size: {new_intermediate_size}\")\n",
        "    print(f\"  Neurons pruned per layer: {original_size - new_intermediate_size}\")\n",
        "    print(f\"  Effective width pruning: {((original_size - new_intermediate_size) / original_size) * 100:.2f}%\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23407a38",
      "metadata": {
        "id": "23407a38"
      },
      "source": [
        "# Calibration and Pruning\n",
        "\n",
        "Now we'll create two pruned models using different calibration datasets:\n",
        "1. **Wiki-pruned**: Calibrated on WikiText-2\n",
        "2. **SMS-pruned**: Calibrated on SMS Spam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81ca44a",
      "metadata": {
        "id": "c81ca44a"
      },
      "source": [
        "## Wiki-Calibrated Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4a46f797",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a46f797",
        "outputId": "d72c5341-f876-401b-8fa3-f110dd56d476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "WIKI CALIBRATION\n",
            "============================================================\n",
            "\n",
            "Setting up activation hooks...\n",
            "✓ Registered 16 hooks on down_proj\n",
            "\n",
            "Running calibration forward passes on WikiText...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Wiki Calibration: 100%|██████████| 25/25 [00:15<00:00,  1.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Processed 25 batches\n",
            "Removing hooks...\n",
            "Extracting activation statistics...\n",
            "✓ Collected activation norms for 16 layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"WIKI CALIBRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Setup hooks\n",
        "print(\"\\nSetting up activation hooks...\")\n",
        "handles_wiki = setup_mlp_hooks_for_importance(model, device)\n",
        "\n",
        "# Step 2: Run calibration forward passes\n",
        "print(\"\\nRunning calibration forward passes on WikiText...\")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloaderwiki, desc=\"Wiki Calibration\")):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(device),\n",
        "            'attention_mask': batch['attention_mask'].to(device)\n",
        "        }\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n✓ Processed {len(dataloaderwiki)} batches\")\n",
        "\n",
        "# Step 3: Clean up hooks\n",
        "print(\"Removing hooks...\")\n",
        "for handle in handles_wiki:\n",
        "    handle.remove()\n",
        "\n",
        "# Step 4: Get activation norms\n",
        "print(\"Extracting activation statistics...\")\n",
        "activation_norms_wiki = get_activation_norms()\n",
        "print(f\"✓ Collected activation norms for {len(activation_norms_wiki)} layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fdf2adac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf2adac",
        "outputId": "7e8134dc-ac77-4a68-a4e1-64e9f7b3eb51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Starting pruning with 20.0% width pruning\n",
            "============================================================\n",
            "\n",
            "  Pruned layers  0- 3: 8192 → 6554 neurons (80.0% kept)\n",
            "  Pruned layers  4- 7: 8192 → 6554 neurons (80.0% kept)\n",
            "  Pruned layers  8-11: 8192 → 6554 neurons (80.0% kept)\n",
            "  Pruned layers 12-15: 8192 → 6554 neurons (80.0% kept)\n",
            "\n",
            "============================================================\n",
            "Pruning completed!\n",
            "============================================================\n",
            "  Layers pruned: 16\n",
            "  Original intermediate size: 8192\n",
            "  New intermediate size: 6554\n",
            "  Neurons pruned per layer: 1638\n",
            "  Effective width pruning: 20.00%\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prune the model using Wiki activations\n",
        "wiki_model = update_model(copy.deepcopy(model), PRUNE_PERCENT, activation_norms_wiki)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(model)\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "7-6LdtLFb76H"
      },
      "id": "7-6LdtLFb76H",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "7b9a1389",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b9a1389",
        "outputId": "88b9b721-8d6e-44bb-fe66-9ed307664f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wiki-pruned model: Paris is the capital of France and is located in the north-east of the country. The city has a population of 2.1 million people, making it the 3rd largest city in France. It is also the largest in terms of surface area, with 100\n"
          ]
        }
      ],
      "source": [
        "# Test wiki model\n",
        "generated_wiki = generate_text(wiki_model, tokenizer, prompt, device)\n",
        "print(f\"Wiki-pruned model: {generated_wiki}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear_gpu_cache()\n",
        "metrics_wiki_wiki = evaluate_metrics(wiki_model, dataloaderwiki)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OULoO1Xy3g8z",
        "outputId": "6882dd55-f015-455f-c2af-2e46a04ed09f"
      },
      "id": "OULoO1Xy3g8z",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 25/25 [00:16<00:00,  1.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_wiki_sms = evaluate_metrics(wiki_model, dataloadersms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmpv66mn4ng6",
        "outputId": "64c452a4-85b7-4859-c308-0baa8c69372a"
      },
      "id": "xmpv66mn4ng6",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 25/25 [00:16<00:00,  1.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_wiki_timing = measure_detailed_performance(wiki_model, tokenizer, dataloaderwiki, max_samples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgr767nUWWuw",
        "outputId": "49f16de0-fbe1-4c47-a29e-d7f72ada3812"
      },
      "id": "dgr767nUWWuw",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring performance on 10 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performance test: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_params = count_parameters(wiki_model)"
      ],
      "metadata": {
        "id": "feAJux7Q4fsM"
      },
      "id": "feAJux7Q4fsM",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_sms_timing = measure_detailed_performance(wiki_model, tokenizer, dataloadersms, max_samples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5_otkEIZKTZ",
        "outputId": "a7f92555-8498-4245-96a2-92abdcb19d8b"
      },
      "id": "a5_otkEIZKTZ",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring performance on 10 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performance test: 100%|██████████| 10/10 [00:38<00:00,  3.85s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(wiki_model)\n",
        "clear_gpu_cache"
      ],
      "metadata": {
        "id": "XS9W2Av-hRWE",
        "outputId": "115fc62f-6702-435d-c4aa-0fdc6d1aeec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "id": "XS9W2Av-hRWE",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function utils.clear_gpu_cache()>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>utils.clear_gpu_cache</b><br/>def clear_gpu_cache()</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/utils.py</a>Clear GPU cache completely</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 18);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea2ece40",
      "metadata": {
        "id": "ea2ece40"
      },
      "source": [
        "## SMS-Calibrated Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24Wi1HfzcCEb",
        "outputId": "e0550b32-012e-4ba4-83a6-1a369de3ab9b"
      },
      "id": "24Wi1HfzcCEb",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2004a2c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2004a2c6",
        "outputId": "be0d766b-1852-4cae-c8d1-1d28a7d0ff87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SMS CALIBRATION\n",
            "============================================================\n",
            "\n",
            "Setting up activation hooks...\n",
            "✓ Registered 16 hooks on down_proj\n",
            "\n",
            "Running calibration forward passes on SMS...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMS Calibration: 100%|██████████| 25/25 [00:15<00:00,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Processed 25 batches\n",
            "Removing hooks...\n",
            "Extracting activation statistics...\n",
            "✓ Collected activation norms for 16 layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Clear GPU cache before SMS calibration\n",
        "clear_gpu_cache()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SMS CALIBRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Setup hooks\n",
        "print(\"\\nSetting up activation hooks...\")\n",
        "handles_sms = setup_mlp_hooks_for_importance(model, device)\n",
        "\n",
        "# Step 2: Run calibration forward passes\n",
        "print(\"\\nRunning calibration forward passes on SMS...\")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloadersms, desc=\"SMS Calibration\")):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(device),\n",
        "            'attention_mask': batch['attention_mask'].to(device)\n",
        "        }\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n✓ Processed {len(dataloadersms)} batches\")\n",
        "\n",
        "# Step 3: Clean up hooks\n",
        "print(\"Removing hooks...\")\n",
        "for handle in handles_sms:\n",
        "    handle.remove()\n",
        "\n",
        "# Step 4: Get activation norms\n",
        "print(\"Extracting activation statistics...\")\n",
        "activation_norms_sms = get_activation_norms()\n",
        "print(f\"✓ Collected activation norms for {len(activation_norms_sms)} layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "08fd3aaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fd3aaa",
        "outputId": "411d4a68-cf93-4f6b-8d24-35a981899307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Starting pruning with 20.0% width pruning\n",
            "============================================================\n",
            "\n",
            "  Pruned layers  0- 3: 8192 → 6554 neurons (80.0% kept)\n",
            "  Pruned layers  4- 7: 8192 → 6554 neurons (80.0% kept)\n",
            "  Pruned layers  8-11: 8192 → 6554 neurons (80.0% kept)\n",
            "  Pruned layers 12-15: 8192 → 6554 neurons (80.0% kept)\n",
            "\n",
            "============================================================\n",
            "Pruning completed!\n",
            "============================================================\n",
            "  Layers pruned: 16\n",
            "  Original intermediate size: 8192\n",
            "  New intermediate size: 6554\n",
            "  Neurons pruned per layer: 1638\n",
            "  Effective width pruning: 20.00%\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prune the model using SMS activations\n",
        "sms_model = update_model(copy.deepcopy(model), PRUNE_PERCENT, activation_norms_sms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(model)\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "NIA_o3VEuXWg"
      },
      "id": "NIA_o3VEuXWg",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ff076cdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff076cdb",
        "outputId": "d5ff926f-f857-4ba2-f438-4c887be019c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMS-pruned model: Paris is the capital of the French department of Paris. It is located on the Seine River, which flows through the city. The city has a population of 1.8 million people. Paris is a major city in the world. Its population is estimated to be \n"
          ]
        }
      ],
      "source": [
        "# Test SMS model\n",
        "generated_sms = generate_text(sms_model, tokenizer, prompt, device)\n",
        "print(f\"SMS-pruned model: {generated_sms}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_sms_wiki = evaluate_metrics(sms_model, dataloaderwiki)\n",
        "metrics_sms_sms = evaluate_metrics(sms_model, dataloadersms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IUiiAIGYs0y",
        "outputId": "515750d4-4b00-4903-e40a-f07454990ce1"
      },
      "id": "7IUiiAIGYs0y",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 25/25 [00:16<00:00,  1.49it/s]\n",
            "Evaluating: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sms_wiki_timing = measure_detailed_performance(sms_model, tokenizer, dataloaderwiki, max_samples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSLgyzMPWeGz",
        "outputId": "7d69ab40-53b7-4a9c-e0d1-273dbcb02f4a"
      },
      "id": "iSLgyzMPWeGz",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring performance on 10 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerformance test:   0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  10%|█         | 1/10 [00:04<00:37,  4.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  20%|██        | 2/10 [00:08<00:33,  4.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  30%|███       | 3/10 [00:12<00:29,  4.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  40%|████      | 4/10 [00:16<00:24,  4.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  50%|█████     | 5/10 [00:17<00:15,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  60%|██████    | 6/10 [00:22<00:13,  3.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  70%|███████   | 7/10 [00:26<00:11,  3.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  80%|████████  | 8/10 [00:30<00:07,  3.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  90%|█████████ | 9/10 [00:34<00:03,  3.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test: 100%|██████████| 10/10 [00:36<00:00,  3.62s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sms_sms_timing = measure_detailed_performance(sms_model, tokenizer, dataloadersms, max_samples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3W4pWW-ZaDs",
        "outputId": "b6f4cc28-6daa-4b7d-f33e-e1cd8cee0b4d"
      },
      "id": "T3W4pWW-ZaDs",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring performance on 10 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerformance test:   0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  10%|█         | 1/10 [00:04<00:37,  4.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  20%|██        | 2/10 [00:08<00:33,  4.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  30%|███       | 3/10 [00:12<00:29,  4.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  40%|████      | 4/10 [00:16<00:24,  4.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  50%|█████     | 5/10 [00:20<00:20,  4.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  60%|██████    | 6/10 [00:25<00:16,  4.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  70%|███████   | 7/10 [00:29<00:12,  4.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  80%|████████  | 8/10 [00:33<00:08,  4.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test:  90%|█████████ | 9/10 [00:37<00:04,  4.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Performance test: 100%|██████████| 10/10 [00:41<00:00,  4.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca50b815",
      "metadata": {
        "id": "ca50b815"
      },
      "source": [
        "## Parameter Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6838f398",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6838f398",
        "outputId": "3c60a5c1-82be-4f83-c2c2-02544453890b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PARAMETER COUNTS\n",
            "============================================================\n",
            "Original model:     1,235,814,400 parameters\n",
            "Wiki-pruned model:  1,074,792,448 parameters (13.03% reduction)\n",
            "SMS-pruned model:   1,074,792,448 parameters (13.03% reduction)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "sms_params = count_parameters(sms_model)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PARAMETER COUNTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original model:     {original_params:,} parameters\")\n",
        "print(f\"Wiki-pruned model:  {wiki_params:,} parameters ({((original_params - wiki_params) / original_params * 100):.2f}% reduction)\")\n",
        "print(f\"SMS-pruned model:   {sms_params:,} parameters ({((original_params - sms_params) / original_params * 100):.2f}% reduction)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ecc9f21",
      "metadata": {
        "id": "2ecc9f21"
      },
      "source": [
        "# Cross-Evaluation Matrix\n",
        "\n",
        "Now we evaluate all three models (base, wiki-pruned, sms-pruned) on both datasets (Wiki and SMS) to understand:\n",
        "- How well each pruning strategy preserves quality on its calibration dataset\n",
        "- How well pruning decisions transfer to the other dataset\n",
        "- Whether domain-specific calibration provides meaningful benefits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b08735ed",
      "metadata": {
        "id": "b08735ed"
      },
      "outputs": [],
      "source": [
        "# Clear original model to save memory\n",
        "clear_gpu_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476741ba",
      "metadata": {
        "id": "476741ba"
      },
      "source": [
        "## Loss and Perplexity Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "85728cd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85728cd8",
        "outputId": "6bb97a3f-8a30-4bb5-81db-3de3c1a68479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATING ON WIKITEXT-2\n",
            "============================================================\n",
            "\n",
            "Base model on Wiki: {'loss': 3.2460288122350294, 'perplexity': np.float64(25.688124727046315)}\n",
            "Wiki-pruned on Wiki: {'loss': 3.5913135137696126, 'perplexity': np.float64(36.28170115548473)}\n",
            "SMS-pruned on Wiki: {'loss': 3.8846293323963836, 'perplexity': np.float64(48.64890654342502)}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING ON WIKITEXT-2\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nBase model on Wiki:\", metrics_base_wiki)\n",
        "print(\"Wiki-pruned on Wiki:\", metrics_wiki_wiki)\n",
        "print(\"SMS-pruned on Wiki:\", metrics_sms_wiki)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "8f400dde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f400dde",
        "outputId": "d4ce7adb-936e-4647-c295-ec2abb3a50a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATING ON SMS SPAM\n",
            "============================================================\n",
            "\n",
            "Base model on SMS: {'loss': 4.8114213257195955, 'perplexity': np.float64(122.90618314977401)}\n",
            "Wiki-pruned on SMS: {'loss': 5.206973014926048, 'perplexity': np.float64(182.540673179065)}\n",
            "SMS-pruned on SMS: {'loss': 5.109189337836538, 'perplexity': np.float64(165.53610660777312)}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING ON SMS SPAM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nBase model on SMS:\", metrics_base_sms)\n",
        "print(\"Wiki-pruned on SMS:\", metrics_wiki_sms)\n",
        "print(\"SMS-pruned on SMS:\", metrics_sms_sms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1lmP_HOg4iaj"
      },
      "id": "1lmP_HOg4iaj"
    },
    {
      "cell_type": "markdown",
      "id": "494cfab0",
      "metadata": {
        "id": "494cfab0"
      },
      "source": [
        "## Text Generation Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5c542cd",
      "metadata": {
        "id": "c5c542cd"
      },
      "source": [
        "## Performance Measurement (Inference Speed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "abe65cb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abe65cb0",
        "outputId": "b6d9e971-d94f-44aa-9748-a11f3c0cd2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PERFORMANCE MEASUREMENT ON WIKI\n",
            "============================================================\n",
            "\n",
            "Base model:       0.6791s (29.74 tok/s)\n",
            "Wiki-pruned:      1.3088s (35.45 tok/s)\n",
            "SMS-pruned:       1.2069s (34.88 tok/s)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE MEASUREMENT ON WIKI\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(f\"\\nBase model:       {base_wiki_timing['avg_time_sec']:.4f}s ({base_wiki_timing['tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"Wiki-pruned:      {wiki_wiki_timing['avg_time_sec']:.4f}s ({wiki_wiki_timing['tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"SMS-pruned:       {sms_wiki_timing['avg_time_sec']:.4f}s ({sms_wiki_timing['tokens_per_sec']:.2f} tok/s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "bd0f5545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd0f5545",
        "outputId": "827ef3c0-096d-4aa4-e950-782a0000c5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PERFORMANCE MEASUREMENT ON SMS\n",
            "============================================================\n",
            "\n",
            "Base model:       1.0957s (34.32 tok/s)\n",
            "Wiki-pruned:      1.2819s (35.88 tok/s)\n",
            "SMS-pruned:       1.3891s (35.99 tok/s)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE MEASUREMENT ON SMS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nBase model:       {base_sms_timing['avg_time_sec']:.4f}s ({base_sms_timing['tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"Wiki-pruned:      {wiki_sms_timing['avg_time_sec']:.4f}s ({wiki_sms_timing['tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"SMS-pruned:       {sms_sms_timing['avg_time_sec']:.4f}s ({sms_sms_timing['tokens_per_sec']:.2f} tok/s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29882f75",
      "metadata": {
        "id": "29882f75"
      },
      "source": [
        "# Summary and Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5ea26207",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ea26207",
        "outputId": "972d349c-d739-46c2-8563-93919363adc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE RESULTS\n",
            "================================================================================\n",
            "      Model  Parameters  Param Reduction %  PPL Wiki    PPL SMS  Loss Wiki  Loss SMS  Time Wiki (s)  Time SMS (s)  Tok/s Wiki  Tok/s SMS\n",
            "       Base  1235814400           0.000000 25.688125 122.906183   3.246029  4.811421       0.679144      1.095701   29.743339  34.315922\n",
            "Wiki-pruned  1074792448          13.029623 36.281701 182.540673   3.591314  5.206973       1.308820      1.281878   35.451768  35.884851\n",
            " SMS-pruned  1074792448          13.029623 48.648907 165.536107   3.884629  5.109189       1.206911      1.389136   34.882451  35.993593\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create comprehensive results table\n",
        "results = {\n",
        "    'Model': ['Base', 'Wiki-pruned', 'SMS-pruned'],\n",
        "    'Parameters': [original_params, wiki_params, sms_params],\n",
        "    'Param Reduction %': [\n",
        "        0,\n",
        "        ((original_params - wiki_params) / original_params * 100),\n",
        "        ((original_params - sms_params) / original_params * 100)\n",
        "    ],\n",
        "    'PPL Wiki': [\n",
        "        metrics_base_wiki['perplexity'],\n",
        "        metrics_wiki_wiki['perplexity'],\n",
        "        metrics_sms_wiki['perplexity']\n",
        "    ],\n",
        "    'PPL SMS': [\n",
        "        metrics_base_sms['perplexity'],\n",
        "        metrics_wiki_sms['perplexity'],\n",
        "        metrics_sms_sms['perplexity']\n",
        "    ],\n",
        "    'Loss Wiki': [\n",
        "        metrics_base_wiki['loss'],\n",
        "        metrics_wiki_wiki['loss'],\n",
        "        metrics_sms_wiki['loss']\n",
        "    ],\n",
        "    'Loss SMS': [\n",
        "        metrics_base_sms['loss'],\n",
        "        metrics_wiki_sms['loss'],\n",
        "        metrics_sms_sms['loss']\n",
        "    ],\n",
        "    'Time Wiki (s)': [\n",
        "        base_wiki_timing['avg_time_sec'],\n",
        "        wiki_wiki_timing['avg_time_sec'],\n",
        "        sms_wiki_timing['avg_time_sec']\n",
        "    ],\n",
        "    'Time SMS (s)': [\n",
        "        base_sms_timing['avg_time_sec'],\n",
        "        wiki_sms_timing['avg_time_sec'],\n",
        "        sms_sms_timing['avg_time_sec']\n",
        "    ],\n",
        "    'Tok/s Wiki': [\n",
        "        base_wiki_timing['tokens_per_sec'],\n",
        "        wiki_wiki_timing['tokens_per_sec'],\n",
        "        sms_wiki_timing['tokens_per_sec']\n",
        "    ],\n",
        "    'Tok/s SMS': [\n",
        "        base_sms_timing['tokens_per_sec'],\n",
        "        wiki_sms_timing['tokens_per_sec'],\n",
        "        sms_sms_timing['tokens_per_sec']\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(df_results.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_base)\n",
        "print(generated_wiki)\n",
        "print(generated_sms)"
      ],
      "metadata": {
        "id": "MNd22xaeGyfp",
        "outputId": "753e8fa3-7833-4b67-fb7e-26f589635de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MNd22xaeGyfp",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a rich history and culture, and it is home to many famous landmarks, including the E\n",
            "Paris is the capital of France and is located in the north-east of the country. The city has a population of 2.1 million people, making it the 3rd largest city in France. It is also the largest in terms of surface area, with 100\n",
            "Paris is the capital of the French department of Paris. It is located on the Seine River, which flows through the city. The city has a population of 1.8 million people. Paris is a major city in the world. Its population is estimated to be \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40% pruning expansion rate.\n",
        "```\n",
        "================================================================================\n",
        "COMPREHENSIVE RESULTS\n",
        "================================================================================\n",
        "      Model  Parameters  Param Reduction %  PPL Wiki    PPL SMS  Loss Wiki  Loss SMS  Time Wiki (s)  Time SMS (s)  Tok/s Wiki  Tok/s SMS\n",
        "       Base  1235814400           0.000000 25.688125 122.906183   3.246029  4.811421       0.679144      1.095701   29.743339  34.315922\n",
        "Wiki-pruned  1074792448          13.029623 36.281701 182.540673   3.591314  5.206973       1.308820      1.281878   35.451768  35.884851\n",
        " SMS-pruned  1074792448          13.029623 48.648907 165.536107   3.884629  5.109189       1.206911      1.389136   34.882451  35.993593\n",
        "================================================================================\n",
        "````\n"
      ],
      "metadata": {
        "id": "kiiX68wPlyUu"
      },
      "id": "kiiX68wPlyUu"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMA23Yq5lwen"
      },
      "id": "LMA23Yq5lwen",
      "execution_count": 48,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}