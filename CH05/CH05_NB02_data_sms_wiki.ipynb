{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3356d42",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH05/CH05_NB02_data_sms_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m6q6m0d2wlp",
   "metadata": {},
   "source": [
    "# Rearchitecting LLMs\n",
    "## Surgical Optimization for Hyper-Efficient Models\n",
    "\n",
    "\n",
    "### Chapter 5: Width Pruning\n",
    "### Notebook: 02. Data-Driven Neuron Selection\n",
    "by [Pere Martra](https://github.com/peremartra)\n",
    "\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white)](https://x.com/PereMartra) [![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
    "\n",
    "_____\n",
    "Colab Environment: GPU T4\n",
    "\n",
    "Models:\n",
    "* Llama-3.2-1B\n",
    "_____\n",
    "\n",
    "In this notebook, we explore **data-driven width pruning**â€”a more sophisticated approach to neural network compression that uses activation patterns from real data to make pruning decisions. Unlike the static magnitude-based pruning from Notebook 01, this method captures how neurons actually behave during inference.\n",
    "\n",
    "We implement a **CFSP-inspired hybrid importance scoring** that combines:\n",
    "- **Static component**: Weight magnitudes (what neurons *could* do)\n",
    "- **Dynamic component**: Activation norms from calibration data (what neurons *actually* do)\n",
    "\n",
    "The key experiment: We create two differently-pruned models from the same Llama-3.2-1B base, using contrasting calibration datasets:\n",
    "- **WikiText-2**: Long-form encyclopedic text with complex language patterns\n",
    "- **SMS Spam**: Short conversational messages with informal language\n",
    "\n",
    "By cross-evaluating all three models (base, wiki-pruned, sms-pruned) on both datasets, we answer a critical question: **Does the choice of calibration data influence which neurons get pruned, and does this create domain-specialized models?**\n",
    "\n",
    "By the end of this notebook, you'll understand how activation-aware pruning differs from static methods, why calibration dataset selection matters, and whether domain-specific pruning offers practical advantages over one-size-fits-all compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f98e07",
   "metadata": {
    "id": "92f98e07"
   },
   "source": [
    "# Setting up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade7571c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ade7571c",
    "outputId": "d27a3cf6-e21f-4b10-b431-0b876272aa3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \\\n",
    "      \"torch\" \\\n",
    "      \"transformers==4.55.4\" \\\n",
    "      \"accelerate==1.10.1\" \\\n",
    "      \"lm_eval==0.4.9.1\" \\\n",
    "      \"sentencepiece==0.2.1\" \\\n",
    "      \"datasets\" \\\n",
    "      \"langdetect\"\\\n",
    "      \"optipfair==0.2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050e0a5",
   "metadata": {
    "id": "5050e0a5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from lm_eval import evaluator\n",
    "from torch import nn\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import gc\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec5a44a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ec5a44a",
    "outputId": "4e11e1f0-a5a8-45a9-bc2c-5478a21e540f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d307df",
   "metadata": {
    "id": "c7d307df"
   },
   "source": [
    "Download helper functions from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36300712",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36300712",
    "outputId": "e7f2b098-f643-40e0-cd91-37cdcad19c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… utils.py downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Download utils.py from GitHub repository\n",
    "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
    "\n",
    "# Verify download\n",
    "import os\n",
    "if os.path.exists('utils.py'):\n",
    "    print(\"âœ… utils.py downloaded successfully\")\n",
    "else:\n",
    "    print(\"âŒ Failed to download utils.py\")\n",
    "\n",
    "from utils import (\n",
    "  model_evaluation, # Evals with lm_eval\n",
    "  evaluate_metrics, # Loss & Perpelexity\n",
    "  generate_text, #test inference model\n",
    "  clear_gpu_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lnRCe9yyUmbZ",
   "metadata": {
    "id": "lnRCe9yyUmbZ"
   },
   "outputs": [],
   "source": [
    "def measure_detailed_performance(model, tokenizer, data_source, num_runs=3, max_new_tokens=50, max_samples=None):\n",
    "    \"\"\"\n",
    "    Measures inference performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        tokenizer: Tokenizer\n",
    "        data_source: DataLoader to sample from\n",
    "        num_runs: Number of runs per sample for averaging\n",
    "        max_new_tokens: Tokens to generate\n",
    "        max_samples: Limit number of samples (None = all)\n",
    "\n",
    "    Returns:\n",
    "        dict with timing statistics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    tokens_generated = []\n",
    "\n",
    "    samples = []\n",
    "    for batch in data_source:\n",
    "        for i in range(len(batch['input_ids'])):\n",
    "            samples.append(batch['input_ids'][i])\n",
    "            if max_samples and len(samples) >= max_samples:\n",
    "                break\n",
    "        if max_samples and len(samples) >= max_samples:\n",
    "            break\n",
    "\n",
    "    if max_samples:\n",
    "        samples = samples[:max_samples]\n",
    "\n",
    "    print(f\"Measuring performance on {len(samples)} samples...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(samples, desc=\"Performance test\"):\n",
    "            input_ids = sample.unsqueeze(0).to(device)\n",
    "\n",
    "            for _ in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "                end_time = time.time()\n",
    "\n",
    "                elapsed = end_time - start_time\n",
    "                times.append(elapsed)\n",
    "                tokens_generated.append(outputs.shape[1] - input_ids.shape[1])\n",
    "\n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    avg_tokens = np.mean(tokens_generated)\n",
    "    tokens_per_sec = avg_tokens / avg_time if avg_time > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'avg_time_sec': avg_time,\n",
    "        'std_time_sec': std_time,\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'num_samples': len(samples),\n",
    "        'num_runs': num_runs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52aeb41",
   "metadata": {
    "id": "c52aeb41"
   },
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9065c773",
   "metadata": {
    "id": "9065c773"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = 'meta-llama/Llama-3.2-1B'\n",
    "\n",
    "# Dataset configuration\n",
    "RECOVERY_SAMPLES = 100  # Calibration samples per dataset\n",
    "MAX_LENGTH = 1024\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Pruning configuration\n",
    "PRUNE_PERCENT = 0.2  # 20% of neurons will be pruned\n",
    "\n",
    "# Generation configuration\n",
    "MAX_NEW_TOKENS = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a02967",
   "metadata": {
    "id": "48a02967"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_p = None\n",
    "model.generation_config.top_k = None\n",
    "\n",
    "print(f\"âœ“ Loaded {MODEL_NAME}\")\n",
    "print(f\"  Layers: {len(model.model.layers)}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Intermediate size: {model.config.intermediate_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7cc965",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef7cc965",
    "outputId": "d634d1e3-7cb2-4d8c-d65a-61cfd622490e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model generation: Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a rich history and culture, and it is home to many famous landmarks, including the E\n"
     ]
    }
   ],
   "source": [
    "# Test the original model\n",
    "prompt = \"Paris is the capital of\"\n",
    "generated_base = generate_text(model, tokenizer, prompt, device)\n",
    "print(f\"Base model generation: {generated_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a14d5",
   "metadata": {
    "id": "935a14d5"
   },
   "source": [
    "# Load and Prepare Calibration Datasets\n",
    "\n",
    "We'll load two contrasting datasets to serve as calibration sources for our pruning experiments:\n",
    "\n",
    "1. **WikiText-2**: Long-form, encyclopedic text with complex language patterns. This dataset represents formal, structured writing with varied vocabulary and sophisticated grammar. It's designed to test a model's ability to handle coherent, knowledge-rich narratives.\n",
    "\n",
    "2. **SMS Spam**: Short conversational messages with informal language. This dataset contains brief text messagesâ€”often with abbreviations, casual grammar, and simple sentence structures. It represents a completely different linguistic domain.\n",
    "\n",
    "**The Hypothesis**: Different datasets will activate different neurons during calibration. When we calculate importance scores based on actual activation patterns, neurons that fire strongly on WikiText might barely activate on SMS, and vice versa. This should produce two distinct pruning patternsâ€”and potentially, two specialized models.\n",
    "\n",
    "These datasets will serve as calibration sources for our two pruned models, allowing us to test whether domain-specific calibration creates measurably different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d44889",
   "metadata": {
    "id": "10d44889"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datawiki = load_dataset('wikitext', 'wikitext-2-raw-v1', split=f'train[:{RECOVERY_SAMPLES}]')\n",
    "datasms = load_dataset('sms_spam', split=f'train[:{RECOVERY_SAMPLES}]')\n",
    "\n",
    "print(f\"âœ“ WikiText samples: {len(datawiki)}\")\n",
    "print(f\"âœ“ SMS samples: {len(datasms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a909bea",
   "metadata": {
    "id": "5a909bea"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, text_field='text'):\n",
    "    \"\"\"\n",
    "    Tokenizes and prepares a dataset for calibration.\n",
    "\n",
    "    Handles different dataset formats (WikiText uses 'text', SMS uses 'sms' field).\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        if text_field in examples:\n",
    "            texts = examples[text_field]\n",
    "        elif 'sms' in examples:  # SMS dataset specific\n",
    "            texts = examples['sms']\n",
    "        elif 'text' in examples:\n",
    "            texts = examples['text']\n",
    "        else:\n",
    "            texts = examples[list(examples.keys())[0]]\n",
    "\n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    return DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f653f",
   "metadata": {
    "id": "736f653f"
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "dataloaderwiki = prepare_dataset(datawiki)\n",
    "dataloadersms = prepare_dataset(datasms)\n",
    "\n",
    "print(f\"âœ“ Created dataloaders\")\n",
    "print(f\"  Wiki batches: {len(dataloaderwiki)}\")\n",
    "print(f\"  SMS batches: {len(dataloadersms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "O48-PwbYrO4z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O48-PwbYrO4z",
    "outputId": "8bbf6d42-83cf-49fd-a284-decf8119eb41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_base_wiki = evaluate_metrics(model, dataloaderwiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3n_AMbL4sU0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3n_AMbL4sU0f",
    "outputId": "808138c2-bff5-4570-9547-a56c7eb8b0c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_base_sms = evaluate_metrics(model, dataloadersms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "akHFaKqOWRuj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akHFaKqOWRuj",
    "outputId": "2ee60603-73e0-452a-ddf6-c021aec808bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring performance on 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performance test:   0%|          | 0/10 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "base_wiki_timing = measure_detailed_performance(model, tokenizer, dataloaderwiki, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "WdMgItGZZQfJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdMgItGZZQfJ",
    "outputId": "e6bb28a1-f1e9-4a58-d252-69ea24125904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring performance on 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "base_sms_timing = measure_detailed_performance(model, tokenizer, dataloadersms, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "yxtr-quOwQuS",
   "metadata": {
    "id": "yxtr-quOwQuS"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "_0RBrAtxvYVq",
   "metadata": {
    "id": "_0RBrAtxvYVq"
   },
   "outputs": [],
   "source": [
    "original_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sxQHY7opwbi8",
   "metadata": {
    "id": "sxQHY7opwbi8"
   },
   "outputs": [],
   "source": [
    "clear_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f18a6",
   "metadata": {
    "id": "031f18a6"
   },
   "source": [
    "# Data-Driven Pruning Functions\n",
    "\n",
    "These functions implement the CFSP-inspired methodology adapted from contextual pruning research. The key innovation: instead of pruning based solely on weight magnitudes (static), we incorporate **runtime activation patterns** (dynamic) to make smarter pruning decisions.\n",
    "\n",
    "## Understanding the Approach\n",
    "\n",
    "**Static pruning** (Notebook 01) asks: \"Which neurons have the smallest weights?\"  \n",
    "**Data-driven pruning** (this notebook) asks: \"Which neurons have the smallest weights *and* barely activate on real data?\"\n",
    "\n",
    "This is accomplished through three stages:\n",
    "1. **Activation capture**: PyTorch hooks on down_proj to record runtime behavior\n",
    "2. **Hybrid importance**: Combines weight magnitudes with activation norms  \n",
    "3. **Neuron pair pruning**: Removes least important neurons from gate_proj, up_proj, and down_proj\n",
    "\n",
    "**Why down_proj?** In the GLU MLP architecture, down_proj receives the output of the gating mechanism:\n",
    "\n",
    "```\n",
    "X â†’ gate_proj â†’ SiLU(Â·) â”€â”€â”\n",
    "                          Ã— â†’ down_proj â†’ output\n",
    "X â†’ up_proj â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "The input to down_proj (X_d) represents the **post-gating activations**â€”the actual information that flows through each neuron after the gate decides what to pass. By measuring ||X_d||, we capture which neurons contribute meaningful activations during forward passes on our calibration data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ac494",
   "metadata": {
    "id": "161ac494"
   },
   "source": [
    "## Activation Capture with PyTorch Hooks\n",
    "\n",
    "**What are PyTorch hooks?** Hooks are callback functions that PyTorch executes automatically during forward or backward passes. They let us \"spy\" on intermediate activations without modifying the model's code.\n",
    "\n",
    "In our case, we register **forward hooks** on the `down_proj` layer of every MLP module. When data flows through the model during calibration, our hook function captures the input tensor to `down_proj` (X_d), calculates its L2 norm, and accumulates these norms across all batches.\n",
    "\n",
    "**Why accumulate?** A single batch might not represent typical activation patterns. By summing L2 norms across all calibration batches, we get a robust estimate of each neuron's average contribution. Neurons with consistently high ||X_d|| values are considered important and preserved; those with low norms are candidates for pruning.\n",
    "\n",
    "**Memory strategy**: We store accumulated norms on CPU rather than GPU to conserve VRAM, since we're processing the full model + calibration batches simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2030c79",
   "metadata": {
    "id": "b2030c79"
   },
   "outputs": [],
   "source": [
    "# Global storage for accumulated activation norms\n",
    "_accumulated_act_norms = {}\n",
    "\n",
    "def setup_mlp_hooks_for_importance(model, device):\n",
    "    \"\"\"\n",
    "    Registers hooks on down_proj inputs (X_d) to calculate L2 norms\n",
    "    for each neuron, following CFSP Equation 8.\n",
    "\n",
    "    This function sets up the activation capture mechanism. For each MLP layer,\n",
    "    we register a forward hook on down_proj that intercepts the input tensor\n",
    "    (X_d) during forward passes, computes its L2 norm across batch and sequence\n",
    "    dimensions, and accumulates these norms globally.\n",
    "\n",
    "    The accumulated norms represent each neuron's total contribution across\n",
    "    all calibration batches, providing a data-driven importance signal.\n",
    "\n",
    "    Args:\n",
    "        model: LlamaForCausalLM model to instrument with hooks\n",
    "        device: Device where model is running (used for initialization)\n",
    "\n",
    "    Returns:\n",
    "        handles: List of hook handles (call handle.remove() to unregister)\n",
    "    \"\"\"\n",
    "    global _accumulated_act_norms\n",
    "    _accumulated_act_norms.clear()\n",
    "\n",
    "    # Free memory before starting to avoid OOM errors during calibration\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    # Initialize storage on CPU to save VRAM during calibration\n",
    "    # Each layer gets a tensor of shape [intermediate_size] initialized to zeros\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        intermediate_size = layer.mlp.down_proj.in_features\n",
    "        _accumulated_act_norms[idx] = torch.zeros(\n",
    "            intermediate_size,\n",
    "            dtype=torch.float32,\n",
    "            device='cpu'  # CPU storage prevents GPU memory overflow\n",
    "        )\n",
    "\n",
    "    def make_hook(layer_idx):\n",
    "        \"\"\"\n",
    "        Factory function that creates a hook closure for a specific layer.\n",
    "        Each hook captures X_d and accumulates its L2 norm.\n",
    "        \"\"\"\n",
    "        def hook(module, input, output):\n",
    "            \"\"\"\n",
    "            Hook function called automatically during forward pass.\n",
    "\n",
    "            Captures X_d (input to down_proj) and calculates its L2 norm.\n",
    "            X_d shape: [batch_size, seq_len, intermediate_size]\n",
    "            Output: [intermediate_size] with ||X_d^i|| for each neuron i\n",
    "\n",
    "            The L2 norm is computed as: ||X_d^i|| = sqrt(sum((X_d[:,:,i])^2))\n",
    "            summed over batch and sequence dimensions.\n",
    "            \"\"\"\n",
    "            X_d = input[0].detach()  # [B, S, I] - input to down_proj\n",
    "\n",
    "            # Calculate L2 norm across batch and sequence dimensions\n",
    "            # This gives us a single importance score per neuron\n",
    "            act_norms_L2 = torch.norm(\n",
    "                X_d.to(torch.float32),\n",
    "                p=2,\n",
    "                dim=(0, 1)  # Sum over batch (0) and sequence (1) dimensions\n",
    "            )\n",
    "\n",
    "            # Accumulate on CPU to save VRAM - we'll use these later for pruning\n",
    "            _accumulated_act_norms[layer_idx] += act_norms_L2.cpu()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    # Register hooks on every MLP layer's down_proj\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        handle = layer.mlp.down_proj.register_forward_hook(\n",
    "            make_hook(idx)\n",
    "        )\n",
    "        handles.append(handle)\n",
    "\n",
    "    print(f\"âœ“ Registered {len(handles)} hooks on down_proj\")\n",
    "\n",
    "    return handles\n",
    "\n",
    "def get_activation_norms():\n",
    "    \"\"\"\n",
    "    Returns the accumulated L2 norms in a format ready for pruning.\n",
    "\n",
    "    After running calibration batches through the model with hooks enabled,\n",
    "    this function retrieves the accumulated activation norms for each layer.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, torch.Tensor]: {layer_idx: norms_L2 [intermediate_size]}\n",
    "            Each tensor contains importance scores for all neurons in that layer.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        layer_idx: norms.clone()\n",
    "        for layer_idx, norms in _accumulated_act_norms.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68596f",
   "metadata": {
    "id": "4b68596f"
   },
   "source": [
    "## Hybrid Importance Scoring\n",
    "\n",
    "The key innovation: we combine **static** and **dynamic** signals to make pruning decisions.\n",
    "\n",
    "**Static component**: Weight magnitude norms\n",
    "- `||W_gate||`, `||W_up||`, `||W_down||` measure potential neuron capacity\n",
    "- Large weights suggest a neuron *could* have significant impact\n",
    "- Fast to compute, but ignores actual runtime behavior\n",
    "\n",
    "**Dynamic component**: Activation norms from calibration data\n",
    "- `||X_d||` measures actual neuron contributions on real inputs\n",
    "- Captures which neurons *actually fire* for a given domain\n",
    "- Expensive to compute, but provides ground truth about neuron utility\n",
    "\n",
    "**Why both matter**: A neuron might have large weights but rarely activate (static overestimates importance), or have small weights but activate consistently (static underestimates importance). By multiplying the normalized static score by the activation norm, we get a hybrid metric that favors neurons that are both structurally significant *and* behaviorally active.\n",
    "\n",
    "**Normalization strategy**: We scale gate, up, and down norms to [0,1] to equalize their contributions, preventing any single component from dominating the importance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DCB9XpBXmV-b",
   "metadata": {
    "id": "DCB9XpBXmV-b"
   },
   "outputs": [],
   "source": [
    "def compute_neuron_pair_importance(gate_weight, up_weight, down_weight, X_d_norm):\n",
    "    \"\"\"\n",
    "    Hybrid CFSP-inspired importance: Static magnitude + Dynamic activation.\n",
    "\n",
    "    Combines weight magnitudes (structural capacity) with activation norms\n",
    "    (runtime behavior) to produce a single importance score per neuron.\n",
    "\n",
    "    Args:\n",
    "        gate_weight: Weight tensor from gate_proj [intermediate_size, hidden_size]\n",
    "        up_weight: Weight tensor from up_proj [intermediate_size, hidden_size]\n",
    "        down_weight: Weight tensor from down_proj [hidden_size, intermediate_size]\n",
    "        X_d_norm: Accumulated activation norms [intermediate_size]\n",
    "\n",
    "    Returns:\n",
    "        importance_scores: Hybrid importance scores [intermediate_size]\n",
    "    \"\"\"\n",
    "    # Convert all tensors to float32 for numerical stability\n",
    "    gate_weight = gate_weight.float()\n",
    "    up_weight = up_weight.float()\n",
    "    down_weight = down_weight.float()\n",
    "    X_d_norm = X_d_norm.float().to(gate_weight.device)\n",
    "\n",
    "    # Static component: Compute L2 norms of weight vectors for each neuron\n",
    "    # gate_proj and up_proj: each neuron is a row, so norm over dim=1\n",
    "    gate_score = torch.norm(gate_weight, p=2, dim=1)\n",
    "    up_score = torch.norm(up_weight, p=2, dim=1)\n",
    "    \n",
    "    # down_proj: each neuron is a column, so norm over dim=0\n",
    "    down_score = torch.norm(down_weight, p=2, dim=0)\n",
    "\n",
    "    # Normalize each component to [0, 1] range to equalize their scales\n",
    "    # This prevents any single component from dominating the importance score\n",
    "    gate_norm = gate_score / (gate_score.max() + 1e-8)\n",
    "    up_norm = up_score / (up_score.max() + 1e-8)\n",
    "    down_norm = down_score / (down_score.max() + 1e-8)\n",
    "\n",
    "    # Combine the three static components\n",
    "    # down_proj receives equal weight as it's the bottleneck where information contracts\n",
    "    # Alternative weighting: 0.4 * down_norm + 0.4 * gate_norm + 0.2 * up_norm\n",
    "    # Current approach: equal weighting for simplicity\n",
    "    structural_score = down_norm + gate_norm + up_norm\n",
    "\n",
    "    # Dynamic fusion: Multiply structural score by actual activation norms\n",
    "    # This ensures neurons that are both structurally significant AND\n",
    "    # active on calibration data receive high importance scores\n",
    "    importance_scores = structural_score * X_d_norm\n",
    "\n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89c3a0",
   "metadata": {
    "id": "4a89c3a0"
   },
   "source": [
    "## Neuron Pair Pruning\n",
    "\n",
    "With importance scores calculated, we now perform the actual surgery on the MLP layers.\n",
    "\n",
    "**Key insight**: In GLU architectures, gate_proj and up_proj work as a **neuron pair**â€”they operate on the same input and their outputs are combined via element-wise multiplication. Therefore, we must prune corresponding neurons from both projections simultaneously.\n",
    "\n",
    "The pruning process:\n",
    "1. **Rank neurons** by importance scores (lowest to highest)\n",
    "2. **Select top-k** most important neurons to keep\n",
    "3. **Create new smaller layers** with dimensions:\n",
    "   - gate_proj: [hidden_size â†’ k] instead of [hidden_size â†’ intermediate_size]\n",
    "   - up_proj: [hidden_size â†’ k]\n",
    "   - down_proj: [k â†’ hidden_size] instead of [intermediate_size â†’ hidden_size]\n",
    "4. **Copy weights** for only the kept neurons\n",
    "\n",
    "**Dimension mapping**:\n",
    "- gate_proj and up_proj: Neurons are **rows** (index with `weight[neuron_idx, :]`)\n",
    "- down_proj: Neurons are **columns** (index with `weight[:, neuron_idx]`)\n",
    "\n",
    "This ensures the pruned MLP maintains compatibility with the rest of the modelâ€”the input and output dimensions remain unchanged at `hidden_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc89ded",
   "metadata": {
    "id": "bdc89ded"
   },
   "outputs": [],
   "source": [
    "def prune_neuron_pairs(mlp, prune_percent, X_d_norm, layer_idx):\n",
    "    \"\"\"\n",
    "    Prunes neuron pairs from MLP block using hybrid importance scores.\n",
    "\n",
    "    Reduces dimensions of gate_proj, up_proj, and down_proj layers by removing\n",
    "    the least important neurons based on the combined static (weight magnitude)\n",
    "    and dynamic (activation norm) scoring.\n",
    "\n",
    "    Args:\n",
    "        mlp: LlamaMLP module to prune\n",
    "        prune_percent: Fraction of neurons to remove (e.g., 0.2 for 20%)\n",
    "        X_d_norm: Tensor [intermediate_size] with accumulated L2 activation norms\n",
    "        layer_idx: Layer index (for logging/debugging)\n",
    "\n",
    "    Returns:\n",
    "        new_gate_proj: Pruned gate projection layer\n",
    "        new_up_proj: Pruned up projection layer\n",
    "        new_down_proj: Pruned down projection layer\n",
    "        k: New intermediate size (number of neurons kept)\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract weights from the three MLP projection layers\n",
    "    gate_weight = mlp.gate_proj.weight.data\n",
    "    up_weight = mlp.up_proj.weight.data\n",
    "    down_weight = mlp.down_proj.weight.data\n",
    "\n",
    "    original_intermediate_size = gate_weight.size(0)\n",
    "\n",
    "    # Compute hybrid importance scores (static + dynamic)\n",
    "    importance_scores = compute_neuron_pair_importance(\n",
    "        gate_weight=gate_weight,\n",
    "        up_weight=up_weight,\n",
    "        down_weight=down_weight,\n",
    "        X_d_norm=X_d_norm\n",
    "    )\n",
    "\n",
    "    # Calculate how many neurons to keep (k)\n",
    "    # Ensure we keep at least 1 neuron to avoid zero-dimension tensors\n",
    "    num_to_prune = min(\n",
    "        int(prune_percent * original_intermediate_size),\n",
    "        original_intermediate_size - 1\n",
    "    )\n",
    "    k = original_intermediate_size - num_to_prune\n",
    "\n",
    "    if k <= 0:\n",
    "        raise ValueError(f\"Invalid k={k} for layer {layer_idx}\")\n",
    "\n",
    "    # Select top-k most important neurons to keep\n",
    "    # topk returns (values, indices) - we only need indices\n",
    "    _, indices_to_keep = torch.topk(\n",
    "        importance_scores,\n",
    "        k,\n",
    "        largest=True,  # Keep highest importance scores\n",
    "        sorted=True\n",
    "    )\n",
    "\n",
    "    # Sort indices to maintain neuron ordering (optional but cleaner)\n",
    "    # This ensures weight matrices remain organized even after pruning\n",
    "    indices_to_keep = indices_to_keep.sort().values\n",
    "\n",
    "    # Create new pruned linear layers with reduced dimensions\n",
    "    new_gate_proj = nn.Linear(\n",
    "        mlp.gate_proj.in_features,  # Input: hidden_size (unchanged)\n",
    "        k,                          # Output: reduced intermediate_size\n",
    "        bias=False\n",
    "    ).to(device)\n",
    "\n",
    "    new_up_proj = nn.Linear(\n",
    "        mlp.up_proj.in_features,    # Input: hidden_size (unchanged)\n",
    "        k,                          # Output: reduced intermediate_size\n",
    "        bias=False\n",
    "    ).to(device)\n",
    "\n",
    "    new_down_proj = nn.Linear(\n",
    "        k,                          # Input: reduced intermediate_size\n",
    "        mlp.down_proj.out_features, # Output: hidden_size (unchanged)\n",
    "        bias=False\n",
    "    ).to(device)\n",
    "\n",
    "    # Copy weights for kept neurons only\n",
    "    # gate_proj and up_proj: neurons are rows, so index first dimension\n",
    "    new_gate_proj.weight.data = gate_weight[indices_to_keep, :]\n",
    "    new_up_proj.weight.data = up_weight[indices_to_keep, :]\n",
    "    \n",
    "    # down_proj: neurons are columns, so index second dimension\n",
    "    new_down_proj.weight.data = down_weight[:, indices_to_keep]\n",
    "\n",
    "    return new_gate_proj, new_up_proj, new_down_proj, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c50c139c",
   "metadata": {
    "id": "c50c139c"
   },
   "outputs": [],
   "source": [
    "def update_model(model, prune_percent, activation_norms):\n",
    "    \"\"\"\n",
    "    Applies pruning to all MLP layers in the model.\n",
    "\n",
    "    Args:\n",
    "        model: LlamaForCausalLM model to prune\n",
    "        prune_percent: Fraction of neurons to remove\n",
    "        activation_norms: Dict mapping layer_idx -> X_d_norm tensor\n",
    "\n",
    "    Returns:\n",
    "        model: Pruned model with updated configuration\n",
    "    \"\"\"\n",
    "\n",
    "    new_intermediate_size = None\n",
    "    pruning_stats = []\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting pruning with {prune_percent*100:.1f}% width pruning\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        mlp = layer.mlp\n",
    "\n",
    "        if idx not in activation_norms:\n",
    "            raise KeyError(f\"No activation norms for layer {idx}\")\n",
    "\n",
    "        X_d_norm = activation_norms[idx]\n",
    "        original_size = mlp.gate_proj.out_features\n",
    "\n",
    "        # Prune neuron pairs\n",
    "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(\n",
    "            mlp=mlp,\n",
    "            prune_percent=prune_percent,\n",
    "            X_d_norm=X_d_norm,\n",
    "            layer_idx=idx\n",
    "        )\n",
    "\n",
    "        # Replace layers\n",
    "        mlp.gate_proj = new_gate_proj\n",
    "        mlp.up_proj = new_up_proj\n",
    "        mlp.down_proj = new_down_proj\n",
    "\n",
    "        pruning_stats.append({\n",
    "            'layer': idx,\n",
    "            'original_size': original_size,\n",
    "            'new_size': new_size,\n",
    "            'pruned': original_size - new_size,\n",
    "            'kept_percent': (new_size / original_size) * 100\n",
    "        })\n",
    "\n",
    "        if new_intermediate_size is None:\n",
    "            new_intermediate_size = new_size\n",
    "\n",
    "        if (idx + 1) % 4 == 0:\n",
    "            print(f\"  Pruned layers {idx-3:2d}-{idx:2d}: \"\n",
    "                  f\"{original_size} â†’ {new_size} neurons \"\n",
    "                  f\"({(new_size/original_size)*100:.1f}% kept)\")\n",
    "\n",
    "    # Update model configuration\n",
    "    model.config.intermediate_size = new_intermediate_size\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Pruning completed!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Layers pruned: {len(pruning_stats)}\")\n",
    "    print(f\"  Original intermediate size: {original_size}\")\n",
    "    print(f\"  New intermediate size: {new_intermediate_size}\")\n",
    "    print(f\"  Neurons pruned per layer: {original_size - new_intermediate_size}\")\n",
    "    print(f\"  Effective width pruning: {((original_size - new_intermediate_size) / original_size) * 100:.2f}%\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23407a38",
   "metadata": {
    "id": "23407a38"
   },
   "source": [
    "# Calibration and Pruning\n",
    "\n",
    "Now we'll create two pruned models using different calibration datasets:\n",
    "1. **Wiki-pruned**: Calibrated on WikiText-2\n",
    "2. **SMS-pruned**: Calibrated on SMS Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ca44a",
   "metadata": {
    "id": "c81ca44a"
   },
   "source": [
    "## Wiki-Calibrated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a46f797",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a46f797",
    "outputId": "d72c5341-f876-401b-8fa3-f110dd56d476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WIKI CALIBRATION\n",
      "============================================================\n",
      "\n",
      "Setting up activation hooks...\n",
      "âœ“ Registered 16 hooks on down_proj\n",
      "\n",
      "Running calibration forward passes on WikiText...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki Calibration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Processed 25 batches\n",
      "Removing hooks...\n",
      "Extracting activation statistics...\n",
      "âœ“ Collected activation norms for 16 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"WIKI CALIBRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Setup hooks\n",
    "print(\"\\nSetting up activation hooks...\")\n",
    "handles_wiki = setup_mlp_hooks_for_importance(model, device)\n",
    "\n",
    "# Step 2: Run calibration forward passes\n",
    "print(\"\\nRunning calibration forward passes on WikiText...\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloaderwiki, desc=\"Wiki Calibration\")):\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nâœ“ Processed {len(dataloaderwiki)} batches\")\n",
    "\n",
    "# Step 3: Clean up hooks\n",
    "print(\"Removing hooks...\")\n",
    "for handle in handles_wiki:\n",
    "    handle.remove()\n",
    "\n",
    "# Step 4: Get activation norms\n",
    "print(\"Extracting activation statistics...\")\n",
    "activation_norms_wiki = get_activation_norms()\n",
    "print(f\"âœ“ Collected activation norms for {len(activation_norms_wiki)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdf2adac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdf2adac",
    "outputId": "7e8134dc-ac77-4a68-a4e1-64e9f7b3eb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting pruning with 20.0% width pruning\n",
      "============================================================\n",
      "\n",
      "  Pruned layers  0- 3: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "  Pruned layers  4- 7: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "  Pruned layers  8-11: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "  Pruned layers 12-15: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "\n",
      "============================================================\n",
      "Pruning completed!\n",
      "============================================================\n",
      "  Layers pruned: 16\n",
      "  Original intermediate size: 8192\n",
      "  New intermediate size: 6554\n",
      "  Neurons pruned per layer: 1638\n",
      "  Effective width pruning: 20.00%\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prune the model using Wiki activations\n",
    "wiki_model = update_model(copy.deepcopy(model), PRUNE_PERCENT, activation_norms_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7-6LdtLFb76H",
   "metadata": {
    "id": "7-6LdtLFb76H"
   },
   "outputs": [],
   "source": [
    "del(model)\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b9a1389",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b9a1389",
    "outputId": "88b9b721-8d6e-44bb-fe66-9ed307664f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki-pruned model: Paris is the capital of France and is located in the north-east of the country. The city has a population of 2.1 million people, making it the 3rd largest city in France. It is also the largest in terms of surface area, with 100\n"
     ]
    }
   ],
   "source": [
    "# Test wiki model\n",
    "generated_wiki = generate_text(wiki_model, tokenizer, prompt, device)\n",
    "print(f\"Wiki-pruned model: {generated_wiki}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "OULoO1Xy3g8z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OULoO1Xy3g8z",
    "outputId": "6882dd55-f015-455f-c2af-2e46a04ed09f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:16<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_cache()\n",
    "metrics_wiki_wiki = evaluate_metrics(wiki_model, dataloaderwiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "xmpv66mn4ng6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmpv66mn4ng6",
    "outputId": "64c452a4-85b7-4859-c308-0baa8c69372a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:16<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_wiki_sms = evaluate_metrics(wiki_model, dataloadersms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dgr767nUWWuw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgr767nUWWuw",
    "outputId": "49f16de0-fbe1-4c47-a29e-d7f72ada3812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring performance on 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.93s/it]\n"
     ]
    }
   ],
   "source": [
    "wiki_wiki_timing = measure_detailed_performance(wiki_model, tokenizer, dataloaderwiki, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "feAJux7Q4fsM",
   "metadata": {
    "id": "feAJux7Q4fsM"
   },
   "outputs": [],
   "source": [
    "wiki_params = count_parameters(wiki_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5_otkEIZKTZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5_otkEIZKTZ",
    "outputId": "a7f92555-8498-4245-96a2-92abdcb19d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring performance on 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.85s/it]\n"
     ]
    }
   ],
   "source": [
    "wiki_sms_timing = measure_detailed_performance(wiki_model, tokenizer, dataloadersms, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "XS9W2Av-hRWE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "XS9W2Av-hRWE",
    "outputId": "115fc62f-6702-435d-c4aa-0fdc6d1aeec8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>utils.clear_gpu_cache</b><br/>def clear_gpu_cache()</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/utils.py</a>Clear GPU cache completely</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 18);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "<function utils.clear_gpu_cache()>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(wiki_model)\n",
    "clear_gpu_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ece40",
   "metadata": {
    "id": "ea2ece40"
   },
   "source": [
    "## SMS-Calibrated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24Wi1HfzcCEb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24Wi1HfzcCEb",
    "outputId": "e0550b32-012e-4ba4-83a6-1a369de3ab9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2004a2c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2004a2c6",
    "outputId": "be0d766b-1852-4cae-c8d1-1d28a7d0ff87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SMS CALIBRATION\n",
      "============================================================\n",
      "\n",
      "Setting up activation hooks...\n",
      "âœ“ Registered 16 hooks on down_proj\n",
      "\n",
      "Running calibration forward passes on SMS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMS Calibration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Processed 25 batches\n",
      "Removing hooks...\n",
      "Extracting activation statistics...\n",
      "âœ“ Collected activation norms for 16 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache before SMS calibration\n",
    "clear_gpu_cache()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SMS CALIBRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Setup hooks\n",
    "print(\"\\nSetting up activation hooks...\")\n",
    "handles_sms = setup_mlp_hooks_for_importance(model, device)\n",
    "\n",
    "# Step 2: Run calibration forward passes\n",
    "print(\"\\nRunning calibration forward passes on SMS...\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloadersms, desc=\"SMS Calibration\")):\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nâœ“ Processed {len(dataloadersms)} batches\")\n",
    "\n",
    "# Step 3: Clean up hooks\n",
    "print(\"Removing hooks...\")\n",
    "for handle in handles_sms:\n",
    "    handle.remove()\n",
    "\n",
    "# Step 4: Get activation norms\n",
    "print(\"Extracting activation statistics...\")\n",
    "activation_norms_sms = get_activation_norms()\n",
    "print(f\"âœ“ Collected activation norms for {len(activation_norms_sms)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08fd3aaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08fd3aaa",
    "outputId": "411d4a68-cf93-4f6b-8d24-35a981899307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting pruning with 20.0% width pruning\n",
      "============================================================\n",
      "\n",
      "  Pruned layers  0- 3: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "  Pruned layers  4- 7: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "  Pruned layers  8-11: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "  Pruned layers 12-15: 8192 â†’ 6554 neurons (80.0% kept)\n",
      "\n",
      "============================================================\n",
      "Pruning completed!\n",
      "============================================================\n",
      "  Layers pruned: 16\n",
      "  Original intermediate size: 8192\n",
      "  New intermediate size: 6554\n",
      "  Neurons pruned per layer: 1638\n",
      "  Effective width pruning: 20.00%\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prune the model using SMS activations\n",
    "sms_model = update_model(copy.deepcopy(model), PRUNE_PERCENT, activation_norms_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "NIA_o3VEuXWg",
   "metadata": {
    "id": "NIA_o3VEuXWg"
   },
   "outputs": [],
   "source": [
    "del(model)\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff076cdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff076cdb",
    "outputId": "d5ff926f-f857-4ba2-f438-4c887be019c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMS-pruned model: Paris is the capital of the French department of Paris. It is located on the Seine River, which flows through the city. The city has a population of 1.8 million people. Paris is a major city in the world. Its population is estimated to be \n"
     ]
    }
   ],
   "source": [
    "# Test SMS model\n",
    "generated_sms = generate_text(sms_model, tokenizer, prompt, device)\n",
    "print(f\"SMS-pruned model: {generated_sms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7IUiiAIGYs0y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IUiiAIGYs0y",
    "outputId": "515750d4-4b00-4903-e40a-f07454990ce1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:16<00:00,  1.49it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:17<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_sms_wiki = evaluate_metrics(sms_model, dataloaderwiki)\n",
    "metrics_sms_sms = evaluate_metrics(sms_model, dataloadersms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "iSLgyzMPWeGz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSLgyzMPWeGz",
    "outputId": "7d69ab40-53b7-4a9c-e0d1-273dbcb02f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring performance on 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rPerformance test:   0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  10%|â–ˆ         | 1/10 [00:04<00:37,  4.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  4.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:29,  4.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:24,  4.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:17<00:15,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:22<00:13,  3.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:26<00:11,  3.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:30<00:07,  3.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:34<00:03,  3.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:36<00:00,  3.62s/it]\n"
     ]
    }
   ],
   "source": [
    "sms_wiki_timing = measure_detailed_performance(sms_model, tokenizer, dataloaderwiki, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "T3W4pWW-ZaDs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3W4pWW-ZaDs",
    "outputId": "b6f4cc28-6daa-4b7d-f33e-e1cd8cee0b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring performance on 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rPerformance test:   0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  10%|â–ˆ         | 1/10 [00:04<00:37,  4.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  4.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:29,  4.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:24,  4.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:20,  4.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:25<00:16,  4.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:29<00:12,  4.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:33<00:08,  4.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:37<00:04,  4.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  4.17s/it]\n"
     ]
    }
   ],
   "source": [
    "sms_sms_timing = measure_detailed_performance(sms_model, tokenizer, dataloadersms, max_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50b815",
   "metadata": {
    "id": "ca50b815"
   },
   "source": [
    "## Parameter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6838f398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6838f398",
    "outputId": "3c60a5c1-82be-4f83-c2c2-02544453890b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARAMETER COUNTS\n",
      "============================================================\n",
      "Original model:     1,235,814,400 parameters\n",
      "Wiki-pruned model:  1,074,792,448 parameters (13.03% reduction)\n",
      "SMS-pruned model:   1,074,792,448 parameters (13.03% reduction)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "sms_params = count_parameters(sms_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COUNTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original model:     {original_params:,} parameters\")\n",
    "print(f\"Wiki-pruned model:  {wiki_params:,} parameters ({((original_params - wiki_params) / original_params * 100):.2f}% reduction)\")\n",
    "print(f\"SMS-pruned model:   {sms_params:,} parameters ({((original_params - sms_params) / original_params * 100):.2f}% reduction)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc9f21",
   "metadata": {
    "id": "2ecc9f21"
   },
   "source": [
    "# Cross-Evaluation Matrix\n",
    "\n",
    "Now we evaluate all three models (base, wiki-pruned, sms-pruned) on both datasets (Wiki and SMS) to answer our core research question: **Does calibration dataset selection influence pruning decisions, and do the resulting models specialize for their calibration domain?**\n",
    "\n",
    "## The Experiment Design\n",
    "\n",
    "**Models**:\n",
    "1. Base model (unpruned Llama-3.2-1B)\n",
    "2. Wiki-pruned (calibrated on WikiText-2, 20% width reduction)\n",
    "3. SMS-pruned (calibrated on SMS Spam, 20% width reduction)\n",
    "\n",
    "**Evaluation datasets**:\n",
    "- WikiText-2 (formal, complex)\n",
    "- SMS Spam (informal, simple)\n",
    "\n",
    "**Hypothesis**:\n",
    "- Wiki-pruned should perform **better on WikiText** than on SMS (domain match)\n",
    "- SMS-pruned should perform **better on SMS** than on WikiText (domain match)\n",
    "- Cross-domain performance (wiki-pruned on SMS, sms-pruned on Wiki) should be **worse** than domain-matched performance\n",
    "\n",
    "This cross-evaluation matrix reveals whether data-driven pruning creates specialized models or if the calibration dataset choice doesn't significantly impact neuron selection.\n",
    "\n",
    "We'll measure:\n",
    "- **Perplexity**: Language modeling quality (lower is better)\n",
    "- **Loss**: Cross-entropy loss (lower is better)\n",
    "- **Inference speed**: Tokens per second (higher is better)\n",
    "\n",
    "Let's see if our hypothesis holds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b08735ed",
   "metadata": {
    "id": "b08735ed"
   },
   "outputs": [],
   "source": [
    "# Clear original model to save memory\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476741ba",
   "metadata": {
    "id": "476741ba"
   },
   "source": [
    "## Loss and Perplexity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85728cd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85728cd8",
    "outputId": "6bb97a3f-8a30-4bb5-81db-3de3c1a68479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING ON WIKITEXT-2\n",
      "============================================================\n",
      "\n",
      "Base model on Wiki: {'loss': 3.2460288122350294, 'perplexity': np.float64(25.688124727046315)}\n",
      "Wiki-pruned on Wiki: {'loss': 3.5913135137696126, 'perplexity': np.float64(36.28170115548473)}\n",
      "SMS-pruned on Wiki: {'loss': 3.8846293323963836, 'perplexity': np.float64(48.64890654342502)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON WIKITEXT-2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBase model on Wiki:\", metrics_base_wiki)\n",
    "print(\"Wiki-pruned on Wiki:\", metrics_wiki_wiki)\n",
    "print(\"SMS-pruned on Wiki:\", metrics_sms_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f400dde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f400dde",
    "outputId": "d4ce7adb-936e-4647-c295-ec2abb3a50a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING ON SMS SPAM\n",
      "============================================================\n",
      "\n",
      "Base model on SMS: {'loss': 4.8114213257195955, 'perplexity': np.float64(122.90618314977401)}\n",
      "Wiki-pruned on SMS: {'loss': 5.206973014926048, 'perplexity': np.float64(182.540673179065)}\n",
      "SMS-pruned on SMS: {'loss': 5.109189337836538, 'perplexity': np.float64(165.53610660777312)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON SMS SPAM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBase model on SMS:\", metrics_base_sms)\n",
    "print(\"Wiki-pruned on SMS:\", metrics_wiki_sms)\n",
    "print(\"SMS-pruned on SMS:\", metrics_sms_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cfab0",
   "metadata": {
    "id": "494cfab0"
   },
   "source": [
    "## Text Generation Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c542cd",
   "metadata": {
    "id": "c5c542cd"
   },
   "source": [
    "## Performance Measurement (Inference Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abe65cb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abe65cb0",
    "outputId": "b6d9e971-d94f-44aa-9748-a11f3c0cd2ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE MEASUREMENT ON WIKI\n",
      "============================================================\n",
      "\n",
      "Base model:       0.6791s (29.74 tok/s)\n",
      "Wiki-pruned:      1.3088s (35.45 tok/s)\n",
      "SMS-pruned:       1.2069s (34.88 tok/s)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE MEASUREMENT ON WIKI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(f\"\\nBase model:       {base_wiki_timing['avg_time_sec']:.4f}s ({base_wiki_timing['tokens_per_sec']:.2f} tok/s)\")\n",
    "print(f\"Wiki-pruned:      {wiki_wiki_timing['avg_time_sec']:.4f}s ({wiki_wiki_timing['tokens_per_sec']:.2f} tok/s)\")\n",
    "print(f\"SMS-pruned:       {sms_wiki_timing['avg_time_sec']:.4f}s ({sms_wiki_timing['tokens_per_sec']:.2f} tok/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd0f5545",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd0f5545",
    "outputId": "827ef3c0-096d-4aa4-e950-782a0000c5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE MEASUREMENT ON SMS\n",
      "============================================================\n",
      "\n",
      "Base model:       1.0957s (34.32 tok/s)\n",
      "Wiki-pruned:      1.2819s (35.88 tok/s)\n",
      "SMS-pruned:       1.3891s (35.99 tok/s)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE MEASUREMENT ON SMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nBase model:       {base_sms_timing['avg_time_sec']:.4f}s ({base_sms_timing['tokens_per_sec']:.2f} tok/s)\")\n",
    "print(f\"Wiki-pruned:      {wiki_sms_timing['avg_time_sec']:.4f}s ({wiki_sms_timing['tokens_per_sec']:.2f} tok/s)\")\n",
    "print(f\"SMS-pruned:       {sms_sms_timing['avg_time_sec']:.4f}s ({sms_sms_timing['tokens_per_sec']:.2f} tok/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29882f75",
   "metadata": {
    "id": "29882f75"
   },
   "source": [
    "# Summary and Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ea26207",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ea26207",
    "outputId": "972d349c-d739-46c2-8563-93919363adc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE RESULTS\n",
      "================================================================================\n",
      "      Model  Parameters  Param Reduction %  PPL Wiki    PPL SMS  Loss Wiki  Loss SMS  Time Wiki (s)  Time SMS (s)  Tok/s Wiki  Tok/s SMS\n",
      "       Base  1235814400           0.000000 25.688125 122.906183   3.246029  4.811421       0.679144      1.095701   29.743339  34.315922\n",
      "Wiki-pruned  1074792448          13.029623 36.281701 182.540673   3.591314  5.206973       1.308820      1.281878   35.451768  35.884851\n",
      " SMS-pruned  1074792448          13.029623 48.648907 165.536107   3.884629  5.109189       1.206911      1.389136   34.882451  35.993593\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive results table\n",
    "results = {\n",
    "    'Model': ['Base', 'Wiki-pruned', 'SMS-pruned'],\n",
    "    'Parameters': [original_params, wiki_params, sms_params],\n",
    "    'Param Reduction %': [\n",
    "        0,\n",
    "        ((original_params - wiki_params) / original_params * 100),\n",
    "        ((original_params - sms_params) / original_params * 100)\n",
    "    ],\n",
    "    'PPL Wiki': [\n",
    "        metrics_base_wiki['perplexity'],\n",
    "        metrics_wiki_wiki['perplexity'],\n",
    "        metrics_sms_wiki['perplexity']\n",
    "    ],\n",
    "    'PPL SMS': [\n",
    "        metrics_base_sms['perplexity'],\n",
    "        metrics_wiki_sms['perplexity'],\n",
    "        metrics_sms_sms['perplexity']\n",
    "    ],\n",
    "    'Loss Wiki': [\n",
    "        metrics_base_wiki['loss'],\n",
    "        metrics_wiki_wiki['loss'],\n",
    "        metrics_sms_wiki['loss']\n",
    "    ],\n",
    "    'Loss SMS': [\n",
    "        metrics_base_sms['loss'],\n",
    "        metrics_wiki_sms['loss'],\n",
    "        metrics_sms_sms['loss']\n",
    "    ],\n",
    "    'Time Wiki (s)': [\n",
    "        base_wiki_timing['avg_time_sec'],\n",
    "        wiki_wiki_timing['avg_time_sec'],\n",
    "        sms_wiki_timing['avg_time_sec']\n",
    "    ],\n",
    "    'Time SMS (s)': [\n",
    "        base_sms_timing['avg_time_sec'],\n",
    "        wiki_sms_timing['avg_time_sec'],\n",
    "        sms_sms_timing['avg_time_sec']\n",
    "    ],\n",
    "    'Tok/s Wiki': [\n",
    "        base_wiki_timing['tokens_per_sec'],\n",
    "        wiki_wiki_timing['tokens_per_sec'],\n",
    "        sms_wiki_timing['tokens_per_sec']\n",
    "    ],\n",
    "    'Tok/s SMS': [\n",
    "        base_sms_timing['tokens_per_sec'],\n",
    "        wiki_sms_timing['tokens_per_sec'],\n",
    "        sms_sms_timing['tokens_per_sec']\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23urrbqko0x",
   "metadata": {},
   "source": [
    "# Results Visualization\n",
    "\n",
    "Let's create visual comparisons to better understand the performance differences across models and datasets. These charts will make it easier to spot patterns in perplexity degradation, inference speed changes, and domain specialization effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cmxynafrc6",
   "metadata": {},
   "source": [
    "## Perplexity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ysemzl3z3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity comparison across all models and datasets\n",
    "models = ['Base', 'Wiki-pruned', 'SMS-pruned']\n",
    "datasets = ['WikiText', 'SMS']\n",
    "\n",
    "# Extract perplexity values from results\n",
    "perplexity_data = [\n",
    "    [metrics_base_wiki['perplexity'], metrics_base_sms['perplexity']],\n",
    "    [metrics_wiki_wiki['perplexity'], metrics_wiki_sms['perplexity']],\n",
    "    [metrics_sms_wiki['perplexity'], metrics_sms_sms['perplexity']]\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, [row[0] for row in perplexity_data],\n",
    "               width, label='WikiText', alpha=0.8, color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, [row[1] for row in perplexity_data],\n",
    "               width, label='SMS', alpha=0.8, color='lightcoral')\n",
    "\n",
    "ax.set_ylabel('Perplexity (Lower is Better)', fontsize=12)\n",
    "ax.set_title('Perplexity Comparison: 3 Models Ã— 2 Datasets', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqil2wm4gkj",
   "metadata": {},
   "source": [
    "## Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1rathqm5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference speed comparison (tokens per second)\n",
    "wiki_speed = [base_wiki_timing['tokens_per_sec'], \n",
    "              wiki_wiki_timing['tokens_per_sec'], \n",
    "              sms_wiki_timing['tokens_per_sec']]\n",
    "sms_speed = [base_sms_timing['tokens_per_sec'], \n",
    "             wiki_sms_timing['tokens_per_sec'], \n",
    "             sms_sms_timing['tokens_per_sec']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, wiki_speed, width, label='WikiText', alpha=0.8, color='forestgreen')\n",
    "bars2 = ax.bar(x + width/2, sms_speed, width, label='SMS', alpha=0.8, color='orange')\n",
    "\n",
    "ax.set_ylabel('Tokens per Second (Higher is Better)', fontsize=12)\n",
    "ax.set_title('Inference Speed Comparison: 3 Models Ã— 2 Datasets', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yzjd8qwojkt",
   "metadata": {},
   "source": [
    "## Loss Degradation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foblpez18gn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss degradation across models\n",
    "wiki_loss = [metrics_base_wiki['loss'], \n",
    "             metrics_wiki_wiki['loss'], \n",
    "             metrics_sms_wiki['loss']]\n",
    "sms_loss = [metrics_base_sms['loss'], \n",
    "            metrics_wiki_sms['loss'], \n",
    "            metrics_sms_sms['loss']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "\n",
    "ax.plot(x, wiki_loss, marker='o', linewidth=2, markersize=8,\n",
    "        label='WikiText', color='blue')\n",
    "ax.plot(x, sms_loss, marker='s', linewidth=2, markersize=8,\n",
    "        label='SMS', color='red')\n",
    "\n",
    "ax.set_ylabel('Loss (Lower is Better)', fontsize=12)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_title('Loss Degradation Across Models', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (wl, sl) in enumerate(zip(wiki_loss, sms_loss)):\n",
    "    ax.text(i, wl, f'{wl:.3f}', ha='center', va='bottom', fontsize=9, color='blue')\n",
    "    ax.text(i, sl, f'{sl:.3f}', ha='center', va='bottom', fontsize=9, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5o8uihia7o3",
   "metadata": {},
   "source": [
    "# Summary and Analysis\n",
    "\n",
    "This notebook explored data-driven neuron pruning using activation patterns from calibration datasets. We created two differently-pruned models (Wiki-calibrated and SMS-calibrated) and cross-evaluated them to test whether calibration dataset selection influences pruning decisions and model specialization.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Parameter Reduction\n",
    "- **Both pruned models**: 13.03% parameter reduction (161M parameters removed)\n",
    "- **Target pruning**: 20% width reduction (8,192 â†’ 6,554 neurons per layer)\n",
    "- **Actual savings**: Modest compared to the 26% achieved with 40% pruning in static method (NB01)\n",
    "\n",
    "### Perplexity Impact\n",
    "\n",
    "The results reveal **strong domain sensitivity**:\n",
    "\n",
    "**WikiText evaluation**:\n",
    "- Base model: 25.69 PPL\n",
    "- Wiki-pruned: 36.28 PPL (+41.2% increase)\n",
    "- SMS-pruned: 48.65 PPL (+89.3% increase) âš ï¸\n",
    "\n",
    "**SMS evaluation**:\n",
    "- Base model: 122.91 PPL\n",
    "- Wiki-pruned: 182.54 PPL (+48.5% increase) âš ï¸\n",
    "- SMS-pruned: 165.54 PPL (+34.7% increase)\n",
    "\n",
    "**Critical observation**: SMS-pruned performs better on SMS than Wiki-pruned (165.5 vs 182.5 PPL), providing evidence of **domain specialization**. However, both pruned models suffer severe degradation in cross-domain evaluation.\n",
    "\n",
    "### Loss Analysis\n",
    "\n",
    "Loss follows similar patterns to perplexity:\n",
    "- Wiki-pruned excels on WikiText (3.59 vs 3.88 for SMS-pruned)\n",
    "- SMS-pruned excels on SMS (5.11 vs 5.21 for Wiki-pruned)\n",
    "- Domain mismatch causes significant loss increases\n",
    "\n",
    "This confirms that calibration dataset choice **does influence** which neurons get pruned, creating models that are partially specialized for their calibration domain.\n",
    "\n",
    "## Performance Trade-offs\n",
    "\n",
    "### Inference Speed: An Unexpected Finding\n",
    "\n",
    "**Hypothesis**: Pruned models should be faster (fewer parameters â†’ less computation)\n",
    "\n",
    "**Reality**: Pruned models show **marginal or even slower** performance in some cases:\n",
    "\n",
    "**WikiText**:\n",
    "- Base: 29.74 tok/s\n",
    "- Wiki-pruned: 35.45 tok/s (+19% faster)\n",
    "- SMS-pruned: 34.88 tok/s (+17% faster)\n",
    "\n",
    "**SMS**:\n",
    "- Base: 34.32 tok/s\n",
    "- Wiki-pruned: 35.88 tok/s (+4.5% faster)\n",
    "- SMS-pruned: 35.99 tok/s (+4.9% faster)\n",
    "\n",
    "**Why are gains minimal?**\n",
    "1. **Memory bandwidth bottleneck**: Modern GPUs are often memory-bound, not compute-bound\n",
    "2. **Irregular tensor shapes**: Non-standard intermediate sizes (6,554) may not map efficiently to GPU hardware\n",
    "3. **Small model size**: Llama-3.2-1B already fits comfortably in memory; pruning doesn't alleviate a bottleneck\n",
    "4. **Framework overhead**: PyTorch may not fully optimize for irregular layer dimensions\n",
    "\n",
    "**Conclusion**: 13% parameter reduction yields only 5-19% speed improvementâ€”insufficient to justify the severe perplexity degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xcbic511h3",
   "metadata": {},
   "source": [
    "## Domain Specialization Analysis\n",
    "\n",
    "**Does data-driven pruning create specialized models?**\n",
    "\n",
    "**Evidence for specialization**:\n",
    "âœ… SMS-pruned outperforms Wiki-pruned on SMS dataset (165.5 vs 182.5 PPL)  \n",
    "âœ… Wiki-pruned outperforms SMS-pruned on WikiText (36.3 vs 48.6 PPL)  \n",
    "âœ… Each model performs best on its calibration domain\n",
    "\n",
    "**Evidence against strong specialization**:\n",
    "âš ï¸ Both pruned models degrade significantly on both datasets  \n",
    "âš ï¸ Cross-domain transfer is poor (Wiki-pruned on SMS: +48.5% PPL)  \n",
    "âš ï¸ Specialization gains are modest compared to overall quality loss\n",
    "\n",
    "**Interpretation**: Calibration datasets **do** influence which neurons survive pruning, creating measurable domain preferences. However, the specialization is **weak**â€”not strong enough to make domain-specific pruning a clear win over general-purpose compression.\n",
    "\n",
    "## Comparison to Static Pruning (Notebook 01)\n",
    "\n",
    "| Metric | Static Pruning (NB01) | Data-Driven (NB02) |\n",
    "|--------|----------------------|-------------------|\n",
    "| **Width reduction** | 40% | 20% |\n",
    "| **Parameter savings** | 26.06% | 13.03% |\n",
    "| **Method** | Weight magnitude only | Magnitude + activations |\n",
    "| **Calibration required** | No | Yes (100 samples) |\n",
    "| **Domain sensitivity** | None | High |\n",
    "| **Quality degradation** | Severe (PPL +386-1471%) | Severe (PPL +34-89%) |\n",
    "\n",
    "**Key insight**: Despite using activation patterns from real data, data-driven pruning with 20% width reduction produces similar quality degradation to static pruning with 40% reduction. This suggests:\n",
    "\n",
    "1. **Activation-aware pruning is more conservative**: It preserves more neurons (20% vs 40% pruning)\n",
    "2. **Calibration dataset matters more than expected**: Different datasets produce different models\n",
    "3. **Hybrid scoring helps, but not dramatically**: The improvement from adding dynamic signals is modest\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "**When to use data-driven pruning**:\n",
    "âœ… You have a **specific deployment domain** with representative calibration data  \n",
    "âœ… You can afford **domain-specific model variants** (e.g., wiki-model for documents, sms-model for chat)  \n",
    "âœ… You need to **minimize quality loss** in a target domain  \n",
    "âœ… You want to **understand which neurons matter** for a specific task\n",
    "\n",
    "**When to avoid it**:\n",
    "âŒ You need a **general-purpose** compressed model  \n",
    "âŒ Cross-domain performance is important  \n",
    "âŒ You lack **representative calibration data**  \n",
    "âŒ Speed/size improvements don't justify quality degradation  \n",
    "âŒ You want simplicity (static pruning requires no calibration)\n",
    "\n",
    "**Bottom line**: Data-driven pruning creates domain-specialized models, but the specialization is **weak** and comes at the cost of **severe cross-domain degradation**. For most applications, the trade-off is unfavorable.\n",
    "\n",
    "## Open Questions and Future Work\n",
    "\n",
    "1. **Longer calibration**: Would using 1,000+ samples instead of 100 improve pruning quality?\n",
    "\n",
    "2. **Hybrid calibration**: Can we combine WikiText + SMS during calibration to create a more robust pruned model?\n",
    "\n",
    "3. **Layer-specific pruning rates**: Should early layers (general features) be pruned less aggressively than late layers (task-specific)?\n",
    "\n",
    "4. **Fine-tuning after pruning**: Would brief fine-tuning on the calibration dataset recover lost performance?\n",
    "\n",
    "5. **Alternative importance metrics**: Are there better ways to combine static and dynamic signals (e.g., attention to activation correlation)?\n",
    "\n",
    "6. **Hardware-aware pruning**: Can we constrain intermediate sizes to GPU-friendly dimensions (powers of 2) to maximize speed gains?\n",
    "\n",
    "7. **Structured vs unstructured**: How does neuron-level pruning compare to more fine-grained weight pruning?\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated that **calibration dataset selection significantly influences data-driven pruning decisions**, creating models with measurable domain preferences. However, the resulting specialization is **insufficient to justify the severe quality degradation** in most practical scenarios.\n",
    "\n",
    "**Key takeaways**:\n",
    "- âœ… Data-driven pruning **does** create domain-specialized models\n",
    "- âš ï¸ Specialization is **weak** compared to overall quality loss\n",
    "- âŒ Speed gains are **minimal** (5-19%) despite 13% parameter reduction\n",
    "- ðŸ“Š Cross-domain performance is **severely degraded**\n",
    "\n",
    "For general-purpose model compression, **alternative techniques** (quantization, distillation, layer dropping) may offer better trade-offs. Data-driven width pruning is best suited for **highly specialized, domain-specific deployments** where calibration data perfectly matches the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "MNd22xaeGyfp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNd22xaeGyfp",
    "outputId": "753e8fa3-7833-4b67-fb7e-26f589635de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a rich history and culture, and it is home to many famous landmarks, including the E\n",
      "Paris is the capital of France and is located in the north-east of the country. The city has a population of 2.1 million people, making it the 3rd largest city in France. It is also the largest in terms of surface area, with 100\n",
      "Paris is the capital of the French department of Paris. It is located on the Seine River, which flows through the city. The city has a population of 1.8 million people. Paris is a major city in the world. Its population is estimated to be \n"
     ]
    }
   ],
   "source": [
    "print(generated_base)\n",
    "print(generated_wiki)\n",
    "print(generated_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kiiX68wPlyUu",
   "metadata": {
    "id": "kiiX68wPlyUu"
   },
   "source": [
    "40% pruning expansion rate.\n",
    "```\n",
    "================================================================================\n",
    "COMPREHENSIVE RESULTS\n",
    "================================================================================\n",
    "      Model  Parameters  Param Reduction %  PPL Wiki    PPL SMS  Loss Wiki  Loss SMS  Time Wiki (s)  Time SMS (s)  Tok/s Wiki  Tok/s SMS\n",
    "       Base  1235814400           0.000000 25.688125 122.906183   3.246029  4.811421       0.679144      1.095701   29.743339  34.315922\n",
    "Wiki-pruned  1074792448          13.029623 36.281701 182.540673   3.591314  5.206973       1.308820      1.281878   35.451768  35.884851\n",
    " SMS-pruned  1074792448          13.029623 48.648907 165.536107   3.884629  5.109189       1.206911      1.389136   34.882451  35.993593\n",
    "================================================================================\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "LMA23Yq5lwen",
   "metadata": {
    "id": "LMA23Yq5lwen"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
