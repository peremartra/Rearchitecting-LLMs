{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH05/CH05_NB02_data_sms_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m6q6m0d2wlp",
      "metadata": {
        "id": "m6q6m0d2wlp"
      },
      "source": [
        "# Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 5: Width Pruning\n",
        "### Notebook: 02. Data-Driven Neuron Selection\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white)](https://x.com/PereMartra) [![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* Llama-3.2-1B\n",
        "_____\n",
        "\n",
        "In this notebook, we explore **data-driven width pruning**â€”a more sophisticated approach to neural network compression that uses activation patterns from real data to make pruning decisions. Unlike the static magnitude-based pruning from Notebook 01, this method captures how neurons actually behave during inference.\n",
        "\n",
        "We implement a **CFSP-inspired hybrid importance scoring** that combines:\n",
        "- **Static component**: Weight magnitudes (what neurons *could* do)\n",
        "- **Dynamic component**: Activation norms from calibration data (what neurons *actually* do)\n",
        "\n",
        "The key experiment: We create two differently-pruned models from the same Llama-3.2-1B base, using contrasting calibration datasets:\n",
        "- **WikiText-2**: Long-form encyclopedic text with complex language patterns\n",
        "- **SMS Spam**: Short conversational messages with informal language\n",
        "\n",
        "By cross-evaluating all three models (base, wiki-pruned, sms-pruned) on both datasets, we answer a critical question: **Does the choice of calibration data influence which neurons get pruned, and does this create domain-specialized models?**\n",
        "\n",
        "By the end of this notebook, you'll understand how activation-aware pruning differs from static methods, why calibration dataset selection matters, and whether domain-specific pruning offers practical advantages over one-size-fits-all compression."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Data-driven neuron selection\n"
      ],
      "metadata": {
        "id": "EQNMi8cQPhoy"
      },
      "id": "EQNMi8cQPhoy"
    },
    {
      "cell_type": "markdown",
      "id": "92f98e07",
      "metadata": {
        "id": "92f98e07"
      },
      "source": [
        "## Setting up notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ade7571c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ade7571c",
        "outputId": "d9054c6c-1330-49ef-96b6-70101bbb31b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.6/357.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m263.3/263.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.53.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "      \"torch\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "      \"lm_eval==0.4.9.1\" \\\n",
        "      \"sentencepiece==0.2.1\" \\\n",
        "      \"datasets\" \\\n",
        "      \"langdetect\"\\\n",
        "      \"codecarbon\"\\\n",
        "      \"optipfair==0.2.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5050e0a5",
      "metadata": {
        "id": "5050e0a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from lm_eval import evaluator\n",
        "from torch import nn\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import gc\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from optipfair import prune_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6ec5a44a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ec5a44a",
        "outputId": "8d55e948-8611-4c29-a695-3483a39fef20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d307df",
      "metadata": {
        "id": "c7d307df"
      },
      "source": [
        "Download helper functions from the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "36300712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36300712",
        "outputId": "3a6f001a-eba2-4ed0-8156-0b0c2cae4ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")\n",
        "\n",
        "from utils import (\n",
        "  model_evaluation, # Evals with lm_eval\n",
        "  evaluate_metrics, # Loss & Perpelexity\n",
        "  generate_text, #test inference model\n",
        "  measure_detailed_performance, # Inference performance\n",
        "  measure_energy_consumption, # Energy consumption\n",
        "  calibrate_idle_power, # Calibrate GPU Idle consumption\n",
        "  clear_gpu_cache\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c52aeb41",
      "metadata": {
        "id": "c52aeb41"
      },
      "source": [
        "## Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9065c773",
      "metadata": {
        "id": "9065c773",
        "outputId": "c546a5a8-4315-4207-f08a-8430de3996c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”‹ Starting idle power calibration (30s)...\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 11:33:53] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Measuring idle power for 30s...\n",
            "âœ… Calibration complete!\n",
            "   Idle Power: 11.05 W\n",
            "   Idle Energy (30s): 0.000092 kWh\n",
            "   GPU Temperature: 59.0Â°C\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = 'meta-llama/Llama-3.2-1B'\n",
        "\n",
        "# Dataset configuration\n",
        "RECOVERY_SAMPLES = 100  # Calibration samples per dataset\n",
        "MAX_LENGTH = 1024\n",
        "BATCH_SIZE = 4\n",
        "MAX_SAMPLES_PERFORMANCE=10\n",
        "MAX_SAMPLES_CARBON=50\n",
        "\n",
        "\n",
        "# Pruning configuration\n",
        "PRUNE_PERCENT = 0.2  # 20% of neurons will be pruned\n",
        "\n",
        "# Generation configuration\n",
        "MAX_NEW_TOKENS = 50\n",
        "\n",
        "idle_calibration = calibrate_idle_power(device=\"cuda\", duration_seconds=30, verbose=True)\n",
        "idle_watts = idle_calibration[\"idle_power_watts\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a02967",
      "metadata": {
        "id": "48a02967"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "model.generation_config.temperature = None\n",
        "model.generation_config.top_p = None\n",
        "model.generation_config.top_k = None\n",
        "\n",
        "print(f\"âœ“ Loaded {MODEL_NAME}\")\n",
        "print(f\"  Layers: {len(model.model.layers)}\")\n",
        "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
        "print(f\"  Intermediate size: {model.config.intermediate_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef7cc965",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef7cc965",
        "outputId": "24a98590-a972-4547-99f2-86cd16e2e95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model generation: Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a rich history and culture, and it is home to many famous landmarks, including the E\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated_base = generate_text(model, tokenizer, prompt, device)\n",
        "print(f\"Base model generation: {generated_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935a14d5",
      "metadata": {
        "id": "935a14d5"
      },
      "source": [
        "## Load and Prepare Calibration Datasets\n",
        "\n",
        "We'll load two contrasting datasets to serve as calibration sources for our pruning experiments:\n",
        "\n",
        "1. **WikiText-2**: Long-form, encyclopedic text with complex language patterns. This dataset represents formal, structured writing with varied vocabulary and sophisticated grammar. It's designed to test a model's ability to handle coherent, knowledge-rich narratives.\n",
        "\n",
        "2. **SMS Spam**: Short conversational messages with informal language. This dataset contains brief text messagesâ€”often with abbreviations, casual grammar, and simple sentence structures. It represents a completely different linguistic domain.\n",
        "\n",
        "**The Hypothesis**: Different datasets will activate different neurons during calibration. When we calculate importance scores based on actual activation patterns, neurons that fire strongly on WikiText might barely activate on SMS, and vice versa. This should produce two distinct pruning patternsâ€”and potentially, two specialized models.\n",
        "\n",
        "These datasets will serve as calibration sources for our two pruned models, allowing us to test whether domain-specific calibration creates measurably different architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d44889",
      "metadata": {
        "id": "10d44889"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "datawiki = load_dataset('wikitext', 'wikitext-2-raw-v1', split=f'train[:{RECOVERY_SAMPLES}]')\n",
        "datasms = load_dataset('sms_spam', split=f'train[:{RECOVERY_SAMPLES}]')\n",
        "\n",
        "print(f\"âœ“ WikiText samples: {len(datawiki)}\")\n",
        "print(f\"âœ“ SMS samples: {len(datasms)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5a909bea",
      "metadata": {
        "id": "5a909bea"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset, text_field='text'):\n",
        "    \"\"\"\n",
        "    Tokenizes and prepares a dataset for calibration.\n",
        "\n",
        "    Handles different dataset formats (WikiText uses 'text', SMS uses 'sms' field).\n",
        "    \"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        if text_field in examples:\n",
        "            texts = examples[text_field]\n",
        "        elif 'sms' in examples:  # SMS dataset specific\n",
        "            texts = examples['sms']\n",
        "        elif 'text' in examples:\n",
        "            texts = examples['text']\n",
        "        else:\n",
        "            texts = examples[list(examples.keys())[0]]\n",
        "\n",
        "        return tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "    return DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "O48-PwbYrO4z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O48-PwbYrO4z",
        "outputId": "c1760b46-0d20-4456-d920-596289df501a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.73it/s]\n"
          ]
        }
      ],
      "source": [
        "metrics_base_wiki = evaluate_metrics(model, dataloaderwiki)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3n_AMbL4sU0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n_AMbL4sU0f",
        "outputId": "759a776c-fdc2-47d7-c42a-293ce13a3ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.63it/s]\n"
          ]
        }
      ],
      "source": [
        "metrics_base_sms = evaluate_metrics(model, dataloadersms)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base_carbon = measure_energy_consumption(model, tokenizer, dataloaderwiki,\n",
        "                                                 idle_power_watts=idle_watts,\n",
        "                                                 max_samples=MAX_SAMPLES_CARBON, max_new_tokens=50)"
      ],
      "metadata": {
        "id": "wqJoXaR3g84f",
        "outputId": "cccc3fc0-e7c1-4bc2-d921-23d67313dda5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wqJoXaR3g84f",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ Measuring energy on 50 samples (1 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Energy measurement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:47<00:00,  1.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "akHFaKqOWRuj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akHFaKqOWRuj",
        "outputId": "8b8a1ab4-d411-4239-8240-a81fc8409164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Optimization active: Using Mixed Precision (torch.bfloat16) if available.\n",
            "Measuring performance on 3 batches (Total samples: 12, 3 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:   0%|          | 0/3 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:31<01:02, 31.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:02<00:31, 31.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:34<00:00, 31.45s/it]\n"
          ]
        }
      ],
      "source": [
        "base_wiki_timing = measure_detailed_performance(model, tokenizer, dataloaderwiki, max_samples=MAX_SAMPLES_PERFORMANCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "WdMgItGZZQfJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdMgItGZZQfJ",
        "outputId": "a31cbcba-c2fa-44c5-fe4a-d257adbf98b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Optimization active: Using Mixed Precision (torch.bfloat16) if available.\n",
            "Measuring performance on 3 batches (Total samples: 12, 3 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:   0%|          | 0/3 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:31<01:02, 31.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:02<00:31, 31.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:34<00:00, 31.44s/it]\n"
          ]
        }
      ],
      "source": [
        "base_sms_timing = measure_detailed_performance(model, tokenizer, dataloadersms, max_samples=MAX_SAMPLES_PERFORMANCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "yxtr-quOwQuS",
      "metadata": {
        "id": "yxtr-quOwQuS"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "_0RBrAtxvYVq",
      "metadata": {
        "id": "_0RBrAtxvYVq"
      },
      "outputs": [],
      "source": [
        "original_params = count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "sxQHY7opwbi8",
      "metadata": {
        "id": "sxQHY7opwbi8"
      },
      "outputs": [],
      "source": [
        "clear_gpu_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031f18a6",
      "metadata": {
        "id": "031f18a6"
      },
      "source": [
        "## Data-Driven Pruning Functions\n",
        "\n",
        "These functions implement the CFSP-inspired methodology adapted from contextual pruning research. The key innovation: instead of pruning based solely on weight magnitudes (static), we incorporate **runtime activation patterns** (dynamic) to make smarter pruning decisions.\n",
        "\n",
        "## Understanding the Approach\n",
        "\n",
        "**Static pruning** (Notebook 01) asks: \"Which neurons have the smallest weights?\"  \n",
        "**Data-driven pruning** (this notebook) asks: \"Which neurons have the smallest weights *and* barely activate on real data?\"\n",
        "\n",
        "This is accomplished through three stages:\n",
        "1. **Activation capture**: PyTorch hooks on down_proj to record runtime behavior\n",
        "2. **Hybrid importance**: Combines weight magnitudes with activation norms  \n",
        "3. **Neuron pair pruning**: Removes least important neurons from gate_proj, up_proj, and down_proj\n",
        "\n",
        "**Why down_proj?** In the GLU MLP architecture, down_proj receives the output of the gating mechanism:\n",
        "\n",
        "```\n",
        "X â†’ gate_proj â†’ SiLU(Â·) â”€â”€â”\n",
        "                          Ã— â†’ down_proj â†’ output\n",
        "X â†’ up_proj â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "The input to down_proj represents the **post-gating activations**â€”the actual information that flows through each neuron after the gate decides what to pass."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.1 Hybrid Importance Through Weights and Activations\n"
      ],
      "metadata": {
        "id": "Efsxk3C9Qfaf"
      },
      "id": "Efsxk3C9Qfaf"
    },
    {
      "cell_type": "markdown",
      "id": "4b68596f",
      "metadata": {
        "id": "4b68596f"
      },
      "source": [
        "\n",
        "The key innovation: we combine **static** and **dynamic** signals to make pruning decisions.\n",
        "\n",
        "**Static component**: Weight magnitude norms\n",
        "- `||W_gate||`, `||W_up||`, `||W_down||` measure potential neuron capacity\n",
        "- Large weights suggest a neuron *could* have significant impact\n",
        "- Fast to compute, but ignores actual runtime behavior\n",
        "\n",
        "**Dynamic component**: Activation norms from calibration data\n",
        "- `||X_d||` measures actual neuron contributions on real inputs\n",
        "- Captures which neurons *actually fire* for a given domain\n",
        "- Expensive to compute, but provides ground truth about neuron utility\n",
        "\n",
        "**Why both matter**: A neuron might have large weights but rarely activate (static overestimates importance), or have small weights but activate consistently (static underestimates importance). By multiplying the normalized static score by the activation norm, we get a hybrid metric that favors neurons that are both structurally significant *and* behaviorally active.\n",
        "\n",
        "**Normalization strategy**: We scale gate, up, and down norms to [0,1] to equalize their contributions, preventing any single component from dominating the importance score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_neuron_pair_importance(gate_weight, up_weight, down_weight, X_d_norm):\n",
        "    \"\"\"\n",
        "    Hybrid Importance: Static Range (Max-Min) * Dynamic Activation.\n",
        "\n",
        "    Adapts the metric from Chapter 5.1 (Weight Range) for use in the\n",
        "    hybrid pipeline of Chapter 5.2.\n",
        "    \"\"\"\n",
        "    # 1. Convert to float32 for numerical stability\n",
        "    gate_weight = gate_weight.float()\n",
        "    up_weight = up_weight.float()\n",
        "    down_weight = down_weight.float()\n",
        "    # Ensure activations are on the same device\n",
        "    X_d_norm = X_d_norm.float().to(gate_weight.device)\n",
        "\n",
        "    # 2. Static Component: Range Metric (Max + Abs(Min))\n",
        "    # This metric values the amplitude of signal that the neuron can generate\n",
        "\n",
        "    # gate_proj and up_proj: Neurons are rows (dim=1)\n",
        "    gate_score = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
        "    up_score = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
        "    # down_proj: Neurons are columns (dim=0)\n",
        "    down_score = torch.max(down_weight, dim=0).values + torch.abs(torch.min(down_weight, dim=0).values)\n",
        "\n",
        "    # 3. Normalization\n",
        "    # Scale each component to [0, 1] so they have similar weight when summed\n",
        "    gate_norm = gate_score / (gate_score.max() + 1e-8)\n",
        "    up_norm = up_score / (up_score.max() + 1e-8)\n",
        "    down_norm = down_score / (down_score.max() + 1e-8)\n",
        "\n",
        "    # 4. Combined Structural Score\n",
        "    # Sum the importance of weights from the 3 layers\n",
        "    structural_score = down_norm + gate_norm + up_norm\n",
        "\n",
        "    # 5. Hybrid Fusion (Data-Driven)\n",
        "    # Multiply structural potential by captured real activation (X_d)\n",
        "    # If a neuron has large weights but zero activation, its score will be 0.\n",
        "    importance_scores = structural_score * X_d_norm\n",
        "\n",
        "    return importance_scores"
      ],
      "metadata": {
        "id": "roKW32SlUoLJ"
      },
      "id": "roKW32SlUoLJ",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "161ac494",
      "metadata": {
        "id": "161ac494"
      },
      "source": [
        "## 5.2.2 Extracting the Dynamic Importance Component\n",
        "\n",
        "We register **forward hooks** on the `down_proj` layer of every MLP module. When data flows through the model during calibration, our hook function captures the input tensor to `down_proj` (X_d), calculates its L2 norm, and accumulates these norms across all batches.\n",
        "\n",
        "**Why accumulate?** A single batch might not represent typical activation patterns. By summing L2 norms across all calibration batches, we get a robust estimate of each neuron's average contribution. Neurons with consistently high ||X_d|| values are considered important and preserved; those with low norms are candidates for pruning.\n",
        "\n",
        "**Memory strategy**: We store accumulated norms on CPU rather than GPU to conserve VRAM, since we're processing the full model + calibration batches simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b2030c79",
      "metadata": {
        "id": "b2030c79"
      },
      "outputs": [],
      "source": [
        "# Global storage for accumulated activation norms\n",
        "_accumulated_act_norms = {}\n",
        "\n",
        "def setup_mlp_hooks_for_importance(model, device):\n",
        "    \"\"\"\n",
        "    Registers hooks on down_proj inputs (X_d) to calculate L2 norms\n",
        "    for each neuron, following CFSP Equation 8.\n",
        "\n",
        "    This function sets up the activation capture mechanism. For each MLP layer,\n",
        "    we register a forward hook on down_proj that intercepts the input tensor\n",
        "    (X_d) during forward passes, computes its L2 norm across batch and sequence\n",
        "    dimensions, and accumulates these norms globally.\n",
        "\n",
        "    The accumulated norms represent each neuron's total contribution across\n",
        "    all calibration batches, providing a data-driven importance signal.\n",
        "\n",
        "    Args:\n",
        "        model: LlamaForCausalLM model to instrument with hooks\n",
        "        device: Device where model is running (used for initialization)\n",
        "\n",
        "    Returns:\n",
        "        handles: List of hook handles (call handle.remove() to unregister)\n",
        "    \"\"\"\n",
        "    global _accumulated_act_norms\n",
        "    _accumulated_act_norms.clear()\n",
        "\n",
        "    # Free memory before starting to avoid OOM errors during calibration\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    handles = []\n",
        "\n",
        "    # Initialize storage on CPU to save VRAM during calibration\n",
        "    # Each layer gets a tensor of shape [intermediate_size] initialized to zeros\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        intermediate_size = layer.mlp.down_proj.in_features\n",
        "        _accumulated_act_norms[idx] = torch.zeros(\n",
        "            intermediate_size,\n",
        "            dtype=torch.float32,\n",
        "            device='cpu'  # CPU storage prevents GPU memory overflow\n",
        "        )\n",
        "\n",
        "    def make_hook(layer_idx):\n",
        "        \"\"\"\n",
        "        Factory function that creates a hook closure for a specific layer.\n",
        "        Each hook captures X_d and accumulates its L2 norm.\n",
        "        \"\"\"\n",
        "        def hook(module, input, output):\n",
        "            \"\"\"\n",
        "            Hook function called automatically during forward pass.\n",
        "\n",
        "            Captures X_d (input to down_proj) and calculates its L2 norm.\n",
        "            X_d shape: [batch_size, seq_len, intermediate_size]\n",
        "            Output: [intermediate_size] with ||X_d^i|| for each neuron i\n",
        "\n",
        "            The L2 norm is computed as: ||X_d^i|| = sqrt(sum((X_d[:,:,i])^2))\n",
        "            summed over batch and sequence dimensions.\n",
        "            \"\"\"\n",
        "            X_d = input[0].detach()  # [B, S, I] - input to down_proj\n",
        "\n",
        "            # Calculate L2 norm across batch and sequence dimensions\n",
        "            # This gives us a single importance score per neuron\n",
        "            act_norms_L2 = torch.norm(\n",
        "                X_d.to(torch.float32),\n",
        "                p=2,\n",
        "                dim=(0, 1)  # Sum over batch (0) and sequence (1) dimensions\n",
        "            )\n",
        "\n",
        "            # Accumulate on CPU to save VRAM - we'll use these later for pruning\n",
        "            _accumulated_act_norms[layer_idx] += act_norms_L2.cpu()\n",
        "\n",
        "        return hook\n",
        "\n",
        "    # Register hooks on every MLP layer's down_proj\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        handle = layer.mlp.down_proj.register_forward_hook(\n",
        "            make_hook(idx)\n",
        "        )\n",
        "        handles.append(handle)\n",
        "\n",
        "    print(f\"âœ“ Registered {len(handles)} hooks on down_proj\")\n",
        "\n",
        "    return handles\n",
        "\n",
        "def get_activation_norms():\n",
        "    \"\"\"\n",
        "    Returns the accumulated L2 norms in a format ready for pruning.\n",
        "\n",
        "    After running calibration batches through the model with hooks enabled,\n",
        "    this function retrieves the accumulated activation norms for each layer.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, torch.Tensor]: {layer_idx: norms_L2 [intermediate_size]}\n",
        "            Each tensor contains importance scores for all neurons in that layer.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        layer_idx: norms.clone()\n",
        "        for layer_idx, norms in _accumulated_act_norms.items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a89c3a0",
      "metadata": {
        "id": "4a89c3a0"
      },
      "source": [
        "## 5.2.3 Reconstructing The MLP with Hybrid Scores.\n",
        "\n",
        "With importance scores calculated, we now perform the actual surgery on the MLP layers.\n",
        "\n",
        "**Key insight**: In GLU architectures, gate_proj and up_proj work as a **neuron pair**â€”they operate on the same input and their outputs are combined via element-wise multiplication. Therefore, we must prune corresponding neurons from both projections simultaneously.\n",
        "\n",
        "The pruning process:\n",
        "1. **Rank neurons** by importance scores (lowest to highest)\n",
        "2. **Select top-k** most important neurons to keep\n",
        "3. **Create new smaller layers** with dimensions:\n",
        "   - gate_proj: [hidden_size â†’ k] instead of [hidden_size â†’ intermediate_size]\n",
        "   - up_proj: [hidden_size â†’ k]\n",
        "   - down_proj: [k â†’ hidden_size] instead of [intermediate_size â†’ hidden_size]\n",
        "4. **Copy weights** for only the kept neurons\n",
        "\n",
        "**Dimension mapping**:\n",
        "- gate_proj and up_proj: Neurons are **rows** (index with `weight[neuron_idx, :]`)\n",
        "- down_proj: Neurons are **columns** (index with `weight[:, neuron_idx]`)\n",
        "\n",
        "This ensures the pruned MLP maintains compatibility with the rest of the modelâ€”the input and output dimensions remain unchanged at `hidden_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "bdc89ded",
      "metadata": {
        "id": "bdc89ded"
      },
      "outputs": [],
      "source": [
        "def prune_neuron_pairs(mlp, prune_percent, X_d_norm, layer_idx):\n",
        "    \"\"\"\n",
        "    Prunes neuron pairs from MLP block using hybrid importance scores.\n",
        "\n",
        "    Reduces dimensions of gate_proj, up_proj, and down_proj layers by removing\n",
        "    the least important neurons based on the combined static (weight magnitude)\n",
        "    and dynamic (activation norm) scoring.\n",
        "\n",
        "    Args:\n",
        "        mlp: LlamaMLP module to prune\n",
        "        prune_percent: Fraction of neurons to remove (e.g., 0.2 for 20%)\n",
        "        X_d_norm: Tensor [intermediate_size] with accumulated L2 activation norms\n",
        "        layer_idx: Layer index (for logging/debugging)\n",
        "\n",
        "    Returns:\n",
        "        new_gate_proj: Pruned gate projection layer\n",
        "        new_up_proj: Pruned up projection layer\n",
        "        new_down_proj: Pruned down projection layer\n",
        "        k: New intermediate size (number of neurons kept)\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract weights from the three MLP projection layers\n",
        "    gate_weight = mlp.gate_proj.weight.data\n",
        "    up_weight = mlp.up_proj.weight.data\n",
        "    down_weight = mlp.down_proj.weight.data\n",
        "\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "\n",
        "    # Compute hybrid importance scores (static + dynamic)\n",
        "    importance_scores = compute_neuron_pair_importance(\n",
        "        gate_weight=gate_weight,\n",
        "        up_weight=up_weight,\n",
        "        down_weight=down_weight,\n",
        "        X_d_norm=X_d_norm\n",
        "    )\n",
        "\n",
        "    # Calculate how many neurons to keep (k)\n",
        "    # Ensure we keep at least 1 neuron to avoid zero-dimension tensors\n",
        "    num_to_prune = min(\n",
        "        int(prune_percent * original_intermediate_size),\n",
        "        original_intermediate_size - 1\n",
        "    )\n",
        "    k = original_intermediate_size - num_to_prune\n",
        "\n",
        "    if k <= 0:\n",
        "        raise ValueError(f\"Invalid k={k} for layer {layer_idx}\")\n",
        "\n",
        "    # Select top-k most important neurons to keep\n",
        "    # topk returns (values, indices) - we only need indices\n",
        "    _, indices_to_keep = torch.topk(\n",
        "        importance_scores,\n",
        "        k,\n",
        "        largest=True,  # Keep highest importance scores\n",
        "        sorted=True\n",
        "    )\n",
        "\n",
        "    # Sort indices to maintain neuron ordering (optional but cleaner)\n",
        "    # This ensures weight matrices remain organized even after pruning\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    # Create new pruned linear layers with reduced dimensions\n",
        "    new_gate_proj = nn.Linear(\n",
        "        mlp.gate_proj.in_features,  # Input: hidden_size (unchanged)\n",
        "        k,                          # Output: reduced intermediate_size\n",
        "        bias=False\n",
        "    ).to(device)\n",
        "\n",
        "    new_up_proj = nn.Linear(\n",
        "        mlp.up_proj.in_features,    # Input: hidden_size (unchanged)\n",
        "        k,                          # Output: reduced intermediate_size\n",
        "        bias=False\n",
        "    ).to(device)\n",
        "\n",
        "    new_down_proj = nn.Linear(\n",
        "        k,                          # Input: reduced intermediate_size\n",
        "        mlp.down_proj.out_features, # Output: hidden_size (unchanged)\n",
        "        bias=False\n",
        "    ).to(device)\n",
        "\n",
        "    # Copy weights for kept neurons only\n",
        "    # gate_proj and up_proj: neurons are rows, so index first dimension\n",
        "    new_gate_proj.weight.data = gate_weight[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = up_weight[indices_to_keep, :]\n",
        "\n",
        "    # down_proj: neurons are columns, so index second dimension\n",
        "    new_down_proj.weight.data = down_weight[:, indices_to_keep]\n",
        "\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, k"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23407a38",
      "metadata": {
        "id": "23407a38"
      },
      "source": [
        "## 5.2.4 Running the calibration loop\n",
        "\n",
        "\n",
        "Now we'll create two pruned models using different calibration datasets:\n",
        "1. **Wiki-pruned**: Calibrated on WikiText-2\n",
        "2. **SMS-pruned**: Calibrated on SMS Spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4a46f797",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a46f797",
        "outputId": "9c288e96-d484-44f0-8bfa-d7ac3d305e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "WIKI CALIBRATION\n",
            "============================================================\n",
            "\n",
            "Setting up activation hooks...\n",
            "âœ“ Registered 16 hooks on down_proj\n",
            "\n",
            "Running calibration forward passes on WikiText...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Wiki Calibration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Processed 25 batches\n",
            "Removing hooks...\n",
            "Extracting activation statistics...\n",
            "âœ“ Collected activation norms for 16 layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"WIKI CALIBRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Setup hooks\n",
        "print(\"\\nSetting up activation hooks...\")\n",
        "handles_wiki = setup_mlp_hooks_for_importance(model, device)\n",
        "\n",
        "# Step 2: Run calibration forward passes\n",
        "print(\"\\nRunning calibration forward passes on WikiText...\")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloaderwiki, desc=\"Wiki Calibration\")):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(device),\n",
        "            'attention_mask': batch['attention_mask'].to(device)\n",
        "        }\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nâœ“ Processed {len(dataloaderwiki)} batches\")\n",
        "\n",
        "# Step 3: Clean up hooks\n",
        "print(\"Removing hooks...\")\n",
        "for handle in handles_wiki:\n",
        "    handle.remove()\n",
        "\n",
        "# Step 4: Get activation norms\n",
        "print(\"Extracting activation statistics...\")\n",
        "activation_norms_wiki = get_activation_norms()\n",
        "print(f\"âœ“ Collected activation norms for {len(activation_norms_wiki)} layers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.5 Creating domain-specialized models\n"
      ],
      "metadata": {
        "id": "mf1kD4De7f4z"
      },
      "id": "mf1kD4De7f4z"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c50c139c",
      "metadata": {
        "id": "c50c139c"
      },
      "outputs": [],
      "source": [
        "def update_model(model, prune_percent, activation_norms):\n",
        "    \"\"\"\n",
        "    Applies pruning to all MLP layers in the model.\n",
        "\n",
        "    Args:\n",
        "        model: LlamaForCausalLM model to prune\n",
        "        prune_percent: Fraction of neurons to remove\n",
        "        activation_norms: Dict mapping layer_idx -> X_d_norm tensor\n",
        "\n",
        "    Returns:\n",
        "        model: Pruned model with updated configuration\n",
        "    \"\"\"\n",
        "\n",
        "    new_intermediate_size = None\n",
        "    pruning_stats = []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting pruning with {prune_percent*100:.1f}% width pruning\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        if idx not in activation_norms:\n",
        "            raise KeyError(f\"No activation norms for layer {idx}\")\n",
        "\n",
        "        X_d_norm = activation_norms[idx]\n",
        "        original_size = mlp.gate_proj.out_features\n",
        "\n",
        "        # Prune neuron pairs\n",
        "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(\n",
        "            mlp=mlp,\n",
        "            prune_percent=prune_percent,\n",
        "            X_d_norm=X_d_norm,\n",
        "            layer_idx=idx\n",
        "        )\n",
        "\n",
        "        # Replace layers\n",
        "        mlp.gate_proj = new_gate_proj\n",
        "        mlp.up_proj = new_up_proj\n",
        "        mlp.down_proj = new_down_proj\n",
        "\n",
        "        pruning_stats.append({\n",
        "            'layer': idx,\n",
        "            'original_size': original_size,\n",
        "            'new_size': new_size,\n",
        "            'pruned': original_size - new_size,\n",
        "            'kept_percent': (new_size / original_size) * 100\n",
        "        })\n",
        "\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "        if (idx + 1) % 4 == 0:\n",
        "            print(f\"  Pruned layers {idx-3:2d}-{idx:2d}: \"\n",
        "                  f\"{original_size} â†’ {new_size} neurons \"\n",
        "                  f\"({(new_size/original_size)*100:.1f}% kept)\")\n",
        "\n",
        "    # Update model configuration\n",
        "    model.config.intermediate_size = new_intermediate_size\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Pruning completed!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Layers pruned: {len(pruning_stats)}\")\n",
        "    print(f\"  Original intermediate size: {original_size}\")\n",
        "    print(f\"  New intermediate size: {new_intermediate_size}\")\n",
        "    print(f\"  Neurons pruned per layer: {original_size - new_intermediate_size}\")\n",
        "    print(f\"  Effective width pruning: {((original_size - new_intermediate_size) / original_size) * 100:.2f}%\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81ca44a",
      "metadata": {
        "id": "c81ca44a"
      },
      "source": [
        "### Wiki-Calibrated Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fdf2adac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf2adac",
        "outputId": "6b2a35a4-aa81-46ef-b515-45c407e879ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Starting pruning with 20.0% width pruning\n",
            "============================================================\n",
            "\n",
            "  Pruned layers  0- 3: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "  Pruned layers  4- 7: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "  Pruned layers  8-11: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "  Pruned layers 12-15: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "\n",
            "============================================================\n",
            "Pruning completed!\n",
            "============================================================\n",
            "  Layers pruned: 16\n",
            "  Original intermediate size: 8192\n",
            "  New intermediate size: 6554\n",
            "  Neurons pruned per layer: 1638\n",
            "  Effective width pruning: 20.00%\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prune the model using Wiki activations\n",
        "wiki_model = update_model(copy.deepcopy(model),\n",
        "                          PRUNE_PERCENT,\n",
        "                          activation_norms_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "````\n",
        "============================================================\n",
        "Starting pruning with 20.0% width pruning\n",
        "============================================================\n",
        "\n",
        "  Pruned layers  0- 3: 8192 â†’ 6554 neurons (80.0% kept)\n",
        "  Pruned layers  4- 7: 8192 â†’ 6554 neurons (80.0% kept)\n",
        "  Pruned layers  8-11: 8192 â†’ 6554 neurons (80.0% kept)\n",
        "  Pruned layers 12-15: 8192 â†’ 6554 neurons (80.0% kept)\n",
        "\n",
        "============================================================\n",
        "Pruning completed!\n",
        "============================================================\n",
        "  Layers pruned: 16\n",
        "  Original intermediate size: 8192\n",
        "  New intermediate size: 6554\n",
        "  Neurons pruned per layer: 1638\n",
        "  Effective width pruning: 20.00%\n",
        "============================================================\n",
        "````\n"
      ],
      "metadata": {
        "id": "QTQCBzXjUs-l"
      },
      "id": "QTQCBzXjUs-l"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7-6LdtLFb76H",
      "metadata": {
        "id": "7-6LdtLFb76H"
      },
      "outputs": [],
      "source": [
        "del(model)\n",
        "clear_gpu_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "7b9a1389",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b9a1389",
        "outputId": "ab775872-d5d9-4083-e1b5-a8bb71db3bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wiki-pruned model: Paris is the capital of France and the largest city in France. It is located in the north-east of the country. The city has a population of 2.1 million people, which makes it the 3rd largest metropolitan area in Europe. Paris is also a major\n"
          ]
        }
      ],
      "source": [
        "# Test wiki model\n",
        "wiki_model.config.eos_token_id = tokenizer.eos_token_id\n",
        "generated_wiki = generate_text(wiki_model, tokenizer, prompt, device)\n",
        "print(f\"Wiki-pruned model: {generated_wiki}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wiki-pruned model: Paris is the capital of France and is located in the north-east of the country. The city has a population of 2.1 million people, making it the 3rd largest city in France. It is also the largest in terms of surface area, with 100"
      ],
      "metadata": {
        "id": "SFJD1FFFUxW6"
      },
      "id": "SFJD1FFFUxW6"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "OULoO1Xy3g8z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OULoO1Xy3g8z",
        "outputId": "8c4ef627-b3f7-41d9-c610-092ec0a42531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:16<00:00,  1.54it/s]\n"
          ]
        }
      ],
      "source": [
        "clear_gpu_cache()\n",
        "metrics_wiki_wiki = evaluate_metrics(wiki_model, dataloaderwiki)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "xmpv66mn4ng6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmpv66mn4ng6",
        "outputId": "5a5d08fa-bfcb-48fc-fbcf-084488fad28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:16<00:00,  1.50it/s]\n"
          ]
        }
      ],
      "source": [
        "metrics_wiki_sms = evaluate_metrics(wiki_model, dataloadersms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "dgr767nUWWuw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgr767nUWWuw",
        "outputId": "084afac0-b0fe-491c-ba4e-96849a6b3ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Optimization active: Using Mixed Precision (torch.bfloat16) if available.\n",
            "Measuring performance on 3 batches (Total samples: 12, 3 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:   0%|          | 0/3 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:25<00:51, 26.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:52<00:26, 26.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:18<00:00, 26.06s/it]\n"
          ]
        }
      ],
      "source": [
        "wiki_wiki_timing = measure_detailed_performance(wiki_model, tokenizer, dataloaderwiki, max_samples=MAX_SAMPLES_PERFORMANCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "feAJux7Q4fsM",
      "metadata": {
        "id": "feAJux7Q4fsM"
      },
      "outputs": [],
      "source": [
        "wiki_params = count_parameters(wiki_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a5_otkEIZKTZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5_otkEIZKTZ",
        "outputId": "11f94ead-8704-4737-b296-c46cfdbdba1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Optimization active: Using Mixed Precision (torch.bfloat16) if available.\n",
            "Measuring performance on 3 batches (Total samples: 12, 3 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:   0%|          | 0/3 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:26<00:52, 26.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:52<00:26, 26.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:18<00:00, 26.09s/it]\n"
          ]
        }
      ],
      "source": [
        "wiki_sms_timing = measure_detailed_performance(wiki_model, tokenizer, dataloadersms, max_samples=MAX_SAMPLES_PERFORMANCE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_wiki_carbon = measure_energy_consumption(wiki_model, tokenizer, dataloaderwiki,\n",
        "                                                 idle_power_watts=idle_watts,\n",
        "                                                 max_samples=MAX_SAMPLES_CARBON, max_new_tokens=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kNktQyriw_W",
        "outputId": "68868569-bcd0-4d9e-bc94-936138663a54"
      },
      "id": "1kNktQyriw_W",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ Measuring energy on 50 samples (1 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Energy measurement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:00<00:00,  1.20s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "XS9W2Av-hRWE",
      "metadata": {
        "id": "XS9W2Av-hRWE"
      },
      "outputs": [],
      "source": [
        "del(wiki_model)\n",
        "clear_gpu_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea2ece40",
      "metadata": {
        "id": "ea2ece40"
      },
      "source": [
        "### SMS-Calibrated Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "24Wi1HfzcCEb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24Wi1HfzcCEb",
        "outputId": "296f7079-c8c2-427e-f6dc-771a5fd906b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2004a2c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2004a2c6",
        "outputId": "5477298a-33d3-47df-a63b-460ab1ac8c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SMS CALIBRATION\n",
            "============================================================\n",
            "\n",
            "Setting up activation hooks...\n",
            "âœ“ Registered 16 hooks on down_proj\n",
            "\n",
            "Running calibration forward passes on SMS...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMS Calibration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Processed 25 batches\n",
            "Removing hooks...\n",
            "Extracting activation statistics...\n",
            "âœ“ Collected activation norms for 16 layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Clear GPU cache before SMS calibration\n",
        "clear_gpu_cache()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SMS CALIBRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Setup hooks\n",
        "print(\"\\nSetting up activation hooks...\")\n",
        "handles_sms = setup_mlp_hooks_for_importance(model, device)\n",
        "\n",
        "# Step 2: Run calibration forward passes\n",
        "print(\"\\nRunning calibration forward passes on SMS...\")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloadersms, desc=\"SMS Calibration\")):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(device),\n",
        "            'attention_mask': batch['attention_mask'].to(device)\n",
        "        }\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nâœ“ Processed {len(dataloadersms)} batches\")\n",
        "\n",
        "# Step 3: Clean up hooks\n",
        "print(\"Removing hooks...\")\n",
        "for handle in handles_sms:\n",
        "    handle.remove()\n",
        "\n",
        "# Step 4: Get activation norms\n",
        "print(\"Extracting activation statistics...\")\n",
        "activation_norms_sms = get_activation_norms()\n",
        "print(f\"âœ“ Collected activation norms for {len(activation_norms_sms)} layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "08fd3aaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fd3aaa",
        "outputId": "a30996e4-0343-4a3e-e0b1-5799bdfa10b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Starting pruning with 20.0% width pruning\n",
            "============================================================\n",
            "\n",
            "  Pruned layers  0- 3: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "  Pruned layers  4- 7: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "  Pruned layers  8-11: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "  Pruned layers 12-15: 8192 â†’ 6554 neurons (80.0% kept)\n",
            "\n",
            "============================================================\n",
            "Pruning completed!\n",
            "============================================================\n",
            "  Layers pruned: 16\n",
            "  Original intermediate size: 8192\n",
            "  New intermediate size: 6554\n",
            "  Neurons pruned per layer: 1638\n",
            "  Effective width pruning: 20.00%\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prune the model using SMS activations\n",
        "sms_model = update_model(copy.deepcopy(model), PRUNE_PERCENT, activation_norms_sms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "NIA_o3VEuXWg",
      "metadata": {
        "id": "NIA_o3VEuXWg"
      },
      "outputs": [],
      "source": [
        "del(model)\n",
        "clear_gpu_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ff076cdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff076cdb",
        "outputId": "0e769e25-b07e-46be-eef4-14b4ddd9e2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMS-pruned model: Paris is the capital of the French region of Paris. It is located in the north-western part of that region. The city has a population of 1.8 million inhabitants, making it larger than New York City, Los Angeles, and Chicago combined. Paris is also\n"
          ]
        }
      ],
      "source": [
        "# Test SMS model\n",
        "sms_model.config.eos_token_id = tokenizer.eos_token_id\n",
        "generated_sms = generate_text(sms_model, tokenizer, prompt, device)\n",
        "print(f\"SMS-pruned model: {generated_sms}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7IUiiAIGYs0y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IUiiAIGYs0y",
        "outputId": "82fa264a-6dec-40ed-9276-01674e4d3f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:16<00:00,  1.53it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:16<00:00,  1.50it/s]\n"
          ]
        }
      ],
      "source": [
        "metrics_sms_wiki = evaluate_metrics(sms_model, dataloaderwiki)\n",
        "metrics_sms_sms = evaluate_metrics(sms_model, dataloadersms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_sms_carbon = measure_energy_consumption(sms_model, tokenizer, dataloaderwiki,\n",
        "                                                idle_power_watts=idle_watts,\n",
        "                                                max_samples=MAX_SAMPLES_CARBON, max_new_tokens=50)"
      ],
      "metadata": {
        "id": "NmMU71x1kA9U",
        "outputId": "ea317fd6-cea9-408a-d9a5-5d56718add10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NmMU71x1kA9U",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ Measuring energy on 50 samples (1 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:   0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:   2%|â–         | 1/50 [00:01<01:07,  1.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:   4%|â–         | 2/50 [00:03<01:13,  1.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:   6%|â–Œ         | 3/50 [00:04<01:14,  1.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:   8%|â–Š         | 4/50 [00:06<01:09,  1.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  10%|â–ˆ         | 5/50 [00:07<01:05,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  12%|â–ˆâ–        | 6/50 [00:08<01:03,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  14%|â–ˆâ–        | 7/50 [00:10<01:00,  1.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  16%|â–ˆâ–Œ        | 8/50 [00:11<00:59,  1.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  18%|â–ˆâ–Š        | 9/50 [00:13<00:57,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  20%|â–ˆâ–ˆ        | 10/50 [00:13<00:43,  1.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  22%|â–ˆâ–ˆâ–       | 11/50 [00:13<00:32,  1.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  24%|â–ˆâ–ˆâ–       | 12/50 [00:15<00:41,  1.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:16<00:45,  1.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  28%|â–ˆâ–ˆâ–Š       | 14/50 [00:18<00:45,  1.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:19<00:46,  1.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:21<00:45,  1.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:21<00:34,  1.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:22<00:36,  1.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:24<00:37,  1.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:25<00:38,  1.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:27<00:40,  1.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:27<00:30,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:27<00:22,  1.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:29<00:26,  1.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:30<00:27,  1.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:32<00:28,  1.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:32<00:21,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:33<00:23,  1.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:35<00:24,  1.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:36<00:24,  1.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:36<00:18,  1.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:38<00:19,  1.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:39<00:21,  1.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:41<00:21,  1.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:42<00:20,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:43<00:16,  1.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:45<00:16,  1.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:46<00:15,  1.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [00:47<00:14,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [00:49<00:13,  1.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [00:50<00:12,  1.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [00:52<00:11,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [00:53<00:10,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [00:55<00:08,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [00:56<00:07,  1.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [00:58<00:05,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [00:58<00:03,  1.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [00:58<00:01,  1.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:00<00:01,  1.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Energy measurement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:01<00:00,  1.23s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "iSLgyzMPWeGz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSLgyzMPWeGz",
        "outputId": "9da93d5b-7c3e-4602-e305-265cc1596117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Optimization active: Using Mixed Precision (torch.bfloat16) if available.\n",
            "Measuring performance on 3 batches (Total samples: 12, 3 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:   0%|          | 0/3 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:26<00:52, 26.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:52<00:26, 26.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:18<00:00, 26.09s/it]\n"
          ]
        }
      ],
      "source": [
        "sms_wiki_timing = measure_detailed_performance(sms_model, tokenizer, dataloaderwiki, max_samples=MAX_SAMPLES_PERFORMANCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "T3W4pWW-ZaDs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3W4pWW-ZaDs",
        "outputId": "fad1fafd-00d3-43ea-b1fb-e005f37b212f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Optimization active: Using Mixed Precision (torch.bfloat16) if available.\n",
            "Measuring performance on 3 batches (Total samples: 12, 3 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:   0%|          | 0/3 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:26<00:52, 26.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:52<00:26, 26.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:18<00:00, 26.08s/it]\n"
          ]
        }
      ],
      "source": [
        "sms_sms_timing = measure_detailed_performance(sms_model, tokenizer, dataloadersms, max_samples=MAX_SAMPLES_PERFORMANCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca50b815",
      "metadata": {
        "id": "ca50b815"
      },
      "source": [
        "### Parameter Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "6838f398",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6838f398",
        "outputId": "45328245-161d-4792-e346-1ae388997e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PARAMETER COUNTS\n",
            "============================================================\n",
            "Original model:     1,235,814,400 parameters\n",
            "Wiki-pruned model:  1,074,792,448 parameters (13.03% reduction)\n",
            "SMS-pruned model:   1,074,792,448 parameters (13.03% reduction)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "sms_params = count_parameters(sms_model)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PARAMETER COUNTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original model:     {original_params:,} parameters\")\n",
        "print(f\"Wiki-pruned model:  {wiki_params:,} parameters ({((original_params - wiki_params) / original_params * 100):.2f}% reduction)\")\n",
        "print(f\"SMS-pruned model:   {sms_params:,} parameters ({((original_params - sms_params) / original_params * 100):.2f}% reduction)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ecc9f21",
      "metadata": {
        "id": "2ecc9f21"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## 5.2.6 Measuring the specialization effect.\n",
        "\n",
        "Now we evaluate all three models (base, wiki-pruned, sms-pruned) on both datasets (Wiki and SMS) to answer our core research question: **Does calibration dataset selection influence pruning decisions, and do the resulting models specialize for their calibration domain?**\n",
        "\n",
        "## The Experiment Design\n",
        "\n",
        "**Models**:\n",
        "1. Base model (unpruned Llama-3.2-1B)\n",
        "2. Wiki-pruned (calibrated on WikiText-2, 20% width reduction)\n",
        "3. SMS-pruned (calibrated on SMS Spam, 20% width reduction)\n",
        "\n",
        "**Evaluation datasets**:\n",
        "- WikiText-2 (formal, complex)\n",
        "- SMS Spam (informal, simple)\n",
        "\n",
        "**Hypothesis**:\n",
        "- Wiki-pruned should perform **better on WikiText** than on SMS (domain match)\n",
        "- SMS-pruned should perform **better on SMS** than on WikiText (domain match)\n",
        "- Cross-domain performance (wiki-pruned on SMS, sms-pruned on Wiki) should be **worse** than domain-matched performance\n",
        "\n",
        "This cross-evaluation matrix reveals whether data-driven pruning creates specialized models or if the calibration dataset choice doesn't significantly impact neuron selection.\n",
        "\n",
        "We'll measure:\n",
        "- **Perplexity**: Language modeling quality (lower is better)\n",
        "- **Loss**: Cross-entropy loss (lower is better)\n",
        "- **Inference speed**: Tokens per second (higher is better)\n",
        "\n",
        "Let's see if our hypothesis holds!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b08735ed",
      "metadata": {
        "id": "b08735ed"
      },
      "outputs": [],
      "source": [
        "# Clear original model to save memory\n",
        "clear_gpu_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476741ba",
      "metadata": {
        "id": "476741ba"
      },
      "source": [
        "### Loss and Perplexity Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "85728cd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85728cd8",
        "outputId": "df1cc516-f072-4792-9939-86adc0283b39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATING ON WIKITEXT-2\n",
            "============================================================\n",
            "\n",
            "Base model on Wiki: {'loss': 3.2460288122350294, 'perplexity': np.float64(25.688124727046315)}\n",
            "Wiki-pruned on Wiki: {'loss': 3.5722096263899807, 'perplexity': np.float64(35.595158321008455)}\n",
            "SMS-pruned on Wiki: {'loss': 3.752049879187817, 'perplexity': np.float64(42.60833447906563)}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING ON WIKITEXT-2\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nBase model on Wiki:\", metrics_base_wiki)\n",
        "print(\"Wiki-pruned on Wiki:\", metrics_wiki_wiki)\n",
        "print(\"SMS-pruned on Wiki:\", metrics_sms_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "````\n",
        "============================================================\n",
        "EVALUATING ON WIKITEXT-2\n",
        "============================================================\n",
        "\n",
        "Base model on Wiki: {'loss': 3.2460288122350294, 'perplexity': np.float64(25.688124727046315)}\n",
        "Wiki-pruned on Wiki: {'loss': 3.5913135137696126, 'perplexity': np.float64(36.28170115548473)}\n",
        "SMS-pruned on Wiki: {'loss': 3.8846293323963836, 'perplexity': np.float64(48.64890654342502)}"
      ],
      "metadata": {
        "id": "4MwsbNKgU8Q_"
      },
      "id": "4MwsbNKgU8Q_"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "8f400dde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f400dde",
        "outputId": "506ebcb8-7288-41b9-da80-6c2faeeb8efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATING ON SMS SPAM\n",
            "============================================================\n",
            "\n",
            "Base model on SMS: {'loss': 4.8114213257195955, 'perplexity': np.float64(122.90618314977401)}\n",
            "Wiki-pruned on SMS: {'loss': 5.198425378473171, 'perplexity': np.float64(180.98703130915655)}\n",
            "SMS-pruned on SMS: {'loss': 5.084889734255438, 'perplexity': np.float64(161.5621234875452)}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING ON SMS SPAM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nBase model on SMS:\", metrics_base_sms)\n",
        "print(\"Wiki-pruned on SMS:\", metrics_wiki_sms)\n",
        "print(\"SMS-pruned on SMS:\", metrics_sms_sms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "````\n",
        "\n",
        "============================================================\n",
        "EVALUATING ON SMS SPAM\n",
        "============================================================\n",
        "\n",
        "Base model on SMS: {'loss': 4.8114213257195955, 'perplexity': np.float64(122.90618314977401)}\n",
        "Wiki-pruned on SMS: {'loss': 5.206973014926048, 'perplexity': np.float64(182.540673179065)}\n",
        "SMS-pruned on SMS: {'loss': 5.109189337836538, 'perplexity': np.float64(165.53610660777312)}"
      ],
      "metadata": {
        "id": "omSGz5e3U_bP"
      },
      "id": "omSGz5e3U_bP"
    },
    {
      "cell_type": "markdown",
      "id": "c5c542cd",
      "metadata": {
        "id": "c5c542cd"
      },
      "source": [
        "## Performance Measurement (Inference Speed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "abe65cb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abe65cb0",
        "outputId": "51091beb-2859-451a-eda6-8809fbba166a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PERFORMANCE MEASUREMENT ON WIKI\n",
            "============================================================\n",
            "\n",
            "Base model:       10.4835s (19.08 tok/s)\n",
            "Wiki-pruned:      8.6873s (23.02 tok/s)\n",
            "SMS-pruned:       8.6954s (23.00 tok/s)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE MEASUREMENT ON WIKI\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(f\"\\nBase model:       {base_wiki_timing['avg_latency_sec']:.4f}s ({base_wiki_timing['throughput_tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"Wiki-pruned:      {wiki_wiki_timing['avg_latency_sec']:.4f}s ({wiki_wiki_timing['throughput_tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"SMS-pruned:       {sms_wiki_timing['avg_latency_sec']:.4f}s ({sms_wiki_timing['throughput_tokens_per_sec']:.2f} tok/s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "bd0f5545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd0f5545",
        "outputId": "60c57348-af10-4f5b-b6b6-ff1919127030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PERFORMANCE MEASUREMENT ON SMS\n",
            "============================================================\n",
            "\n",
            "Base model:       10.4782s (19.09 tok/s)\n",
            "Wiki-pruned:      8.6945s (23.00 tok/s)\n",
            "SMS-pruned:       8.6923s (23.01 tok/s)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE MEASUREMENT ON SMS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nBase model:       {base_sms_timing['avg_latency_sec']:.4f}s ({base_sms_timing['throughput_tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"Wiki-pruned:      {wiki_sms_timing['avg_latency_sec']:.4f}s ({wiki_sms_timing['throughput_tokens_per_sec']:.2f} tok/s)\")\n",
        "print(f\"SMS-pruned:       {sms_sms_timing['avg_latency_sec']:.4f}s ({sms_sms_timing['throughput_tokens_per_sec']:.2f} tok/s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29882f75",
      "metadata": {
        "id": "29882f75"
      },
      "source": [
        "# Summary and Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_energy_metrics(**models):\n",
        "    \"\"\"\n",
        "    Creates a comparison table of energy metrics across different models.\n",
        "\n",
        "    The first model passed is used as the baseline for calculating reductions.\n",
        "\n",
        "    Args:\n",
        "        **models: Named models with their energy metrics.\n",
        "                 Example: compare_energy_metrics(\n",
        "                     base=metrics_base_carbon,\n",
        "                     pruned_sms=metrics_sms_carbon,\n",
        "                     pruned_wiki=metrics_wiki_carbon\n",
        "                 )\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Formatted comparison table with reduction percentages.\n",
        "\n",
        "    Example:\n",
        "        >>> table = compare_energy_metrics(\n",
        "        ...     base=metrics_base_carbon,\n",
        "        ...     sms=metrics_sms_carbon,\n",
        "        ...     wiki=metrics_wiki_carbon,\n",
        "        ...     optimized=metrics_opti_carbon\n",
        "        ... )\n",
        "        >>> print(table)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    if not models:\n",
        "        print(\"âš ï¸ No models provided for comparison.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Prepare data for DataFrame\n",
        "    rows = []\n",
        "    model_names = list(models.keys())\n",
        "\n",
        "    # Get baseline (first model) for reduction calculations\n",
        "    baseline_name = model_names[0]\n",
        "    baseline_metrics = models[baseline_name]\n",
        "    baseline_efficiency = baseline_metrics['efficiency_joules_per_token']\n",
        "    baseline_energy_j = baseline_metrics['energy_net_kwh'] * 3_600_000\n",
        "\n",
        "    for model_name, metrics in models.items():\n",
        "        # Convert energy to Joules for better readability\n",
        "        energy_joules = metrics['energy_net_kwh'] * 3_600_000\n",
        "        efficiency = metrics['efficiency_joules_per_token']\n",
        "\n",
        "        # Calculate reductions relative to baseline\n",
        "        if model_name == baseline_name:\n",
        "            efficiency_reduction = \"-\"\n",
        "            energy_reduction = \"-\"\n",
        "        else:\n",
        "            efficiency_reduction = f\"{(1 - efficiency / baseline_efficiency) * 100:.1f}%\"\n",
        "            energy_reduction = f\"{(1 - energy_joules / baseline_energy_j) * 100:.1f}%\"\n",
        "\n",
        "        row = {\n",
        "            'Model': model_name,\n",
        "            'Energy (J)': f\"{energy_joules:.2f}\",\n",
        "            'Efficiency (J/token)': f\"{efficiency:.3f}\",\n",
        "            'Efficiency Î”': efficiency_reduction,\n",
        "            'COâ‚‚ (kg)': f\"{metrics['co2_emissions_kg']:.6f}\",\n",
        "            'Duration (s)': f\"{metrics['duration_sec']:.2f}\",\n",
        "            'Tokens': metrics['total_tokens']\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def compare_energy_metrics_styled(**models):\n",
        "    \"\"\"\n",
        "    Creates a styled comparison table with color-coded improvements.\n",
        "\n",
        "    Same as compare_energy_metrics() but with visual styling for Jupyter notebooks.\n",
        "    Green = improvement, Red = degradation (relative to baseline).\n",
        "\n",
        "    Args:\n",
        "        **models: Named models with their energy metrics.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame.style: Styled DataFrame for notebook display.\n",
        "\n",
        "    Example:\n",
        "        >>> compare_energy_metrics_styled(\n",
        "        ...     base=metrics_base_carbon,\n",
        "        ...     optimized=metrics_opti_carbon\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    df = compare_energy_metrics(**models)\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    def highlight_reduction(val):\n",
        "        \"\"\"Apply color based on reduction percentage.\"\"\"\n",
        "        if val == \"-\":\n",
        "            return 'background-color: lightgray'\n",
        "\n",
        "        # Extract numeric value from percentage string\n",
        "        try:\n",
        "            numeric_val = float(val.rstrip('%'))\n",
        "            if numeric_val > 0:  # Positive reduction = good\n",
        "                return 'background-color: lightgreen'\n",
        "            elif numeric_val < 0:  # Negative reduction = bad\n",
        "                return 'background-color: lightcoral'\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return ''\n",
        "\n",
        "    # Apply styling\n",
        "    styled_df = df.style.applymap(\n",
        "        highlight_reduction,\n",
        "        subset=['Efficiency Î”']\n",
        "    ).set_properties(**{\n",
        "        'text-align': 'center'\n",
        "    }).set_table_styles([\n",
        "        {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]}\n",
        "    ])\n",
        "\n",
        "    return styled_df"
      ],
      "metadata": {
        "id": "-BOsFs9Ppo2P"
      },
      "id": "-BOsFs9Ppo2P",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "5ea26207",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ea26207",
        "outputId": "0918241f-d58f-4761-dd10-b8c73c087a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE RESULTS\n",
            "================================================================================\n",
            "      Model  Parameters  Param Reduction %  PPL Wiki    PPL SMS  Loss Wiki  Loss SMS  Time Wiki (s)  Time SMS (s)  Tok/s Wiki  Tok/s SMS\n",
            "       Base  1235814400           0.000000 25.688125 122.906183   3.246029  4.811421      10.483521     10.478154   19.077560  19.087332\n",
            "Wiki-pruned  1074792448          13.029623 35.595158 180.987031   3.572210  5.198425       8.687341      8.694524   23.022004  23.002984\n",
            " SMS-pruned  1074792448          13.029623 42.608334 161.562123   3.752050  5.084890       8.695385      8.692314   23.000708  23.008833\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create comprehensive results table\n",
        "results = {\n",
        "    'Model': ['Base', 'Wiki-pruned', 'SMS-pruned'],\n",
        "    'Parameters': [original_params, wiki_params, sms_params],\n",
        "    'Param Reduction %': [\n",
        "        0,\n",
        "        ((original_params - wiki_params) / original_params * 100),\n",
        "        ((original_params - sms_params) / original_params * 100)\n",
        "    ],\n",
        "    'PPL Wiki': [\n",
        "        metrics_base_wiki['perplexity'],\n",
        "        metrics_wiki_wiki['perplexity'],\n",
        "        metrics_sms_wiki['perplexity']\n",
        "    ],\n",
        "    'PPL SMS': [\n",
        "        metrics_base_sms['perplexity'],\n",
        "        metrics_wiki_sms['perplexity'],\n",
        "        metrics_sms_sms['perplexity']\n",
        "    ],\n",
        "    'Loss Wiki': [\n",
        "        metrics_base_wiki['loss'],\n",
        "        metrics_wiki_wiki['loss'],\n",
        "        metrics_sms_wiki['loss']\n",
        "    ],\n",
        "    'Loss SMS': [\n",
        "        metrics_base_sms['loss'],\n",
        "        metrics_wiki_sms['loss'],\n",
        "        metrics_sms_sms['loss']\n",
        "    ],\n",
        "    'Time Wiki (s)': [\n",
        "        base_wiki_timing['avg_latency_sec'],\n",
        "        wiki_wiki_timing['avg_latency_sec'],\n",
        "        sms_wiki_timing['avg_latency_sec']\n",
        "    ],\n",
        "    'Time SMS (s)': [\n",
        "        base_sms_timing['avg_latency_sec'],\n",
        "        wiki_sms_timing['avg_latency_sec'],\n",
        "        sms_sms_timing['avg_latency_sec']\n",
        "    ],\n",
        "    'Tok/s Wiki': [\n",
        "        base_wiki_timing['throughput_tokens_per_sec'],\n",
        "        wiki_wiki_timing['throughput_tokens_per_sec'],\n",
        "        sms_wiki_timing['throughput_tokens_per_sec']\n",
        "    ],\n",
        "    'Tok/s SMS': [\n",
        "        base_sms_timing['throughput_tokens_per_sec'],\n",
        "        wiki_sms_timing['throughput_tokens_per_sec'],\n",
        "        sms_sms_timing['throughput_tokens_per_sec']\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(df_results.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table = compare_energy_metrics(\n",
        "    base=metrics_base_carbon,\n",
        "    sms=metrics_sms_carbon,\n",
        "    wiki=metrics_wiki_carbon,\n",
        ")\n",
        "print (table)"
      ],
      "metadata": {
        "id": "6TOqSdHdpfNN",
        "outputId": "3d797aac-b850-4150-ecfc-5f13cde3da53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6TOqSdHdpfNN",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model Energy (J) Efficiency (J/token) Efficiency Î”  COâ‚‚ (kg) Duration (s)  \\\n",
            "0  base     272.21                0.189            -  0.000220        47.03   \n",
            "1   sms     356.37                0.173         8.2%  0.000287        61.39   \n",
            "2  wiki     349.43                0.175         7.2%  0.000281        60.03   \n",
            "\n",
            "   Tokens  \n",
            "0    1444  \n",
            "1    2059  \n",
            "2    1997  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23urrbqko0x",
      "metadata": {
        "id": "23urrbqko0x"
      },
      "source": [
        "# Results Visualization\n",
        "\n",
        "Let's create visual comparisons to better understand the performance differences across models and datasets. These charts will make it easier to spot patterns in perplexity degradation, inference speed changes, and domain specialization effects."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cmxynafrc6",
      "metadata": {
        "id": "3cmxynafrc6"
      },
      "source": [
        "## Perplexity Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "ysemzl3z3be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "ysemzl3z3be",
        "outputId": "c945bdaf-6f07-41d4-8244-04ed14d30c66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Perplexity comparison across all models and datasets\n",
        "models = ['Base', 'Wiki-pruned', 'SMS-pruned']\n",
        "datasets = ['WikiText', 'SMS']\n",
        "\n",
        "# Extract perplexity values from results\n",
        "perplexity_data = [\n",
        "    [metrics_base_wiki['perplexity'], metrics_base_sms['perplexity']],\n",
        "    [metrics_wiki_wiki['perplexity'], metrics_wiki_sms['perplexity']],\n",
        "    [metrics_sms_wiki['perplexity'], metrics_sms_sms['perplexity']]\n",
        "]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, [row[0] for row in perplexity_data],\n",
        "               width, label='WikiText', alpha=0.8, color='skyblue')\n",
        "bars2 = ax.bar(x + width/2, [row[1] for row in perplexity_data],\n",
        "               width, label='SMS', alpha=0.8, color='lightcoral')\n",
        "\n",
        "ax.set_ylabel('Perplexity (Lower is Better)', fontsize=12)\n",
        "ax.set_title('Perplexity Comparison: 3 Models Ã— 2 Datasets', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eqil2wm4gkj",
      "metadata": {
        "id": "eqil2wm4gkj"
      },
      "source": [
        "## Performance Metrics Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "s1rathqm5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "s1rathqm5e",
        "outputId": "31d879e8-096b-40f5-cc42-4cbf62f49818"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Inference speed comparison (tokens per second)\n",
        "wiki_speed = [base_wiki_timing['throughput_tokens_per_sec'],\n",
        "              wiki_wiki_timing['throughput_tokens_per_sec'],\n",
        "              sms_wiki_timing['throughput_tokens_per_sec']]\n",
        "sms_speed = [base_sms_timing['throughput_tokens_per_sec'],\n",
        "             wiki_sms_timing['throughput_tokens_per_sec'],\n",
        "             sms_sms_timing['throughput_tokens_per_sec']]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, wiki_speed, width, label='WikiText', alpha=0.8, color='forestgreen')\n",
        "bars2 = ax.bar(x + width/2, sms_speed, width, label='SMS', alpha=0.8, color='orange')\n",
        "\n",
        "ax.set_ylabel('Tokens per Second (Higher is Better)', fontsize=12)\n",
        "ax.set_title('Inference Speed Comparison: 3 Models Ã— 2 Datasets', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model with optipfair\n",
        "This model will be created with optipfair incorporating the option to maintain an extension rate divisible by 128, ideal for modern GPUs to run the model efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYqevSt9GbjH"
      },
      "id": "KYqevSt9GbjH"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Q9gr8OjYHARe"
      },
      "id": "Q9gr8OjYHARe",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZmcdc-uvfSE",
        "outputId": "67d11acc-a4f6-4be0-e595-7ab79ee578d0"
      },
      "id": "NZmcdc-uvfSE",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3543"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model_optip, stats = prune_model(\n",
        "    model=model,\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"MAW\",\n",
        "    pruning_percentage=40,\n",
        "    #expansion_divisor=64,\n",
        "    dataloader=dataloaderwiki,\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")"
      ],
      "metadata": {
        "id": "lT3BUqtfGamm",
        "outputId": "4e892b41-3993-4169-ec28-ebe4314f1c97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lT3BUqtfGamm",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calibration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.75it/s]\n",
            "Pruning all layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  3.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats"
      ],
      "metadata": {
        "id": "ibjMCWih_yE-",
        "outputId": "98aacd61-7bae-4733-a7cf-7f33026fe9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ibjMCWih_yE-",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_parameters': 1235814400,\n",
              " 'pruned_parameters': 1072236544,\n",
              " 'reduction': 163577856,\n",
              " 'percentage_reduction': 13.236441977047686,\n",
              " 'expansion_rate': 318.75}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model_optip"
      ],
      "metadata": {
        "id": "HaX3auzSAAea",
        "outputId": "97c37811-063d-4ef8-aa4a-b6dcd5cab9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HaX3auzSAAea",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=6528, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=6528, bias=False)\n",
              "          (down_proj): Linear(in_features=6528, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760ae7f9-739b-4764-d2fc-5dea6e0998a4",
        "id": "1mrmJdy30ZI8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Optimization active: Using Mixed Precision (torch.bfloat16) if available.\n",
            "Measuring performance on 3 batches (Total samples: 12, 3 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:   0%|          | 0/3 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:28<00:56, 28.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:56<00:28, 28.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Performance test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:24<00:00, 28.08s/it]\n"
          ]
        }
      ],
      "source": [
        "optip_wiki_timing = measure_detailed_performance(pruned_model_optip,\n",
        "                                                 tokenizer,\n",
        "                                                 dataloaderwiki,\n",
        "                                                 max_samples=MAX_SAMPLES_PERFORMANCE)"
      ],
      "id": "1mrmJdy30ZI8"
    },
    {
      "cell_type": "code",
      "source": [
        "optip_wiki_timing"
      ],
      "metadata": {
        "id": "sMYhxk1p1-KX",
        "outputId": "e35a7eb7-41ab-4eba-8768-cc3fe660fd93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sMYhxk1p1-KX",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_latency_sec': 9.358437750074598,\n",
              " 'std_latency_sec': 0.03532836734096967,\n",
              " 'avg_tokens_per_generation': 200.0,\n",
              " 'throughput_tokens_per_sec': 21.371088352690677,\n",
              " 'num_unique_samples': 12,\n",
              " 'num_runs_per_sample': 3,\n",
              " 'total_measurements': 9,\n",
              " 'total_tokens': 1800,\n",
              " 'avg_latency_per_sample_sec': 2.3396094375186496,\n",
              " 'dtype_used': 'torch.bfloat16'}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_wiki_timing"
      ],
      "metadata": {
        "id": "P3uQWOWo2CL2",
        "outputId": "3af3611c-5250-43ee-829d-3c929e0dd794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P3uQWOWo2CL2",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_latency_sec': 8.687341027789646,\n",
              " 'std_latency_sec': 0.02738739606160486,\n",
              " 'avg_tokens_per_generation': 200.0,\n",
              " 'throughput_tokens_per_sec': 23.022004012531184,\n",
              " 'num_unique_samples': 12,\n",
              " 'num_runs_per_sample': 3,\n",
              " 'total_measurements': 9,\n",
              " 'total_tokens': 1800,\n",
              " 'avg_latency_per_sample_sec': 2.1718352569474115,\n",
              " 'dtype_used': 'torch.bfloat16'}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_opti_carbon = measure_energy_consumption(pruned_model_optip, tokenizer, dataloaderwiki,\n",
        "                                                 idle_power_watts=idle_watts,\n",
        "                                                 max_samples=MAX_SAMPLES_CARBON, max_new_tokens=50)"
      ],
      "metadata": {
        "outputId": "20ee86fb-c481-4761-f9b0-dbf5e11c7c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naCp7blr0ZI9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ Measuring energy on 50 samples (1 runs each)...\n",
            "   ğŸ”¥ Performing GPU Warm-up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:   0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:   2%|â–         | 1/50 [00:01<01:08,  1.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:   4%|â–         | 2/50 [00:02<01:07,  1.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:   6%|â–Œ         | 3/50 [00:04<01:09,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:   8%|â–Š         | 4/50 [00:06<01:12,  1.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  10%|â–ˆ         | 5/50 [00:07<01:08,  1.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  12%|â–ˆâ–        | 6/50 [00:07<00:48,  1.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  14%|â–ˆâ–        | 7/50 [00:09<00:51,  1.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  16%|â–ˆâ–Œ        | 8/50 [00:10<00:52,  1.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  18%|â–ˆâ–Š        | 9/50 [00:12<00:53,  1.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  20%|â–ˆâ–ˆ        | 10/50 [00:13<00:53,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  22%|â–ˆâ–ˆâ–       | 11/50 [00:13<00:39,  1.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  24%|â–ˆâ–ˆâ–       | 12/50 [00:15<00:43,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:16<00:47,  1.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  28%|â–ˆâ–ˆâ–Š       | 14/50 [00:18<00:49,  1.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:19<00:48,  1.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:20<00:36,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:20<00:30,  1.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:21<00:26,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:22<00:30,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:24<00:33,  1.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:25<00:34,  1.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:25<00:25,  1.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:26<00:19,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:27<00:24,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:29<00:29,  1.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:30<00:30,  1.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:32<00:30,  1.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:33<00:29,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:34<00:28,  1.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:36<00:27,  1.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:36<00:19,  1.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:37<00:20,  1.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:39<00:20,  1.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:41<00:21,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:42<00:21,  1.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:43<00:15,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:44<00:15,  1.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:45<00:15,  1.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [00:47<00:14,  1.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [00:48<00:13,  1.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [00:50<00:12,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [00:51<00:10,  1.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [00:53<00:10,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [00:54<00:08,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [00:56<00:07,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [00:57<00:05,  1.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [00:58<00:04,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [00:59<00:02,  1.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:00<00:01,  1.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Energy measurement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:01<00:00,  1.24s/it]\n"
          ]
        }
      ],
      "id": "naCp7blr0ZI9"
    },
    {
      "cell_type": "code",
      "source": [
        "table = compare_energy_metrics(\n",
        "    base=metrics_base_carbon,\n",
        "    wiki=metrics_wiki_carbon,\n",
        "    opti=metrics_opti_carbon,\n",
        ")\n",
        "print (table)"
      ],
      "metadata": {
        "id": "aNQD1E-eybv9",
        "outputId": "c8b9d856-8de6-4352-a440-1d73285adbb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aNQD1E-eybv9",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model Energy (J) Efficiency (J/token) Efficiency Î”  COâ‚‚ (kg) Duration (s)  \\\n",
            "0  base     272.21                0.189            -  0.000220        47.03   \n",
            "1  wiki     349.43                0.175         7.2%  0.000281        60.03   \n",
            "2  opti     357.29                0.173         8.3%  0.000289        61.95   \n",
            "\n",
            "   Tokens  \n",
            "0    1444  \n",
            "1    1997  \n",
            "2    2067  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    torch_dtype=torch.bfloat16, # Recommended for Llama 3.2\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 2. Apply 20% Width Pruning using MAW\n",
        "pruned_model, stats = prune_model(\n",
        "    model=model,\n",
        "    pruning_type=\"MLP_GLU\",          # Width pruning for GLU architectures\n",
        "    neuron_selection_method=\"MAW\",   # Maximum Absolute Weight criterion\n",
        "    pruning_percentage=20,           # 20% reduction\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVPWEnf6yXz1",
        "outputId": "f5f16d49-c4d4-4cf6-f0d5-6f01cc474db1"
      },
      "id": "VVPWEnf6yXz1",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning all layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5o8uihia7o3",
      "metadata": {
        "id": "5o8uihia7o3"
      },
      "source": [
        "# Summary and Analysis\n",
        "\n",
        "This notebook explored data-driven neuron pruning using activation patterns from calibration datasets. We created two differently-pruned models (Wiki-calibrated and SMS-calibrated) and cross-evaluated them to test whether calibration dataset selection influences pruning decisions and model specialization.\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### Parameter Reduction\n",
        "- **Both pruned models**: 13.03% parameter reduction (161M parameters removed)\n",
        "- **Target pruning**: 20% width reduction (8,192 â†’ 6,554 neurons per layer)\n",
        "\n",
        "### Perplexity Impact\n",
        "\n",
        "The results reveal **strong domain sensitivity**:\n",
        "\n",
        "**WikiText evaluation**:\n",
        "- Base model: 25.69 PPL\n",
        "- Wiki-pruned: 36.28 PPL (+41.2% increase)\n",
        "- SMS-pruned: 48.65 PPL (+89.3% increase) âš ï¸\n",
        "\n",
        "**SMS evaluation**:\n",
        "- Base model: 122.91 PPL\n",
        "- Wiki-pruned: 182.54 PPL (+48.5% increase) âš ï¸\n",
        "- SMS-pruned: 165.54 PPL (+34.7% increase)\n",
        "\n",
        "**Critical observation**: SMS-pruned performs better on SMS than Wiki-pruned (165.5 vs 182.5 PPL), and Wiki-pruned performs better on Wiketext. Providing evidence of **domain specialization**.\n",
        "\n",
        "This confirms that calibration dataset choice **does influence** which neurons get pruned, creating models that are partially specialized for their calibration domain.\n",
        "\n",
        "## Performance Trade-offs\n",
        "\n",
        "### Inference Speed.\n",
        "\n",
        "**Hypothesis**: Pruned models should be faster (fewer parameters â†’ less computation)\n",
        "\n",
        "**Reality**: Pruned models show **better but not significantr** performance:\n",
        "\n",
        "**WikiText**:\n",
        "- Base: 29.74 tok/s\n",
        "- Wiki-pruned: 35.45 tok/s (+19% faster)\n",
        "- SMS-pruned: 34.88 tok/s (+17% faster)\n",
        "\n",
        "**SMS**:\n",
        "- Base: 34.32 tok/s\n",
        "- Wiki-pruned: 35.88 tok/s (+4.5% faster)\n",
        "- SMS-pruned: 35.99 tok/s (+4.9% faster)\n",
        "\n",
        "**Why are gains minimal?**\n",
        "1. **Memory bandwidth bottleneck**: Modern GPUs are often memory-bound, not compute-bound\n",
        "2. **Irregular tensor shapes**: Non-standard intermediate sizes (6,554) may not map efficiently to GPU hardware\n",
        "3. **Small model size**: Llama-3.2-1B already fits comfortably in memory; pruning doesn't alleviate a bottleneck\n",
        "4. **Framework overhead**: PyTorch may not fully optimize for irregular layer dimensions\n",
        "5. **No alignment with GPU hardware**: Tensor Cores process efficiently in multiples of 128; irregular dimensions waste parallelism and cause inefficient memory accesses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xcbic511h3",
      "metadata": {
        "id": "xcbic511h3"
      },
      "source": [
        "## Domain Specialization Analysis\n",
        "\n",
        "**Does data-driven pruning create specialized models?**\n",
        "\n",
        "**Evidence for specialization**:\n",
        "âœ… SMS-pruned outperforms Wiki-pruned on SMS dataset (165.5 vs 182.5 PPL)  \n",
        "âœ… Wiki-pruned outperforms SMS-pruned on WikiText (36.3 vs 48.6 PPL)  \n",
        "âœ… Each model performs best on its calibration domain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494cfab0",
      "metadata": {
        "id": "494cfab0"
      },
      "source": [
        "### Text Generation Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "MNd22xaeGyfp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNd22xaeGyfp",
        "outputId": "b6446bdc-c9bb-477a-9ae1-eba5c831255d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a rich history and culture, and it is home to many famous landmarks, including the E\n",
            "* Paris is the capital of France and the largest city in France. It is located in the north-east of the country. The city has a population of 2.1 million people, which makes it the 3rd largest metropolitan area in Europe. Paris is also a major\n",
            "* Paris is the capital of France and the largest city in the world. It is located on the Seine River and has a population of 2.1 million. The city is also known as Parisien, Parisian, or Parisienne, depending on where you live.\n",
            "* Paris is the capital of the French region of Paris. It is located in the north-western part of that region. The city has a population of 1.8 million inhabitants, making it larger than New York City, Los Angeles, and Chicago combined. Paris is also\n"
          ]
        }
      ],
      "source": [
        "generated_optip = generate_text(pruned_model_optip, tokenizer, prompt, device)\n",
        "print(\"* \" + generated_base)\n",
        "print(\"* \" + generated_wiki)\n",
        "print(\"* \" + generated_optip)\n",
        "print(\"* \" + generated_sms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the responses of the pruned models, a degradation in factuality is observed, introducing inaccuracies in their texts, but maintaining the capacity to generate coherent text.\n"
      ],
      "metadata": {
        "id": "5VD_udiZzyKH"
      },
      "id": "5VD_udiZzyKH"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}