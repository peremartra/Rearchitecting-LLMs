{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNytHy38KsCr58CzXcGC/5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Rearchitecting-LLMs/blob/main/CH05/CH05_NB03_bonus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rearchitecting LLMs\n",
        "## Surgical Optimization for Hyper-Efficient Models\n",
        "\n",
        "\n",
        "### Chapter 5: Width Pruning\n",
        "### Notebook: bonus. Testing static pruned 20% model on Wiki2 and SMS Datasets.\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pere-martra/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=flat&logo=github&logoColor=white)](https://github.com/peremartra) [![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white)](https://x.com/PereMartra) [![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-blue)](https://huggingface.co/oopere)\n",
        "\n",
        "_____\n",
        "Colab Environment: GPU T4\n",
        "\n",
        "Models:\n",
        "* Llama-3.2-1B\n",
        "_____\n"
      ],
      "metadata": {
        "id": "CcjLZSLK15Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "      \"torch\" \\\n",
        "      \"transformers==4.55.4\" \\\n",
        "      \"accelerate==1.10.1\" \\\n",
        "      \"lm_eval==0.4.9.1\" \\\n",
        "      \"sentencepiece==0.2.1\" \\\n",
        "      \"datasets\" \\\n",
        "      \"langdetect\"\\\n",
        "      \"codecarbon\"\\\n",
        "      \"optipfair==0.2.3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbDoZhsPv1ty",
        "outputId": "4bc9a43e-afca-4682-9bed-9d33f862e6f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.6/357.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m263.3/263.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.53.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from lm_eval import evaluator\n",
        "from torch import nn\n",
        "from lm_eval.models.huggingface import HFLM\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import gc\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from optipfair import prune_model"
      ],
      "metadata": {
        "id": "9l7nkzkRzMve"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s1V5e3B4z750"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RECOVERY_SAMPLES = 100  # Calibration samples per dataset\n",
        "BATCH_SIZE = 4\n",
        "MAX_LENGTH = 1024"
      ],
      "metadata": {
        "id": "_4sBNulazfjA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "muJgshN_0DRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIhrK4HAvfeb"
      },
      "outputs": [],
      "source": [
        "# 1. Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    torch_dtype=torch.bfloat16, # Recommended for Llama 3.2\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 2. Apply 20% Width Pruning using MAW\n",
        "pruned_model, stats = prune_model(\n",
        "    model=model,\n",
        "    pruning_type=\"MLP_GLU\",          # Width pruning for GLU architectures\n",
        "    neuron_selection_method=\"MAW\",   # Maximum Absolute Weight criterion\n",
        "    pruning_percentage=20,           # 20% reduction\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. View Results\n",
        "print(f\"Original Parameters: {stats['original_parameters']:,}\")\n",
        "print(f\"Pruned Parameters:   {stats['pruned_parameters']:,}\")\n",
        "print(f\"Total Reduction:     {stats['percentage_reduction']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA1jMGxAv44_",
        "outputId": "27d852a6-b8ac-4fd5-e041-565bb0e51303"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Parameters: 1,235,814,400\n",
            "Pruned Parameters:   1,074,792,448\n",
            "Total Reduction:     13.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")\n",
        "\n",
        "from utils import (\n",
        "  evaluate_metrics, # Loss & Perpelexity\n",
        "  generate_text, #test inference model\n",
        "  measure_detailed_performance, # Inference performance\n",
        "  measure_energy_consumption, # Energy consumption\n",
        "  calibrate_idle_power, # Calibrate GPU Idle consumption\n",
        "  clear_gpu_cache\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toO7cw5SwChg",
        "outputId": "58046ac5-97a0-45e7-feb3-b207ff390e1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… utils.py downloaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "datawiki = load_dataset('wikitext', 'wikitext-2-raw-v1', split=f'train[:{RECOVERY_SAMPLES}]')\n",
        "datasms = load_dataset('sms_spam', split=f'train[:{RECOVERY_SAMPLES}]')\n",
        "\n",
        "print(f\"âœ“ WikiText samples: {len(datawiki)}\")\n",
        "print(f\"âœ“ SMS samples: {len(datasms)}\")"
      ],
      "metadata": {
        "id": "w4KlEDPsxGyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset, text_field='text'):\n",
        "    \"\"\"\n",
        "    Tokenizes and prepares a dataset for calibration.\n",
        "\n",
        "    Handles different dataset formats (WikiText uses 'text', SMS uses 'sms' field).\n",
        "    \"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        if text_field in examples:\n",
        "            texts = examples[text_field]\n",
        "        elif 'sms' in examples:  # SMS dataset specific\n",
        "            texts = examples['sms']\n",
        "        elif 'text' in examples:\n",
        "            texts = examples['text']\n",
        "        else:\n",
        "            texts = examples[list(examples.keys())[0]]\n",
        "\n",
        "        return tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "    return DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "i_NFOHwkyGz9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "dataloaderwiki = prepare_dataset(datawiki)\n",
        "dataloadersms = prepare_dataset(datasms)\n",
        "\n",
        "print(f\"âœ“ Created dataloaders\")\n",
        "print(f\"  Wiki batches: {len(dataloaderwiki)}\")\n",
        "print(f\"  SMS batches: {len(dataloadersms)}\")"
      ],
      "metadata": {
        "id": "LHbTUev0zmLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_pruned_wiki = evaluate_metrics(pruned_model, dataloaderwiki)\n",
        "metrics_pruned_sms = evaluate_metrics(pruned_model, dataloadersms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW0Qgrngzqsm",
        "outputId": "6f0f34a3-e724-4bb6-f790-9e2cda2a266f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06<00:00,  3.59it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06<00:00,  3.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base_wiki"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I4FR4fxztyK",
        "outputId": "a9a76c07-4dd5-49ef-d760-7d1060b7bba1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 3.9513443335929312, 'perplexity': np.float64(52.00523224381492)}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base_sms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FDG6-Ge1Fsb",
        "outputId": "3ed62f00-0eb8-4dd9-9425-9b33a501a010"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 5.383995887757847, 'perplexity': np.float64(217.89120699882324)}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf8unDo50-Y-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}